# -*- coding: utf-8 -*-
"""
Google Cloud AI Platform - å•ä¸ªæ•æ„Ÿæ€§åˆ†æå®éªŒæ‰§è¡Œè„šæœ¬
ä»GCSä¸‹è½½æ•°æ®ï¼Œè¿è¡Œå•ä¸ªå®éªŒï¼Œä¸Šä¼ ç»“æœ
"""

import os
import sys
import json
import argparse
import numpy as np
import pandas as pd
from pathlib import Path
from google.cloud import storage
import joblib
from sklearn.metrics import roc_curve

# ============================================
# Matplotlib é…ç½®ï¼ˆäº‘ç«¯ç¯å¢ƒï¼‰
# ============================================
# åœ¨æ— å›¾å½¢ç•Œé¢çš„äº‘ç«¯ç¯å¢ƒä¸­ï¼Œå¿…é¡»ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé¿å… plt.show() æŠ¥é”™
import matplotlib.pyplot as plt
print("âœ… Matplotlib å·²é…ç½®ä¸º Agg åç«¯ï¼ˆé€‚åˆäº‘ç«¯ç¯å¢ƒï¼‰")

# æ·»åŠ é¡¹ç›®è·¯å¾„
script_dir = Path(__file__).parent
project_root = script_dir.parent.parent
sys.path.insert(0, str(project_root))

# ============================================
# TensorFlow è¯Šæ–­ï¼ˆåœ¨å¯¼å…¥å…¶ä»–æ¨¡å—ä¹‹å‰ï¼‰
# ============================================
print("="*80)
print("TensorFlow è¯Šæ–­")
print("="*80)

# æ£€æŸ¥ç¯å¢ƒå˜é‡
print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}")
print(f"PYTHONPATH: {os.environ.get('PYTHONPATH', 'Not set')}")

# å°è¯•å¯¼å…¥ TensorFlow
try:
    import tensorflow as tf
    print(f"âœ… TensorFlow å¯¼å…¥æˆåŠŸ")
    print(f"TensorFlow ç‰ˆæœ¬: {tf.__version__}")
    
    # æ£€æŸ¥ GPU
    try:
        gpus = tf.config.list_physical_devices('GPU')
        print(f"GPU è®¾å¤‡: {len(gpus)} ä¸ª")
        for i, gpu in enumerate(gpus):
            print(f"  GPU {i}: {gpu.name}")
    except Exception as e:
        print(f"âš ï¸ GPU æ£€æŸ¥å¤±è´¥: {e}")
    
    # è®¾ç½® GPU å†…å­˜å¢é•¿
    try:
        for gpu in tf.config.list_physical_devices('GPU'):
            tf.config.experimental.set_memory_growth(gpu, True)
        print("âœ… GPU å†…å­˜å¢é•¿å·²å¯ç”¨")
    except Exception as e:
        print(f"âš ï¸ GPU å†…å­˜è®¾ç½®å¤±è´¥: {e}")
        
except ImportError as e:
    print(f"âŒ TensorFlow ImportError: {e}")
    import traceback
    traceback.print_exc()
except Exception as e:
    print(f"âŒ TensorFlow å¯¼å…¥å¤±è´¥ ({type(e).__name__}): {e}")
    import traceback
    traceback.print_exc()

print("="*80)

# å¯¼å…¥è®­ç»ƒç®¡é“å‡½æ•°
try:
    from function.pipeline import run_correct_training_pipeline
    from function.model_saving import save_complete_model_pipeline
    print("âœ… å¯¼å…¥è®­ç»ƒç®¡é“å‡½æ•°æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¯¼å…¥è®­ç»ƒç®¡é“å‡½æ•°å¤±è´¥: {e}")
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"functionç›®å½•å­˜åœ¨: {(project_root / 'function').exists()}")
    import traceback
    traceback.print_exc()
    sys.exit(2)

# å¯¼å…¥å·¥å…·å‡½æ•°ï¼ˆå¤„ç†å®¹å™¨å†…è·¯å¾„ï¼‰
try:
    from cloud.gcp.sensitivity_utils import find_project_root, extract_metrics
    print("âœ… ä» cloud.gcp.sensitivity_utils å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ ä» cloud.gcp.sensitivity_utils å¯¼å…¥å¤±è´¥: {e}")
    # å¦‚æœåœ¨å®¹å™¨å†…ï¼Œå¯èƒ½éœ€è¦ç›´æ¥å¯¼å…¥
    import importlib.util
    utils_path = script_dir / 'sensitivity_utils.py'
    print(f"å°è¯•ä» {utils_path} å¯¼å…¥...")
    if utils_path.exists():
        spec = importlib.util.spec_from_file_location("sensitivity_utils", utils_path)
        sensitivity_utils = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(sensitivity_utils)
        find_project_root = sensitivity_utils.find_project_root
        extract_metrics = sensitivity_utils.extract_metrics
        print("âœ… ä»æœ¬åœ°æ–‡ä»¶å¯¼å…¥æˆåŠŸ")
    else:
        print(f"âŒ æ‰¾ä¸åˆ° sensitivity_utils.py æ–‡ä»¶: {utils_path}")
        raise


def download_from_gcs(bucket_name, gcs_path, local_path):
    """ä»GCSä¸‹è½½æ–‡ä»¶"""
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(gcs_path)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    local_path = Path(local_path)
    local_path.parent.mkdir(parents=True, exist_ok=True)
    
    blob.download_to_filename(str(local_path))
    print(f"âœ… ä¸‹è½½: gs://{bucket_name}/{gcs_path} -> {local_path}")


def upload_to_gcs(bucket_name, local_path, gcs_path):
    """ä¸Šä¼ æ–‡ä»¶åˆ°GCS"""
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(gcs_path)
    blob.upload_from_filename(str(local_path))
    print(f"âœ… ä¸Šä¼ : {local_path} -> gs://{bucket_name}/{gcs_path}")


def load_data_from_gcs(bucket_name, data_dir='/data'):
    """ä»GCSåŠ è½½æ•°æ®"""
    os.makedirs(data_dir, exist_ok=True)
    
    # ä¸‹è½½æ•°æ®æ–‡ä»¶
    files_to_download = [
        ('data/df_positive.pkl', 'df_positive.pkl'),
        ('data/df_prediction_pool.pkl', 'df_prediction_pool.pkl'),
        ('data/features.json', 'features.json'),
        ('data/gmm_model.pkl', 'gmm_model.pkl')  # å¯é€‰
    ]
    
    downloaded_files = {}
    for gcs_path, local_file in files_to_download:
        local_path = os.path.join(data_dir, local_file)
        try:
            download_from_gcs(bucket_name, gcs_path, local_path)
            downloaded_files[local_file] = local_path
        except Exception as e:
            if local_file == 'gmm_model.pkl':
                print(f"âš ï¸ GMMæ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†é‡æ–°è®­ç»ƒ: {e}")
            else:
                raise
    
    # ä¸‹è½½shapefileæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        us_data_dir = os.path.join(data_dir, 'US_data')
        os.makedirs(us_data_dir, exist_ok=True)
        # å°è¯•ä¸‹è½½shapefileç›¸å…³æ–‡ä»¶
        shapefile_extensions = ['.shp', '.shx', '.dbf', '.prj', '.cpg']
        shapefile_base = 'data/US_data/cb_2018_us_nation_5m'
        for ext in shapefile_extensions:
            gcs_path = f"{shapefile_base}{ext}"
            local_path = os.path.join(us_data_dir, f"cb_2018_us_nation_5m{ext}")
            try:
                download_from_gcs(bucket_name, gcs_path, local_path)
            except Exception as e:
                print(f"âš ï¸ Shapefileæ–‡ä»¶ {ext} ä¸å­˜åœ¨: {e}")
    except Exception as e:
        print(f"âš ï¸ Shapefileä¸‹è½½å¤±è´¥ï¼ˆå¯èƒ½ä¸éœ€è¦ï¼‰: {e}")
    
    # åŠ è½½æ•°æ®
    df_positive = pd.read_pickle(downloaded_files['df_positive.pkl'])
    df_prediction_pool = pd.read_pickle(downloaded_files['df_prediction_pool.pkl'])
    
    with open(downloaded_files['features.json'], 'r') as f:
        features_data = json.load(f)
        features_no_coords = features_data['features_no_coords']
    
    # åŠ è½½GMMæ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    gmm_model_path = downloaded_files.get('gmm_model.pkl')
    gmm_model = None
    if gmm_model_path and os.path.exists(gmm_model_path):
        gmm_model = joblib.load(gmm_model_path)
        print("âœ… åŠ è½½é¢„è®­ç»ƒGMMæ¨¡å‹")
    
    return df_positive, df_prediction_pool, features_no_coords, gmm_model


def run_single_experiment(exp_id, params, config):
    """è¿è¡Œå•ä¸ªå®éªŒ"""
    bucket_name = config['bucket_name']
    data_dir = config.get('data_dir', '/data')
    output_dir = config.get('output_dir', '/output')
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    print(f"\n{'='*80}")
    print(f"å®éªŒ {exp_id}: åŠ è½½æ•°æ®")
    print(f"{'='*80}")
    df_positive, df_prediction_pool, features_no_coords, gmm_model = load_data_from_gcs(
        bucket_name, data_dir
    )
    
    # å¦‚æœGMMæ¨¡å‹å­˜åœ¨ï¼Œéœ€è¦å°†å…¶å¤åˆ¶åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼ˆpipelineä¼šä»é‚£é‡ŒåŠ è½½ï¼‰
    if gmm_model is not None:
        project_root = find_project_root()
        gmm_model_path = project_root / 'gmm_model_23c_fixed.pkl'
        joblib.dump(gmm_model, gmm_model_path)
        print(f"âœ… GMMæ¨¡å‹å·²ä¿å­˜åˆ°: {gmm_model_path}")
    
    # æ„å»ºtransformer_config
    transformer_config = {
        'd_model': params['d_model'],
        'num_heads': params.get('num_heads', 4),
        'num_layers': params['num_layers']
    }
    resnet_layers = params['resnet_layers']
    
    # è¿è¡Œè®­ç»ƒç®¡é“
    print(f"\n{'='*80}")
    print(f"å®éªŒ {exp_id}: å¼€å§‹è®­ç»ƒ")
    print(f"å‚æ•°: {params}")
    print(f"{'='*80}")
    
    result = run_correct_training_pipeline(
        df_positive=df_positive,
        df_prediction_pool=df_prediction_pool,
        features_no_coords=features_no_coords,
        negative_strategy=config['negative_strategy'],
        negative_ratio=config['negative_ratio'],
        augmentation_ratio=1,
        test_size=config['test_size'],
        val_size=config['val_size'],
        epochs=config['epochs'],
        batch_size=config['batch_size'],
        random_state=config['random_state'],
        learning_rate=params['learning_rate'],
        dropout_rate=params['dropout_rate'],
        resnet_layers=resnet_layers,
        transformer_config=transformer_config,
        model_type='transformer',
        train_mode='single',
        plot_learning_curve=config['plot_learning_curve'],
        learning_curve_epochs=config['learning_curve_epochs'],
        run_shap=False
    )
    
    # æå–æŒ‡æ ‡
    metrics = extract_metrics(result, params)
    metrics['exp_id'] = exp_id
    
    # ä¿å­˜å’Œä¸Šä¼ å›¾ç‰‡
    if result is not None and result.get('training_results') is not None:
        try:
            from function.evaluation import plot_training_results, plot_complete_pipeline_results
            
            # åˆ›å»ºå›¾ç‰‡è¾“å‡ºç›®å½•
            images_output_dir = os.path.join(output_dir, 'images')
            os.makedirs(images_output_dir, exist_ok=True)
            
            training_results = result.get('training_results')
            
            # 1. ä¿å­˜è®­ç»ƒç»“æœå›¾ï¼ˆå¦‚æœæœ‰è®­ç»ƒå†å²ï¼‰
            if training_results and training_results.get('history') is not None:
                history = training_results.get('history')
                # ä» training_results ä¸­æå– ROC æ•°æ®
                test_auc = training_results.get('test_auc', 0.0)
                splits = training_results.get('splits', {})
                y_test = splits.get('y_test')
                
                # éœ€è¦é‡æ–°è®¡ç®— ROC æ›²çº¿ï¼ˆå¦‚æœå†å²ä¸­æ²¡æœ‰ä¿å­˜ï¼‰
                if y_test is not None:
                    try:
                        # å°è¯•ä»æ¨¡å‹é¢„æµ‹è·å– y_test_pred
                        model = training_results.get('model')
                        X_test = splits.get('X_test')
                        if model is not None and X_test is not None:
                            y_test_pred = model.predict(X_test, verbose=0).ravel()
                            fpr, tpr, _ = roc_curve(y_test, y_test_pred)
                            
                            # ä¿å­˜è®­ç»ƒç»“æœå›¾
                            training_plot_path = os.path.join(images_output_dir, f"{exp_id}_training_results.png")
                            plot_training_results(history, fpr, tpr, test_auc, y_test, y_test_pred, 
                                                 save_path=training_plot_path)
                            
                            # ä¸Šä¼ è®­ç»ƒç»“æœå›¾
                            gcs_training_plot_path = f"results/{exp_id}/images/{exp_id}_training_results.png"
                            upload_to_gcs(bucket_name, training_plot_path, gcs_training_plot_path)
                            metrics['training_plot_path'] = gcs_training_plot_path
                            print(f"âœ… è®­ç»ƒç»“æœå›¾å·²ä¿å­˜å¹¶ä¸Šä¼ ")
                    except Exception as e:
                        print(f"âš ï¸ ä¿å­˜è®­ç»ƒç»“æœå›¾å¤±è´¥: {e}")
            
            # 2. ä¿å­˜å®Œæ•´ç®¡é“åˆ†æå›¾
            try:
                final_results = result.get('final_results')
                negative_results = result.get('negative_samples')
                prediction_results = result.get('prediction_results')
                
                if training_results and final_results is not None and negative_results is not None and prediction_results is not None:
                    pipeline_plot_path = os.path.join(images_output_dir, f"{exp_id}_pipeline_analysis.png")
                    plot_complete_pipeline_results(
                        training_results, final_results, negative_results, prediction_results,
                        save_path=pipeline_plot_path
                    )
                    
                    # ä¸Šä¼ å®Œæ•´ç®¡é“åˆ†æå›¾
                    gcs_pipeline_plot_path = f"results/{exp_id}/images/{exp_id}_pipeline_analysis.png"
                    upload_to_gcs(bucket_name, pipeline_plot_path, gcs_pipeline_plot_path)
                    metrics['pipeline_plot_path'] = gcs_pipeline_plot_path
                    print(f"âœ… å®Œæ•´ç®¡é“åˆ†æå›¾å·²ä¿å­˜å¹¶ä¸Šä¼ ")
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜å®Œæ•´ç®¡é“åˆ†æå›¾å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
            
            # 3. ä¿å­˜å­¦ä¹ æ›²çº¿å›¾ï¼ˆå¦‚æœæœ‰å­¦ä¹ æ›²çº¿åˆ†æç»“æœï¼‰
            try:
                lc_analysis = training_results.get('learning_curve_results')
                if lc_analysis is not None and isinstance(lc_analysis, dict):
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®æ¥ç»˜åˆ¶å­¦ä¹ æ›²çº¿
                    if 'train_sizes' in lc_analysis and 'train_scores_mean' in lc_analysis and 'val_scores_mean' in lc_analysis:
                        from function.learning_curve import plot_learning_curve
                        import numpy as np
                        
                        # æ„å»ºå­¦ä¹ æ›²çº¿æ•°æ®å­—å…¸
                        lc_data = {
                            'train_sizes': np.array(lc_analysis['train_sizes']) if isinstance(lc_analysis['train_sizes'], list) else lc_analysis['train_sizes'],
                            'train_scores_mean': np.array(lc_analysis['train_scores_mean']) if isinstance(lc_analysis['train_scores_mean'], list) else lc_analysis['train_scores_mean'],
                            'train_scores_std': np.array(lc_analysis['train_scores_std']) if isinstance(lc_analysis['train_scores_std'], list) else lc_analysis['train_scores_std'],
                            'val_scores_mean': np.array(lc_analysis['val_scores_mean']) if isinstance(lc_analysis['val_scores_mean'], list) else lc_analysis['val_scores_mean'],
                            'val_scores_std': np.array(lc_analysis['val_scores_std']) if isinstance(lc_analysis['val_scores_std'], list) else lc_analysis['val_scores_std']
                        }
                        
                        # è·å–è¿‡æ‹Ÿåˆåˆ†æç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        overfitting_analysis = lc_analysis.get('overfitting_analysis', {})
                        # ä»æ¨¡å‹é…ç½®ä¸­è·å–dropout_rateï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
                        model_config = lc_analysis.get('model_config', {})
                        dropout_rate = model_config.get('dropout_rate', 0.3)
                        
                        # ä¿å­˜å­¦ä¹ æ›²çº¿å›¾
                        learning_curve_plot_path = os.path.join(images_output_dir, f"{exp_id}_learning_curve.png")
                        plot_learning_curve(
                            lc_data=lc_data,
                            scoring="f1",  # é»˜è®¤ä½¿ç”¨f1ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                            dropout_rate=dropout_rate,
                            show_plot=False,  # äº‘ç«¯ç¯å¢ƒä¸æ˜¾ç¤º
                            save_path=learning_curve_plot_path
                        )
                        
                        # ä¸Šä¼ å­¦ä¹ æ›²çº¿å›¾
                        gcs_learning_curve_path = f"results/{exp_id}/images/{exp_id}_learning_curve.png"
                        upload_to_gcs(bucket_name, learning_curve_plot_path, gcs_learning_curve_path)
                        metrics['learning_curve_plot_path'] = gcs_learning_curve_path
                        print(f"âœ… å­¦ä¹ æ›²çº¿å›¾å·²ä¿å­˜å¹¶ä¸Šä¼ ")
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜å­¦ä¹ æ›²çº¿å›¾å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                
        except ImportError as e:
            print(f"âš ï¸ æ— æ³•å¯¼å…¥ç»˜å›¾å‡½æ•°: {e}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    # ä¿å­˜æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if config.get('save_models', False):
        model_output_dir = os.path.join(output_dir, 'models')
        os.makedirs(model_output_dir, exist_ok=True)
        
        model_name = f"{exp_id}_transformer_generation"
        
        # æ£€æŸ¥ result å’Œ training_results æ˜¯å¦å­˜åœ¨
        if result is None:
            print("âš ï¸ result ä¸º Noneï¼Œä¿å­˜å®éªŒé…ç½®å’Œé”™è¯¯ä¿¡æ¯")
            # å³ä½¿è®­ç»ƒå¤±è´¥ï¼Œä¹Ÿä¿å­˜å®éªŒé…ç½®ä¿¡æ¯
            try:
                error_info = {
                    'exp_id': exp_id,
                    'params': params,
                    'config': config,
                    'error': 'Training failed - result is None',
                    'timestamp': pd.Timestamp.now().isoformat()
                }
                error_file = os.path.join(model_output_dir, f"{model_name}_error_info.json")
                with open(error_file, 'w') as f:
                    json.dump(error_info, f, indent=2, default=str)
                print(f"âœ… å·²ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°: {error_file}")
                metrics['error_info_path'] = error_file
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜é”™è¯¯ä¿¡æ¯å¤±è´¥: {e}")
        elif result.get('training_results') is None:
            print("âš ï¸ training_results ä¸º Noneï¼Œå°è¯•ä¿å­˜éƒ¨åˆ†ç»“æœ")
            print(f"result çš„é”®: {list(result.keys()) if result else 'result is None'}")
            # å°è¯•ä¿å­˜éƒ¨åˆ†ç»“æœï¼ˆå¦‚æœæœ‰GMM pipelineç­‰ï¼‰
            try:
                if result.get('gmm_pipeline') is not None:
                    # è‡³å°‘ä¿å­˜GMM pipeline
                    gmm_file = os.path.join(model_output_dir, f"{model_name}_gmm_pipeline.pkl")
                    joblib.dump(result.get('gmm_pipeline'), gmm_file)
                    print(f"âœ… å·²ä¿å­˜GMM pipelineåˆ°: {gmm_file}")
                    metrics['gmm_pipeline_path'] = gmm_file
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜éƒ¨åˆ†ç»“æœå¤±è´¥: {e}")
        else:
            try:
                # éªŒè¯å¿…éœ€å‚æ•°
                training_results = result.get('training_results')
                if training_results is None:
                    raise ValueError("training_results ä¸º Noneï¼Œæ— æ³•ä¿å­˜æ¨¡å‹")
                
                gmm_pipeline = result.get('gmm_pipeline')
                dl_model = result.get('model')
                retrained_preprocessor = training_results.get('preprocessor')
                
                # æ£€æŸ¥å…³é”®ç»„ä»¶
                missing_components = []
                if gmm_pipeline is None:
                    missing_components.append('gmm_pipeline')
                if dl_model is None:
                    missing_components.append('dl_model')
                if retrained_preprocessor is None:
                    missing_components.append('preprocessor')
                
                if missing_components:
                    raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„æ¨¡å‹ç»„ä»¶: {', '.join(missing_components)}")
                
                # è°ƒç”¨ä¿å­˜å‡½æ•°
                saved_path = save_complete_model_pipeline(
                    gmm_pipeline=gmm_pipeline,
                    dl_model=dl_model,
                    retrained_preprocessor=retrained_preprocessor,
                    training_results=training_results,
                    final_results=result.get('final_results'),
                    negative_results=result.get('negative_samples'),
                    prediction_results=result.get('prediction_results'),
                    features=features_no_coords,
                    config=result.get('config', {}),
                    save_dir=model_output_dir,
                    model_name=model_name,
                    model_type='transformer',
                    negative_strategy=config['negative_strategy'],
                    train_mode='single',
                    pu_evaluation=result.get('pu_evaluation')
                )
                metrics['model_path'] = saved_path
                print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {saved_path}")
            except ValueError as e:
                # å‚æ•°éªŒè¯é”™è¯¯
                error_msg = f"æ¨¡å‹ä¿å­˜å‚æ•°éªŒè¯å¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                metrics['model_save_error'] = error_msg
                metrics['model_save_error_type'] = 'validation_error'
            except Exception as e:
                # å…¶ä»–ä¿å­˜é”™è¯¯
                error_msg = f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                import traceback
                traceback.print_exc()
                metrics['model_save_error'] = error_msg
                metrics['model_save_error_type'] = 'save_error'
        
        # ä¸Šä¼ æ¨¡å‹åˆ°GCS
        if 'model_path' in metrics and os.path.exists(metrics.get('model_path')):
            saved_path = metrics['model_path']
            model_dir = os.path.dirname(saved_path)
            model_base_name = os.path.basename(saved_path).replace('.pkl', '')
            
            print(f"\nğŸ“¦ ä¸Šä¼ æ¨¡å‹æ–‡ä»¶åˆ°GCS...")
            
            # ä¸Šä¼ æ¨¡å‹ç›®å½•ä¸­æ‰€æœ‰ç›¸å…³æ–‡ä»¶
            uploaded_files = []
            for file in os.listdir(model_dir):
                # åŒ¹é…æ‰€æœ‰ä»¥æ¨¡å‹åŸºç¡€åç§°å¼€å¤´çš„æ–‡ä»¶
                if file.startswith(model_base_name):
                    local_file = os.path.join(model_dir, file)
                    if os.path.isfile(local_file):  # ç¡®ä¿æ˜¯æ–‡ä»¶è€Œä¸æ˜¯ç›®å½•
                        gcs_file_path = f"results/{exp_id}/models/{file}"
                        try:
                            upload_to_gcs(bucket_name, local_file, gcs_file_path)
                            uploaded_files.append(file)
                        except Exception as e:
                            print(f"  âš ï¸ ä¸Šä¼ å¤±è´¥ {file}: {e}")
            
            print(f"âœ… å·²ä¸Šä¼  {len(uploaded_files)} ä¸ªæ¨¡å‹æ–‡ä»¶:")
            for f in uploaded_files:
                print(f"    - {f}")
            
            metrics['uploaded_model_files'] = uploaded_files
            metrics['model_gcs_path'] = f"gs://{bucket_name}/results/{exp_id}/models/"
    
    # ä¿å­˜ç»“æœ
    result_file = os.path.join(output_dir, f"{exp_id}_results.json")
    with open(result_file, 'w') as f:
        json.dump(metrics, f, indent=2, default=str)
    
    # ä¸Šä¼ ç»“æœåˆ°GCS
    gcs_result_path = f"results/{exp_id}/{exp_id}_results.json"
    upload_to_gcs(bucket_name, result_file, gcs_result_path)
    
    print(f"\nâœ… å®éªŒ {exp_id} å®Œæˆ")
    print(f"  - Accuracy: {metrics.get('accuracy', 'N/A'):.4f}" if isinstance(metrics.get('accuracy'), (int, float)) else f"  - Accuracy: {metrics.get('accuracy', 'N/A')}")
    print(f"  - F1: {metrics.get('f1', 'N/A'):.4f}" if isinstance(metrics.get('f1'), (int, float)) else f"  - F1: {metrics.get('f1', 'N/A')}")
    print(f"  - è¿‡æ‹Ÿåˆscore: {metrics.get('overfitting_score', 'N/A'):.4f}" if isinstance(metrics.get('overfitting_score'), (int, float)) else f"  - è¿‡æ‹Ÿåˆscore: {metrics.get('overfitting_score', 'N/A')}")
    
    return metrics


def main():
    # æ·»åŠ å¯åŠ¨æ—¥å¿—
    print("="*80)
    print("å¼€å§‹æ‰§è¡Œå®éªŒè„šæœ¬")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"è„šæœ¬è·¯å¾„: {__file__}")
    print(f"å‘½ä»¤è¡Œå‚æ•°: {sys.argv}")
    print("="*80)
    
    try:
        parser = argparse.ArgumentParser(description='GCPæ•æ„Ÿæ€§åˆ†æå®éªŒ')
        parser.add_argument('--exp_id', type=int, required=True, help='å®éªŒID (1-18)')
        parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶GCSè·¯å¾„ (gs://bucket/path)')
        parser.add_argument('--data_dir', type=str, default='/data', help='æœ¬åœ°æ•°æ®ç›®å½•')
        parser.add_argument('--output_dir', type=str, default='/output', help='æœ¬åœ°è¾“å‡ºç›®å½•')
        
        args = parser.parse_args()
        print(f"âœ… å‚æ•°è§£ææˆåŠŸ:")
        print(f"  - exp_id: {args.exp_id}")
        print(f"  - config: {args.config}")
        print(f"  - data_dir: {args.data_dir}")
        print(f"  - output_dir: {args.output_dir}")
    except SystemExit as e:
        print(f"âŒ å‚æ•°è§£æå¤±è´¥: {e}")
        print(f"å‘½ä»¤è¡Œå‚æ•°: {sys.argv}")
        import traceback
        traceback.print_exc()
        sys.exit(2)
    except Exception as e:
        print(f"âŒ å‚æ•°è§£ææ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(2)
    
    # ä»GCSä¸‹è½½é…ç½®æ–‡ä»¶
    if args.config.startswith('gs://'):
        bucket_name, config_path = args.config[5:].split('/', 1)
        local_config = '/tmp/config.json'
        download_from_gcs(bucket_name, config_path, local_config)
    else:
        local_config = args.config
        bucket_name = None
    
    # åŠ è½½é…ç½®
    with open(local_config, 'r') as f:
        config = json.load(f)
    
    if bucket_name:
        config['bucket_name'] = bucket_name
    else:
        config['bucket_name'] = os.environ.get('GCS_BUCKET', 'pv_cropland')
    
    config['data_dir'] = args.data_dir
    config['output_dir'] = args.output_dir
    
    # è·å–å‚æ•°ç»„åˆ
    params = config['param_combinations'][args.exp_id - 1]
    exp_id = f"E{args.exp_id}"
    
    # è¿è¡Œå®éªŒ
    try:
        metrics = run_single_experiment(exp_id, params, config)
        sys.exit(0)
    except Exception as e:
        print(f"âŒ å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    try:
        main()
    except SystemExit as e:
        # é‡æ–°æŠ›å‡ºSystemExitä»¥ä¿æŒæ­£ç¡®çš„é€€å‡ºç 
        raise
    except Exception as e:
        print(f"âŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

