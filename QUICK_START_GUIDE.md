# å¿«é€Ÿå…¥é—¨æŒ‡å— (Quick Start Guide)

æœ¬æŒ‡å—å¸®åŠ©ä½ åœ¨30åˆ†é’Ÿå†…å®Œæˆç¯å¢ƒé…ç½®å¹¶è¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚

---

## ğŸš€ 15åˆ†é’Ÿå¿«é€Ÿæ¼”ç¤º

### Step 0: ç¯å¢ƒå‡†å¤‡ (5åˆ†é’Ÿ)

```bash
# 1. å…‹éš†æˆ–è¿›å…¥é¡¹ç›®ç›®å½•
cd c:\Dev\Landuse_Zhong_clean

# 2. åˆ›å»ºcondaç¯å¢ƒ
conda env create -f geo.yml
conda activate geo

# 3. éªŒè¯å®‰è£…
python -c "import xarray; import geopandas; import tensorflow; print('âœ… All packages installed')"
```

### Step 1: æ•°æ®å‡†å¤‡æ¼”ç¤º (5åˆ†é’Ÿ)

```bash
# å¯åŠ¨Jupyter
jupyter notebook

# æ‰“å¼€ä»¥ä¸‹notebookè¿›è¡Œå¿«é€Ÿæµ‹è¯•:
# 0.0 PV_dataset.ipynb
```

**åœ¨notebookä¸­è¿è¡Œ**:
```python
import pandas as pd
import geopandas as gpd

# æ£€æŸ¥æ•°æ®ç»“æ„
# è¿™ä¸€æ­¥ä»…æ£€æŸ¥æ•°æ®æ ¼å¼,ä¸æ‰§è¡Œå®Œæ•´æµç¨‹
print("âœ… æ•°æ®åŠ è½½æµ‹è¯•å®Œæˆ")
```

### Step 2: å¿«é€Ÿè®­ç»ƒç¤ºä¾‹ (5åˆ†é’Ÿ)

```python
# åœ¨ 3.0 pre-training.ipynb ä¸­è¿è¡Œå°æ ·æœ¬æµ‹è¯•

from function import *

# ä½¿ç”¨500ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿæµ‹è¯•
df_positive_sample = df_positive.sample(500, random_state=42)
df_prediction_sample = df_prediction.sample(1000, random_state=42)

# è¿è¡Œå¿«é€Ÿè®­ç»ƒ
results = run_correct_training_pipeline(
    df_positive_sample, 
    df_prediction_sample,
    features_no_coords,
    epochs=10,  # å¿«é€Ÿæµ‹è¯•ä»…10ä¸ªepoch
    plot_learning_curve=True
)

print("âœ… æ¨¡å‹è®­ç»ƒæµ‹è¯•å®Œæˆ")
```

---

## ğŸ“ å®Œæ•´æµç¨‹è¿è¡ŒæŒ‡å—

### å‰ç½®æ¡ä»¶æ£€æŸ¥æ¸…å•

- [ ] **ç¡¬ä»¶**: è‡³å°‘16GB RAM, å»ºè®®32GB
- [ ] **å­˜å‚¨**: è‡³å°‘100GBå¯ç”¨ç©ºé—´
- [ ] **Python**: 3.8-3.10 (æ¨è3.9)
- [ ] **æ•°æ®**: å·²ä¸‹è½½å¿…éœ€çš„æ•°æ®é›†

### æ•°æ®ç›®å½•ç»“æ„å»ºè®®

```
D:\xarray\                          # ä¸»æ•°æ®ç›®å½• (å¯è‡ªå®šä¹‰)
â”œâ”€â”€ merged_chunk_2\                 # åºŸå¼ƒå†œç”°æ•°æ®
â”‚   â””â”€â”€ *.nc
â”œâ”€â”€ aligned2\
â”‚   â”œâ”€â”€ Feature_all\                # 15ä¸ªç¯å¢ƒç‰¹å¾
â”‚   â”‚   â””â”€â”€ *.nc
â”‚   â”œâ”€â”€ economic_cost\              # AR6ç»æµæ•°æ®
â”‚   â”‚   â””â”€â”€ national_growth_rate\
â”‚   â”‚       â”œâ”€â”€ AR6_Scenarios_*.csv
â”‚   â””â”€â”€ carbon\                     # ç¢³æ±‡æ•°æ®
â”‚       â””â”€â”€ *.nc
â””â”€â”€ output\                         # è¾“å‡ºç»“æœ
    â””â”€â”€ models\
```

---

## ğŸ¯ é˜¶æ®µæ€§è¿è¡ŒæŒ‡å—

### é˜¶æ®µä¸€: æ•°æ®é¢„å¤„ç† (é¢„è®¡4-6å°æ—¶)

#### 1.1 åºŸå¼ƒå†œç”°è¯†åˆ«
```bash
jupyter notebook "0.0 PV_dataset.ipynb"
```

**å…³é”®é…ç½®**:
```python
# ä¿®æ”¹æ•°æ®è·¯å¾„
PATHS = {
    'abandonment': r"ä½ çš„è·¯å¾„\merged_chunk_2\*.nc",
    'csv': "å¯¹é½åçš„æ•°æ®.csv"
}

# 5å¹´ç§»åŠ¨çª—å£å‚æ•°
WINDOW_SIZE = 5
MIN_DURATION = 5  # æœ€å°åºŸå¼ƒå¹´é™
```

**é¢„æœŸè¾“å‡º**:
- âœ… åºŸå¼ƒå†œç”°CSV (~4.7Mè¡Œ)
- âœ… ç©ºé—´åˆ†å¸ƒåœ°å›¾

#### 1.2 æ•°æ®å¯¹é½ä¸ç‰¹å¾æå–
```bash
# ä¾æ¬¡è¿è¡Œ
jupyter notebook "2.1 process_csv_for_aligning.ipynb"
jupyter notebook "2.2 process_csv_for_embedding.ipynb"
jupyter notebook "2.3 process_csv_for_prediction.ipynb"
```

**å…³é”®æ­¥éª¤**:
```python
# åœ¨ 2.2 ä¸­æå–15ç»´ç‰¹å¾
features_to_extract = [
    # ç‰©ç†åœ°ç†
    'DEM', 'Slope', 'land_cover', 'gdmp',
    # æ°”å€™
    'tas', 'wind', 'rsds',
    # ç¤¾ä¼šç»æµ
    'Population', 'GDPpc', 'GDPtot',
    'GURdist', 'Powerdist',
    'PrimaryRoad', 'SecondaryRoad', 'TertiaryRoad'
]
```

**éªŒè¯æ£€æŸ¥**:
```python
# æ£€æŸ¥ç‰¹å¾çŸ©é˜µ
print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {df.shape}")  # åº”è¯¥æ˜¯ (N, 15+å…¶ä»–åˆ—)
print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:\n{df.isnull().sum()}")  # åº”è¯¥æ²¡æœ‰æˆ–å¾ˆå°‘ç¼ºå¤±å€¼
```

---

### é˜¶æ®µäºŒ: ç¯å¢ƒé€‚å®œæ€§å»ºæ¨¡ (é¢„è®¡6-10å°æ—¶)

#### 2.1 GMMè®­ç»ƒä¸è´Ÿæ ·æœ¬ç”Ÿæˆ
```bash
jupyter notebook "3.0 pre-training.ipynb"
```

**æ ¸å¿ƒä»£ç **:
```python
from function import run_correct_training_pipeline

# å®Œæ•´è®­ç»ƒé…ç½®
results = run_correct_training_pipeline(
    df_positive=df_pv,                    # å…‰ä¼æ­£æ ·æœ¬
    df_prediction_pool=df_abandoned,      # åºŸå¼ƒå†œç”°
    features_no_coords=features_15,       # 15ä¸ªç‰¹å¾
    
    # è´Ÿæ ·æœ¬ç­–ç•¥
    negative_strategy='selection',        # 'selection'æˆ–'generation'
    negative_ratio=1.0,                   # æ­£è´Ÿæ¯”ä¾‹1:1
    sampling_strategy='pit_based',        # åŸºäºPITçš„åˆ†å±‚é‡‡æ ·
    difficulty_levels=3,                  # 3ä¸ªéš¾åº¦çº§åˆ«
    
    # è®­ç»ƒå‚æ•°
    model_type='transformer',             # 'transformer', 'mlp', æˆ– 'rf'
    epochs=50,
    batch_size=32,
    learning_rate=0.001,
    
    # æ¨¡å‹æ¶æ„
    transformer_config={
        'd_model': 64,
        'num_heads': 4,
        'num_layers': 2
    },
    resnet_layers=[128, 128, 64],
    
    # è¯Šæ–­
    plot_learning_curve=True,
    run_shap=True,                        # ç‰¹å¾é‡è¦æ€§åˆ†æ
    
    random_state=42
)
```

**ä¸­é—´æ£€æŸ¥**:
```python
# æ£€æŸ¥GMMç»“æœ
print(f"GMMç»„ä»¶æ•°: {results['gmm_pipeline'].named_steps['gmm'].n_components}")
print(f"BICå€¼: {results['gmm_pipeline'].named_steps['gmm'].bic(X)}")

# æ£€æŸ¥è´Ÿæ ·æœ¬è´¨é‡
neg_scores = results['negative_samples']['gmm_score']
print(f"è´Ÿæ ·æœ¬å¾—åˆ†èŒƒå›´: [{neg_scores.min():.3f}, {neg_scores.max():.3f}]")
```

**é¢„æœŸè¾“å‡º**:
- âœ… `gmm_model_23c_fixed.pkl` (~50MB)
- âœ… è®­ç»ƒå†å²æ›²çº¿å›¾
- âœ… SHAPç‰¹å¾é‡è¦æ€§å›¾
- âœ… F1 Score > 0.85

#### 2.2 æ¨¡å‹é¢„æµ‹
```python
# è·å–é¢„æµ‹ç»“æœ
predictions = results['prediction_results']
print(f"é¢„æµ‹æ¦‚ç‡å‡å€¼: {predictions['predicted_prob'].mean():.3f}")
print(f"é«˜é€‚å®œæ€§(>0.7)æ¯”ä¾‹: {(predictions['predicted_prob']>0.7).mean():.1%}")

# ä¿å­˜é¢„æµ‹ç»“æœ
predictions.to_csv('output/prediction_probability.csv', index=False)
```

---

### é˜¶æ®µä¸‰: ç¢³å‡æ’æ½œåŠ› (é¢„è®¡3-5å°æ—¶)

```bash
jupyter notebook "4.1 Emission_reduction_potential.ipynb"
```

**å…³é”®å‚æ•°**:
```python
# å…‰ä¼å‚æ•°
PV_PARAMS = {
    'efficiency': 0.17,           # kW/mÂ²
    'system_loss': 0.8,           # ç³»ç»Ÿæ•ˆç‡
    'lifetime': 30,               # å¹´
    'annual_hours': 8760          # å°æ—¶/å¹´
}

# LNCSç­–ç•¥æƒé‡(ä»å†å²æ•°æ®å­¦ä¹ )
lncs_weights = calculate_lncs_probability(
    df_abandoned, 
    strategy='knn_idw',
    k_neighbors=10
)
```

**è¾“å‡ºéªŒè¯**:
```python
# æ£€æŸ¥å‡æ’ç»“æœ
pv_mitigation = df['pv_carbon_total'].sum() / 1e9  # Gt CO2
lncs_mitigation = df['lncs_carbon_total'].sum() / 1e9
net_mitigation = pv_mitigation - lncs_mitigation

print(f"å…‰ä¼æ€»å‡æ’: {pv_mitigation:.2f} Gt COâ‚‚")
print(f"LNCSæ€»ç¢³æ±‡: {lncs_mitigation:.2f} Gt COâ‚‚")
print(f"å‡€å‡æ’: {net_mitigation:.2f} Gt COâ‚‚")

# åº”è¯¥æ¥è¿‘: å…‰ä¼~62.83, LNCS~3.91, å‡€~58.92
```

---

### é˜¶æ®µå››: ç»æµå¯è¡Œæ€§ (é¢„è®¡2-4å°æ—¶)

```bash
jupyter notebook "5.1 Economical_feasibility.ipynb"
```

**AR6æƒ…æ™¯åŠ è½½**:
```python
# è¯»å–AR6æ•°æ®
df_ar6 = pd.read_csv('AR6_Scenarios_Database_R10_regions_v1.1.csv')

# ç­›é€‰ç¾å›½æ•°æ®
df_us = df_ar6[df_ar6['Region'] == 'R10NORTH_AM']

# æå–å…³é”®å˜é‡
scenarios = ['P1a', 'P1b', 'P2a', 'P2b', 'P2c', 'P3a', 'P3b', 'P3c']
years = [2020, 2030, 2040, 2050]

# æå–ç”µä»·ã€æŠ•èµ„æˆæœ¬ã€è¿è¥æˆæœ¬
electricity_prices = extract_prices(df_us, scenarios, years)
investment_costs = extract_costs(df_us, scenarios, years, 'Capital')
operation_costs = extract_costs(df_us, scenarios, years, 'O&M')
```

**NPVè®¡ç®—**:
```python
def calculate_npv(row, scenario, discount_rate=0.05):
    """è®¡ç®—å•ä¸ªåƒå…ƒçš„NPV"""
    revenue = row['generation'] * electricity_prices[scenario]
    costs = investment_costs[scenario] + operation_costs[scenario]
    lncs_cost = row['lncs_opportunity_cost']
    
    npv = sum([
        (revenue[t] - costs[t]) / ((1 + discount_rate) ** t)
        for t in range(30)
    ]) - lncs_cost
    
    return npv

# åº”ç”¨åˆ°æ‰€æœ‰åƒå…ƒ
for scenario in scenarios:
    df[f'npv_{scenario}'] = df.apply(
        lambda row: calculate_npv(row, scenario), 
        axis=1
    )

# è®¡ç®—å‡å€¼
df['avg_npv'] = df[[f'npv_{s}' for s in scenarios]].mean(axis=1)
```

---

### é˜¶æ®µäº”: 3EååŒåˆ†æ (é¢„è®¡2-3å°æ—¶)

#### 5.1 3E-SynergyæŒ‡æ•°
```bash
jupyter notebook "6.4 3E_synergy_index.ipynb"
```

**å½’ä¸€åŒ–å¤„ç†**:
```python
from sklearn.preprocessing import MinMaxScaler

# ä¸‰ä¸ªç»´åº¦
e1_env = df['predicted_prob']              # ç¯å¢ƒ (å·²åœ¨0-1)
e2_emission = df['net_carbon_mitigation']  # å‡æ’ (éœ€å½’ä¸€åŒ–)
e3_economic = df['avg_npv']                # ç»æµ (éœ€å½’ä¸€åŒ–)

# å½’ä¸€åŒ–
scaler = MinMaxScaler()
df['E1'] = e1_env
df['E2'] = scaler.fit_transform(e2_emission.values.reshape(-1, 1))
df['E3'] = scaler.fit_transform(e3_economic.values.reshape(-1, 1))
```

**WCCDè®¡ç®—**:
```python
from scipy.optimize import minimize

def calculate_3e_synergy(row):
    """ä¸ºå•ä¸ªåƒå…ƒè®¡ç®—3E-synergy"""
    U = [row['E1'], row['E2'], row['E3']]
    
    def objective(w):
        """ä¼˜åŒ–ç›®æ ‡: æœ€å¤§åŒ–CCD"""
        C = np.prod([u**w[i] for i, u in enumerate(U)]) / (np.mean(U) ** 3)
        T = sum(w[i] * U[i] for i in range(3))
        return -(C * T) ** 0.5  # è´Ÿå·å› ä¸ºminimize
    
    # çº¦æŸ: Î£w=1, wâ‰¥0
    constraints = {'type': 'eq', 'fun': lambda w: sum(w) - 1}
    bounds = [(0, 1) for _ in range(3)]
    
    result = minimize(
        objective, 
        x0=[1/3, 1/3, 1/3],
        method='SLSQP',
        constraints=constraints,
        bounds=bounds
    )
    
    return -result.fun  # è¿”å›æœ€å¤§CCDå€¼

# åº”ç”¨åˆ°æ‰€æœ‰åƒå…ƒ
df['3e_synergy'] = df.apply(calculate_3e_synergy, axis=1)
```

#### 5.2 ä¼˜å…ˆçº§æ’åº
```bash
jupyter notebook "6.5 Figure2_priority_total.ipynb"
```

**å¯¹æ¯”åˆ†æ**:
```python
# å››ç§ç­–ç•¥
strategies = {
    'env_optimal': df.sort_values('E1', ascending=False),
    'emission_optimal': df.sort_values('E2', ascending=False),
    'economic_optimal': df.sort_values('E3', ascending=False),
    '3e_synergy': df.sort_values('3e_synergy', ascending=False)
}

# ç´¯ç§¯æ€§èƒ½
def cumulative_performance(df_sorted, target_area=0.1):
    """è®¡ç®—ç´¯ç§¯æ€§èƒ½"""
    n = int(len(df_sorted) * target_area)
    top_n = df_sorted.head(n)
    
    return {
        'env': top_n['E1'].mean(),
        'emission': top_n['E2'].sum(),
        'economic': top_n['E3'].sum()
    }

# å¯¹æ¯”
for name, df_sorted in strategies.items():
    perf = cumulative_performance(df_sorted, 0.1)
    print(f"{name}: E1={perf['env']:.3f}, E2={perf['emission']:.1f}Gt, E3=${perf['economic']:.0f}B")
```

---

### é˜¶æ®µå…­: å¤šå°ºåº¦åˆ†æ (é¢„è®¡2-3å°æ—¶)

#### 6.1 æ¢ç´¢æ€§æ•°æ®åˆ†æ(å¯é€‰)
```bash
jupyter notebook "6.1 EDA_data.ipynb"
```

#### 6.2 å›½å®¶å±‚é¢åˆ†æ
```bash
jupyter notebook "7.0 Analysis_Nation_level.ipynb"
```

#### 6.3 å·å±‚é¢åˆ†æ
```bash
jupyter notebook "7.1 Analysis_State_level.ipynb"
```

#### 6.4 å¤šç›®æ ‡ä¼˜åŒ–ä¸èƒ½æºéœ€æ±‚
```bash
jupyter notebook "8.0 Multi-objective.ipynb"
jupyter notebook "9.0 Energy_demand_adjust.ipynb"
```

#### 6.5 é™„å½•åˆ†æ
```bash
jupyter notebook "7.3 Appendix_figure.ipynb"
```

**å·çº§æ±‡æ€»**:
```python
# ç©ºé—´è¿æ¥
gdf = gpd.GeoDataFrame(
    df, 
    geometry=gpd.points_from_xy(df.lon, df.lat),
    crs='EPSG:4326'
)

# ä¸å·è¾¹ç•Œå åŠ 
us_states = gpd.read_file('data/us_states.shp')
gdf_with_state = gpd.sjoin(gdf, us_states, how='left')

# å·çº§æ±‡æ€»
state_summary = gdf_with_state.groupby('STATE_NAME').agg({
    'abandoned_area': 'sum',           # kha
    'pv_capacity': 'sum',              # GW
    'E1': 'mean',                      # ç¯å¢ƒ
    'net_carbon_mitigation': 'sum',    # Gt CO2
    'avg_npv': 'mean',                 # k USD/ha
    '3e_synergy': 'mean',              # ååŒæŒ‡æ•°
    'energy_demand': 'first'           # TWh
}).reset_index()

# èƒ½æºéœ€æ±‚æ»¡è¶³åº¦
state_summary['demand_met'] = (
    state_summary['pv_capacity'] * 8760 * 0.2 / 1000  # TWh
) / state_summary['energy_demand']

print(f"å¯100%æ»¡è¶³éœ€æ±‚çš„å·æ•°: {(state_summary['demand_met'] >= 1).sum()}")
```

---

---

## ğŸ¨ å¯è§†åŒ–å¿«é€Ÿç”Ÿæˆ

**æ³¨æ„**: å¯è§†åŒ–æ¨¡å—éœ€è¦å…ˆå®Œæˆå‰é¢çš„æ ¸å¿ƒè®¡ç®—æ­¥éª¤

### ä¸»å›¾å¿«é€Ÿç”Ÿæˆ
```bash
# Figure 1: ç¯å¢ƒé€‚å®œæ€§ç©ºé—´åˆ†å¸ƒ
jupyter notebook "6.6 Figure1_Enviromental_plot.ipynb"

# Figure 2: æ”¿ç­–æƒ…æ™¯çŸ©é˜µä¸ä¼˜å…ˆçº§
jupyter notebook "6.7 Figure2_Policy_matrix.ipynb"
jupyter notebook "6.5 Figure2_priority_total.ipynb"

# Figure 3: å…‰ä¼vs LNCSç¢³å‡æ’å¯¹æ¯”
jupyter notebook "6.8 Figure3_Carbon_LNCS.ipynb"

# Figure 4: ç´¯ç§¯æ”¶ç›Šæ›²çº¿
jupyter notebook "6.9 Figure4_Cumulative_pirority.ipynb"
```

**æ‰§è¡Œé¡ºåºå»ºè®®**:
1. å…ˆå®Œæˆé˜¶æ®µ1-5çš„æ ¸å¿ƒè®¡ç®—
2. ç¡®ä¿ç”Ÿæˆäº†æ‰€æœ‰ä¸­é—´ç»“æœæ–‡ä»¶
3. å†è¿è¡Œå¯è§†åŒ–notebookç”Ÿæˆå›¾è¡¨

### åŸºç¡€ç»˜å›¾æ¨¡æ¿
```python
import matplotlib.pyplot as plt
import seaborn as sns

# ç©ºé—´åˆ†å¸ƒå›¾
fig, ax = plt.subplots(figsize=(12, 8))
gdf.plot(
    column='3e_synergy',
    cmap='RdYlGn',
    legend=True,
    ax=ax,
    vmin=0, vmax=1
)
us_states.boundary.plot(ax=ax, linewidth=0.5, edgecolor='black')
ax.set_title('3E-Synergy Index Spatial Distribution')
plt.savefig('figure/3e_synergy_map.pdf', dpi=300, bbox_inches='tight')

# ç´¯ç§¯æ›²çº¿
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
for i, (dim, label) in enumerate([('E1', 'Environment'), 
                                    ('E2', 'Emission'), 
                                    ('E3', 'Economic')]):
    for name, df_sorted in strategies.items():
        cumsum = df_sorted[dim].cumsum() / df_sorted[dim].sum()
        axes[i].plot(
            np.linspace(0, 1, len(cumsum)),
            cumsum,
            label=name
        )
    axes[i].set_title(label)
    axes[i].legend()
plt.savefig('figure/cumulative_curves.pdf', dpi=300, bbox_inches='tight')
```

---

## ğŸ’¾ ç»“æœä¿å­˜å»ºè®®

```python
# 1. é¢„æµ‹ç»“æœ
df_results = df[[
    'lon', 'lat', 'STATE_NAME',
    'predicted_prob', 'net_carbon_mitigation', 'avg_npv',
    'E1', 'E2', 'E3', '3e_synergy'
]]
df_results.to_csv('output/final_results.csv', index=False)

# 2. å·çº§æ±‡æ€»
state_summary.to_csv('output/state_summary.csv', index=False)

# 3. æ¨¡å‹æ–‡ä»¶
results['model'].save('output/models/transformer_model.h5')
joblib.dump(results['gmm_pipeline'], 'output/models/gmm_pipeline.pkl')

# 4. æ …æ ¼è¾“å‡º (å¯é€‰)
from rasterio.transform import from_origin

# è½¬ä¸ºæ …æ ¼
raster = df_results.pivot(index='lat', columns='lon', values='3e_synergy')
with rasterio.open(
    'output/3e_synergy.tif', 'w',
    driver='GTiff',
    height=raster.shape[0],
    width=raster.shape[1],
    count=1,
    dtype=raster.values.dtype,
    crs='EPSG:4326',
    transform=from_origin(raster.columns.min(), raster.index.max(), 0.00833, 0.00833)
) as dst:
    dst.write(raster.values, 1)
```

---

## ğŸ› å¸¸è§é”™è¯¯é€ŸæŸ¥

### é”™è¯¯1: "ModuleNotFoundError: No module named 'function'"
```bash
# è§£å†³: ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
cd c:\Dev\Landuse_Zhong_clean
python -c "import function; print(function.__file__)"
```

### é”™è¯¯2: "KeyError: 'predicted_prob'"
```bash
# è§£å†³: æ£€æŸ¥æ˜¯å¦å®Œæˆé˜¶æ®µäºŒè®­ç»ƒ
# é¢„æµ‹ç»“æœåº”è¯¥åŒ…å«'predicted_prob'åˆ—
```

### é”™è¯¯3: "MemoryError"
```python
# è§£å†³: åˆ†æ‰¹å¤„ç†
chunk_size = 10000
for i in range(0, len(df), chunk_size):
    chunk = df.iloc[i:i+chunk_size]
    process(chunk)
```

### é”™è¯¯4: "ValueError: operands could not be broadcast"
```python
# è§£å†³: æ£€æŸ¥æ•°ç»„å½¢çŠ¶
print(f"æ•°ç»„å½¢çŠ¶: {arr.shape}")
arr = arr.reshape(-1, 1)  # è½¬ä¸ºåˆ—å‘é‡
```

---

## ğŸ“š è¿›é˜¶å­¦ä¹ èµ„æº

1. **è®ºæ–‡åŸæ–‡**: `ã€2020-Policy-informed priority...ã€‘.md`
2. **æ–¹æ³•è®ºæ–‡æ¡£**: `docs/REGRESSION_ANALYSIS_COMPREHENSIVE_GUIDE.md`
3. **GPUç¯å¢ƒ**: `docs/HOW_TO_USE_BAYES_GPU_KERNEL.md`
4. **æ¨¡å‹ä¿å­˜**: `docs/SAVE_MODEL_PARAMS.md`

---

## ğŸ“ è·å–å¸®åŠ©

- **æ–‡æ¡£**: æŸ¥çœ‹ `README.md` å’Œ `PIPELINE_VISUALIZATION.md`
- **ä»£ç æ³¨é‡Š**: æ‰€æœ‰functionæ¨¡å—éƒ½æœ‰è¯¦ç»†docstring
- **Issues**: é¡¹ç›®issue tracker (å¦‚æœ‰)

---

**ç¥ä½ ç ”ç©¶é¡ºåˆ©! ğŸ‰**
