# -*- coding: utf-8 -*-
"""
å­¦ä¹ æ›²çº¿æ¨¡å—
åŒ…å«ç¥ç»ç½‘ç»œå­¦ä¹ æ›²çº¿åˆ†æåŠŸèƒ½ï¼ˆæ‰‹å†™CVå¾ªç¯ç‰ˆæœ¬ï¼Œé¿å…sklearnç±»å‹æ£€æµ‹é—®é¢˜ï¼‰
"""

from __future__ import annotations

import inspect
import platform
import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.base import clone
from sklearn.model_selection import StratifiedKFold
from sklearn.pipeline import Pipeline as SkPipeline
from sklearn.metrics import f1_score, roc_auc_score

# scikeras å»¶è¿Ÿå¯¼å…¥ï¼ˆåœ¨ TensorFlow æˆåŠŸå¯¼å…¥åï¼Œé¿å… keras.api å…¼å®¹æ€§é—®é¢˜ï¼‰
SCIKERAS_AVAILABLE = False
KerasClassifier = None

def _ensure_scikeras():
    """ç¡®ä¿ scikeras å·²å¯¼å…¥ï¼Œå¤„ç† keras.api å…¼å®¹æ€§é—®é¢˜"""
    global SCIKERAS_AVAILABLE, KerasClassifier
    if SCIKERAS_AVAILABLE and KerasClassifier is not None:
        return True
    
    try:
        # å»¶è¿Ÿå¯¼å…¥ scikeras
        from scikeras.wrappers import KerasClassifier as _KerasClassifier
        KerasClassifier = _KerasClassifier
        SCIKERAS_AVAILABLE = True
        return True
    except ModuleNotFoundError as e:
        if 'keras.api' in str(e):
            print(f"âš ï¸ scikeras version incompatible with TensorFlow 2.11: {e}")
            print("ğŸ’¡ Try upgrading scikeras: pip install --upgrade scikeras>=0.12.0")
        SCIKERAS_AVAILABLE = False
        return False
    except ImportError:
        SCIKERAS_AVAILABLE = False
        print("âš ï¸ scikeras not available (pip install scikeras)")
        return False
    except Exception as e:
        SCIKERAS_AVAILABLE = False
        print(f"âš ï¸ scikeras import failed: {type(e).__name__}: {e}")
        return False


def compute_learning_curve(
    build_model_fn,
    X_raw: pd.DataFrame,
    y: np.ndarray,
    features_no_coords: list,
    preprocessor_class=None,
    preprocessor_instance=None,
    train_sizes=np.linspace(0.2, 1.0, 5),
    cv_splits: int = 5,
    epochs: int = 30,
    batch_size: int = 64,
    learning_rate: float = 0.001,
    hidden_layers=[128, 64, 32],
    dropout_rate: float = 0.3,
    scoring: str = "f1",
    random_state: int = 42,
    transformer_config=None,
    resnet_layers=None,
):
    """
    è®¡ç®—å­¦ä¹ æ›²çº¿æ•°æ®ï¼ˆæ‰‹å†™CVå¾ªç¯ç‰ˆæœ¬ï¼Œé¿å…sklearnç±»å‹æ£€æµ‹é—®é¢˜ï¼‰
    
    Parameters:
    -----------
    build_model_fn : callable
        æ„å»ºæ¨¡å‹çš„å‡½æ•°
    X_raw : pd.DataFrame
        åŸå§‹ç‰¹å¾æ•°æ®ï¼ˆDataFrameæ ¼å¼ï¼‰
    y : np.ndarray
        æ ‡ç­¾æ•°ç»„
    features_no_coords : list
        ç‰¹å¾åˆ—ååˆ—è¡¨ï¼ˆä¸å«åæ ‡ï¼‰
    preprocessor_class : class, optional
        é¢„å¤„ç†å™¨ç±»
    preprocessor_instance : object, optional
        é¢„å¤„ç†å™¨å®ä¾‹
    train_sizes : array-like
        è®­ç»ƒé›†å¤§å°ï¼ˆæ¯”ä¾‹æˆ–ç»å¯¹æ•°é‡ï¼‰
    cv_splits : int
        äº¤å‰éªŒè¯æŠ˜æ•°
    epochs : int
        è®­ç»ƒè½®æ•°
    batch_size : int
        æ‰¹æ¬¡å¤§å°
    learning_rate : float
        å­¦ä¹ ç‡
    hidden_layers : list
        éšè—å±‚é…ç½®ï¼ˆç”¨äºMLPï¼‰
    dropout_rate : float
        Dropoutç‡
    scoring : str
        è¯„åˆ†æŒ‡æ ‡ï¼ˆ'f1' æˆ– 'roc_auc'ï¼‰
    random_state : int
        éšæœºç§å­
    transformer_config : dict, optional
        Transformeré…ç½®ï¼ˆç”¨äºTransformeræ¨¡å‹ï¼‰
    resnet_layers : list, optional
        ResNetå±‚é…ç½®ï¼ˆç”¨äºTransformeræ¨¡å‹ï¼‰
        
    Returns:
    --------
    dict: å­¦ä¹ æ›²çº¿æ•°æ®å­—å…¸
    """
    # ç¡®ä¿ scikeras å¯ç”¨
    if not _ensure_scikeras():
        raise ImportError("scikeras not available, cannot compute learning curve. "
                         "Please ensure scikeras>=0.12.0 is installed for TensorFlow 2.11 compatibility.")

    print("=" * 60)
    print("è®¡ç®—å­¦ä¹ æ›²çº¿æ•°æ®ï¼ˆæ‰‹å†™CVå¾ªç¯ç‰ˆæœ¬ï¼‰")
    print("=" * 60)

    # -------- è‡ªå®šä¹‰ scorer å·¥å‚ï¼Œç»•è¿‡ sklearn çš„è‡ªåŠ¨ response_method é€»è¾‘ --------
    def make_nn_scorer(scoring="f1"):
        """
        è¿”å›ä¸€ä¸ªä¸ä¾èµ– estimator._estimator_type çš„ scorer å‡½æ•°ï¼š
        - å¯¹äº F1ï¼šå…è®¸é¢„æµ‹ä¸ºæ¦‚ç‡æˆ–ç±»åˆ«ï¼Œè‡ªåŠ¨åš 0.5 é˜ˆå€¼å¤„ç†
        - å¯¹äº ROC-AUCï¼šä¼˜å…ˆä½¿ç”¨ predict_probaï¼Œå¦åˆ™ç”¨ predict çš„è¿ç»­è¾“å‡º
        """
        if scoring == "f1":
            def scorer(estimator, X, y_true):
                # âœ… ç›´æ¥è°ƒç”¨ predictï¼Œä¸ä¾èµ– sklearn çš„è‡ªåŠ¨é€‰æ‹©
                y_pred = estimator.predict(X)
                
                # æœ‰äº› KerasClassifier ä¼šè¾“å‡º (n, 1)ï¼Œå…ˆæ‹å¹³
                if hasattr(y_pred, "ndim") and y_pred.ndim > 1:
                    y_pred = y_pred.ravel()
                
                y_pred = np.asarray(y_pred)
                
                # å¦‚æœæ˜¯ float ç±»å‹ï¼Œå½“ä½œæ¦‚ç‡ â†’ é˜ˆå€¼ 0.5
                if y_pred.dtype.kind in "fc":
                    y_pred = (y_pred >= 0.5).astype(int)
                
                return f1_score(y_true, y_pred)
            
        elif scoring == "roc_auc":
            def scorer(estimator, X, y_true):
                # âœ… ä¼˜å…ˆå°è¯• predict_probaï¼ˆå¦‚æœå¯ç”¨ï¼‰
                try:
                    if hasattr(estimator, "predict_proba"):
                        proba = estimator.predict_proba(X)
                        # å…¼å®¹ (n, 2) æˆ– (n, 1) çš„è¾“å‡º
                        proba = np.asarray(proba)
                        if proba.ndim > 1:
                            proba = proba[:, -1]  # å–æœ€åä¸€åˆ—ï¼ˆæ­£ç±»æ¦‚ç‡ï¼‰
                        return roc_auc_score(y_true, proba)
                except (AttributeError, ValueError, TypeError):
                    # å¦‚æœ predict_proba å¤±è´¥ï¼Œfallback åˆ° predict
                    pass
                
                # âœ… fallbackï¼šç”¨ predict çš„è¿ç»­è¾“å‡ºï¼ˆæ¦‚ç‡å€¼ï¼‰
                y_score = estimator.predict(X)
                y_score = np.asarray(y_score)
                if y_score.ndim > 1:
                    y_score = y_score.ravel()
                return roc_auc_score(y_true, y_score)
        else:
            raise ValueError(f"æš‚ä¸æ”¯æŒ scoring='{scoring}'ï¼Œå»ºè®®ç”¨ 'f1' æˆ– 'roc_auc'ã€‚")
        
        return scorer

    if preprocessor_class is None and preprocessor_instance is None:
        raise ValueError("å¿…é¡»æä¾›é¢„å¤„ç†å™¨ç±»æˆ–å®ä¾‹")

    if len(X_raw) != len(y):
        raise ValueError("X_raw å’Œ y çš„é•¿åº¦ä¸åŒ¹é…")

    pos_ratio = y.mean()
    if pos_ratio < 0.1 or pos_ratio > 0.9:
        print(f"âš ï¸ æ­£æ ·æœ¬æ¯”ä¾‹å¼‚å¸¸: {pos_ratio:.3f}ï¼Œå¯èƒ½å½±å“å­¦ä¹ æ›²çº¿åˆ†æ")

    print("æ¨æ–­é¢„å¤„ç†åçš„ç‰¹å¾ç»´åº¦...")
    if preprocessor_instance is not None:
        temp_preprocessor = clone(preprocessor_instance)
    else:
        temp_preprocessor = preprocessor_class()
    
    temp_preprocessor.fit(X_raw[features_no_coords])
    X_sample = temp_preprocessor.transform(X_raw[features_no_coords].head(100))
    input_dim = X_sample.shape[1]
    
    print(f"âœ… æ¨æ–­çš„è¾“å…¥ç»´åº¦: {input_dim}")
    print(f"âœ… é¢„æœŸç‰¹å¾æ„æˆ: 14ä¸ªæ•°å€¼ç‰¹å¾ + 9ä¸ªlandcover One-Hot = 23ä¸ªæ€»ç‰¹å¾")

    # ------ pipeline æ„é€ ï¼ˆä¿æŒä½ å†™çš„é€»è¾‘ï¼‰------
    def make_complete_pipeline():
        def make_model():
            import inspect
            sig = inspect.signature(build_model_fn)
            params = sig.parameters

            model_kwargs = {
                "input_dim": input_dim,
                "dropout_rate": dropout_rate,
                "learning_rate": learning_rate,
            }

            if ("resnet_layers" in params) or ("d_model" in params):
                if resnet_layers is not None and "resnet_layers" in params:
                    model_kwargs["resnet_layers"] = resnet_layers

                if transformer_config is not None:
                    if "d_model" in params:
                        model_kwargs["d_model"] = transformer_config.get("d_model", 64)
                    if "num_heads" in params:
                        model_kwargs["num_heads"] = transformer_config.get("num_heads", 4)
                    if "num_transformer_layers" in params:
                        model_kwargs["num_transformer_layers"] = transformer_config.get("num_layers", 2)
            elif "hidden_layers" in params:
                model_kwargs["hidden_layers"] = hidden_layers

            return build_model_fn(**model_kwargs)  # å†…éƒ¨å·² compile()

        # âœ… ç²¾ç®€ç‰ˆ KerasClassifierï¼šä¸ä¼  loss/optimizerï¼Œä¸æ‰‹åŠ¨æ”¹ _estimator_type
        # å› ä¸ºæ¨¡å‹å†…éƒ¨å·²ç» compile äº†
        clf = KerasClassifier(
            model=make_model,
            epochs=epochs,
            batch_size=batch_size,
            verbose=0,
            random_state=random_state
        )

        if preprocessor_instance is not None:
            pipeline = SkPipeline([
                ("preprocessor", clone(preprocessor_instance)),
                ("classifier", clf),
            ])
        else:
            pipeline = SkPipeline([
                ("preprocessor", preprocessor_class()),
                ("classifier", clf),
            ])
        return pipeline

    try:
        print(f"å¼€å§‹æ‰‹å·¥å­¦ä¹ æ›²çº¿è®¡ç®—...")
        print(f"  æ•°æ®é›†å¤§å°: {len(X_raw)}")
        print(f"  CVæŠ˜æ•°: {cv_splits}")
        print(f"  åŸå§‹ train_sizes: {train_sizes}")
        print(f"  è¯„åˆ†æŒ‡æ ‡: {scoring}")

        n_samples = len(X_raw)
        train_sizes = np.array(train_sizes)

        # æ”¯æŒ [0.2, 0.5, 1.0] è¿™ç§æ¯”ä¾‹å½¢å¼
        if train_sizes.dtype.kind in "fc":
            train_sizes_abs = (train_sizes * n_samples).astype(int)
        else:
            train_sizes_abs = train_sizes.astype(int)

        # è¾¹ç•Œå¤„ç†
        train_sizes_abs = np.clip(train_sizes_abs, 2, n_samples - 1)
        train_sizes_abs = np.unique(train_sizes_abs)

        print(f"  å®é™…ä½¿ç”¨çš„è®­ç»ƒé›†å¤§å°: {train_sizes_abs}")

        cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=random_state)
        scorer_fn = make_nn_scorer(scoring)

        # ç»“æœçŸ©é˜µ: (n_train_sizes, n_cv_splits)
        train_scores = np.zeros((len(train_sizes_abs), cv_splits))
        val_scores = np.zeros((len(train_sizes_abs), cv_splits))

        # âœ… ä¿æŒ DataFrame æ ¼å¼ï¼Œä½¿ç”¨ iloc ç´¢å¼•
        X_all = X_raw[features_no_coords]  # ä¿æŒ DataFrame
        y_all = np.asarray(y)

        for i, n_train in enumerate(train_sizes_abs):
            print(f"\nğŸ”¹ è®­ç»ƒé›†å¤§å° {n_train} / {n_samples}")

            for fold, (train_idx, val_idx) in enumerate(cv.split(X_all, y_all)):
                if n_train > len(train_idx):
                    # æç«¯æƒ…å†µä¸‹ï¼Œclip ä¸€ä¸‹
                    this_train_idx = train_idx
                else:
                    this_train_idx = train_idx[:n_train]

                # âœ… ä½¿ç”¨ DataFrame çš„ iloc è€Œä¸æ˜¯ numpy ç´¢å¼•
                X_train = X_all.iloc[this_train_idx]
                y_train = y_all[this_train_idx]
                X_val = X_all.iloc[val_idx]
                y_val = y_all[val_idx]

                # âœ… åˆ›å»ºæ–°çš„ pipeline å®ä¾‹
                pipeline = make_complete_pipeline()
                
                # âœ… è®­ç»ƒ pipeline
                pipeline.fit(X_train, y_train)

                # âœ… ä½¿ç”¨è‡ªå®šä¹‰ scorer è®¡ç®—åˆ†æ•°
                train_scores[i, fold] = scorer_fn(pipeline, X_train, y_train)
                val_scores[i, fold] = scorer_fn(pipeline, X_val, y_val)

                print(f"    Fold {fold+1}/{cv_splits} | "
                      f"train_score={train_scores[i,fold]:.3f}, "
                      f"val_score={val_scores[i,fold]:.3f}")

        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        train_mean = train_scores.mean(axis=1)
        train_std = train_scores.std(axis=1)
        val_mean = val_scores.mean(axis=1)
        val_std = val_scores.std(axis=1)

        print(f"\nâœ… å­¦ä¹ æ›²çº¿è®¡ç®—å®Œæˆ")
        print(f"  è®­ç»ƒé›†å¤§å°: {train_sizes_abs.shape}")
        print(f"  è®­ç»ƒå¾—åˆ†å½¢çŠ¶: {train_scores.shape}")
        print(f"  éªŒè¯å¾—åˆ†å½¢çŠ¶: {val_scores.shape}")

        result = {
            'train_sizes': train_sizes_abs,
            'train_scores': train_scores,
            'val_scores': val_scores,
            'train_scores_mean': train_mean,
            'train_scores_std': train_std,
            'val_scores_mean': val_mean,
            'val_scores_std': val_std,
            'input_dim': input_dim,
            'cv_config': {
                'n_splits': cv_splits,
                'scoring': scoring,
                'random_state': random_state,
                'n_train_sizes': len(train_sizes_abs),
                'train_sizes_relative': train_sizes.tolist() if isinstance(train_sizes, np.ndarray) else train_sizes,
            },
            'model_config': {
                'hidden_layers': hidden_layers,
                'dropout_rate': dropout_rate,
                'learning_rate': learning_rate,
                'epochs': epochs,
                'batch_size': batch_size
            }
        }
        return result

    except Exception as e:
        print(f"âŒ å­¦ä¹ æ›²çº¿è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def plot_learning_curve(
    lc_data: dict,
    scoring: str = "f1",
    dropout_rate: float = 0.3,
    show_plot: bool = True,
    save_path: str = None
):
    """
    ç»˜åˆ¶å­¦ä¹ æ›²çº¿ï¼ˆç»˜å›¾éƒ¨åˆ†ï¼‰
    
    Parameters:
    -----------
    lc_data : dict
        ç”± compute_learning_curve è¿”å›çš„æ•°æ®å­—å…¸
    scoring : str
        è¯„åˆ†æŒ‡æ ‡åç§°ï¼ˆç”¨äºæ ‡ç­¾ï¼‰
    dropout_rate : float
        Dropoutç‡ï¼ˆç”¨äºå»ºè®®ï¼‰
    show_plot : bool
        æ˜¯å¦æ˜¾ç¤ºå›¾è¡¨
    save_path : str, optional
        ä¿å­˜è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneåˆ™ä¸ä¿å­˜ï¼‰
        
    Returns:
    --------
    dict: è¿‡æ‹Ÿåˆåˆ†æç»“æœ
    """
    import matplotlib as mpl
    
    # è®¾ç½®matplotlibæ ·å¼
    if platform.system() in ['Linux', 'Darwin']:
        mpl.rcParams['font.family'] = 'DejaVu Sans'
    else:
        mpl.rcParams['font.family'] = 'Arial'
    mpl.rcParams['axes.titlesize'] = 14
    mpl.rcParams['axes.labelsize'] = 12
    mpl.rcParams['xtick.labelsize'] = 10
    mpl.rcParams['ytick.labelsize'] = 10
    mpl.rcParams['legend.fontsize'] = 10

    print("=" * 60)
    print("ç»˜åˆ¶å­¦ä¹ æ›²çº¿")
    print("=" * 60)

    # æå–æ•°æ®
    train_sizes_abs = lc_data['train_sizes']
    train_mean = lc_data['train_scores_mean']
    train_std = lc_data['train_scores_std']
    val_mean = lc_data['val_scores_mean']
    val_std = lc_data['val_scores_std']

    def analyze_overfitting(train_mean, val_mean, train_std, val_std):
        """è¯¦ç»†åˆ†æè¿‡æ‹Ÿåˆæƒ…å†µ"""
        analysis = {}
        
        # 1. è®­ç»ƒ-éªŒè¯åˆ†æ•°å·®å¼‚åˆ†æ
        score_gaps = train_mean - val_mean
        final_gap = score_gaps[-1]
        max_gap = np.max(score_gaps)
        gap_trend = np.diff(score_gaps)
        
        # 2. Overfitting severity classification
        if final_gap <= 0.02:
            overfitting_level = "No overfitting"
            overfitting_color = "green"
        elif final_gap <= 0.05:
            overfitting_level = "Mild overfitting"
            overfitting_color = "yellow"
        elif final_gap <= 0.10:
            overfitting_level = "Moderate overfitting"
            overfitting_color = "orange"
        else:
            overfitting_level = "Severe overfitting"
            overfitting_color = "red"
        
        # 3. éªŒè¯æ›²çº¿è¶‹åŠ¿åˆ†æ
        val_trend = np.diff(val_mean)
        val_improving = np.sum(val_trend > 0) > np.sum(val_trend < 0)
        val_stable = np.abs(val_trend[-2:]).mean() < 0.01 if len(val_trend) >= 2 else False
        
        # 4. æ–¹å·®åˆ†æ
        train_variance = np.mean(train_std)
        val_variance = np.mean(val_std)
        high_variance = train_variance > 0.05 or val_variance > 0.05
        
        # 5. å­¦ä¹ æ•ˆç‡åˆ†æ
        initial_gap = score_gaps[0]
        learning_efficiency = (initial_gap - final_gap) / initial_gap if initial_gap > 0 else 0
        
        analysis.update({
            'final_gap': final_gap,
            'max_gap': max_gap,
            'gap_trend': gap_trend,
            'overfitting_level': overfitting_level,
            'overfitting_color': overfitting_color,
            'overfitting_detected': final_gap > 0.05,
            'val_improving': val_improving,
            'val_stable': val_stable,
            'high_variance': high_variance,
            'train_variance': train_variance,
            'val_variance': val_variance,
            'learning_efficiency': learning_efficiency,
            'recommendations': []
        })
        
        # 6. ç”Ÿæˆå»ºè®®
        if final_gap > 0.10:
            analysis['recommendations'].extend([
                f"å¢åŠ Dropoutç‡ (å½“å‰: {dropout_rate:.2f})",
                "å‡å°‘æ¨¡å‹å¤æ‚åº¦ (å‡å°‘éšè—å±‚èŠ‚ç‚¹)",
                "å¢åŠ æ­£åˆ™åŒ– (L1/L2)",
                "æ”¶é›†æ›´å¤šè®­ç»ƒæ•°æ®"
            ])
        elif final_gap > 0.05:
            analysis['recommendations'].extend([
                "é€‚å½“å¢åŠ Dropoutç‡",
                "è€ƒè™‘æ—©åœç­–ç•¥",
                "å¢åŠ è®­ç»ƒæ•°æ®é‡"
            ])
        
        if high_variance:
            analysis['recommendations'].append("å¢åŠ è®­ç»ƒæ•°æ®ä»¥é™ä½æ–¹å·®")
        
        if not val_improving and len(val_trend) >= 2:
            analysis['recommendations'].append("éªŒè¯æ€§èƒ½å·²é¥±å’Œï¼Œè€ƒè™‘è°ƒæ•´æ¨¡å‹æ¶æ„")
        
        return analysis
    
    # æ‰§è¡Œè¿‡æ‹Ÿåˆåˆ†æ
    overfitting_analysis = analyze_overfitting(train_mean, val_mean, train_std, val_std)
    
    # åˆ›å»ºå¢å¼ºç‰ˆå­¦ä¹ æ›²çº¿å›¾ï¼ˆ2x2å¸ƒå±€ï¼‰
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. Main Learning Curve
    ax1 = axes[0, 0]
    ax1.plot(train_sizes_abs, train_mean, 'o-', color='blue', label='Training Score', linewidth=2)
    ax1.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')
    ax1.plot(train_sizes_abs, val_mean, 'o-', color='red', label='Validation Score', linewidth=2)
    ax1.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.2, color='red')
    
    ax1.set_xlabel('Training Set Size')
    ax1.set_ylabel(f'{scoring.upper()} Score')
    ax1.set_title(f'Learning Curve - {scoring.upper()}')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Overfitting level annotation
    ax1.text(0.02, 0.98, f'Overfitting Level: {overfitting_analysis["overfitting_level"]}',
            transform=ax1.transAxes, fontsize=12, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor=overfitting_analysis["overfitting_color"], alpha=0.3))
    
    # 2. Train-Validation Score Gap Plot
    ax2 = axes[0, 1]
    score_gaps = train_mean - val_mean
    ax2.plot(train_sizes_abs, score_gaps, 'o-', color='purple', linewidth=2, label='Train-Val Gap')
    ax2.fill_between(train_sizes_abs, 0, score_gaps, alpha=0.3, color='purple')
    ax2.axhline(y=0.05, color='orange', linestyle='--', alpha=0.7, label='Mild Overfitting Threshold')
    ax2.axhline(y=0.10, color='red', linestyle='--', alpha=0.7, label='Severe Overfitting Threshold')
    
    ax2.set_xlabel('Training Set Size')
    ax2.set_ylabel('Training - Validation Score')
    ax2.set_title('Overfitting Analysis (Score Gap)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Variance Analysis Plot
    ax3 = axes[1, 0]
    ax3.plot(train_sizes_abs, train_std, 'o-', color='blue', alpha=0.7, label='Training Variance')
    ax3.plot(train_sizes_abs, val_std, 'o-', color='red', alpha=0.7, label='Validation Variance')
    ax3.axhline(y=0.05, color='gray', linestyle='--', alpha=0.7, label='High Variance Threshold')
    
    ax3.set_xlabel('Training Set Size')
    ax3.set_ylabel('Score Standard Deviation')
    ax3.set_title('Variance Analysis')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. Performance Improvement Trend
    ax4 = axes[1, 1]
    train_improvement = np.diff(train_mean)
    val_improvement = np.diff(val_mean)
    
    if len(train_improvement) > 0:
        ax4.plot(train_sizes_abs[1:], train_improvement, 'o-', color='blue', alpha=0.7, label='Training Improvement')
        ax4.plot(train_sizes_abs[1:], val_improvement, 'o-', color='red', alpha=0.7, label='Validation Improvement')
        ax4.axhline(y=0, color='gray', linestyle='-', alpha=0.5)
        
        ax4.set_xlabel('Training Set Size')
        ax4.set_ylabel('Score Improvement')
        ax4.set_title('Performance Improvement Trend')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
    else:
        ax4.text(0.5, 0.5, 'Not enough data points\nfor trend analysis',
                ha='center', va='center', transform=ax4.transAxes)
        ax4.set_title('Performance Improvement Trend')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡ï¼ˆå¦‚æœæä¾›äº†ä¿å­˜è·¯å¾„ï¼‰
    if save_path is not None:
        os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)
        fig.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"âœ… å­¦ä¹ æ›²çº¿å›¾å·²ä¿å­˜: {save_path}")
        plt.close(fig)  # å…³é—­å›¾å½¢ä»¥é‡Šæ”¾å†…å­˜
    elif show_plot:
        plt.show()
    else:
        plt.close(fig)  # å¦‚æœä¸æ˜¾ç¤ºä¹Ÿä¸ä¿å­˜ï¼Œå…³é—­å›¾å½¢
    
    # æ‰“å°è¯¦ç»†åˆ†ææŠ¥å‘Š
    print(f"\n" + "=" * 60)
    print("ğŸ“Š è¯¦ç»†å­¦ä¹ æ›²çº¿åˆ†ææŠ¥å‘Š")
    print("=" * 60)
    
    print(f"\nğŸ¯ åŸºæœ¬æ€§èƒ½æŒ‡æ ‡:")
    print(f"  æœ€ç»ˆè®­ç»ƒåˆ†æ•°: {train_mean[-1]:.4f} (Â±{train_std[-1]:.4f})")
    print(f"  æœ€ç»ˆéªŒè¯åˆ†æ•°: {val_mean[-1]:.4f} (Â±{val_std[-1]:.4f})")
    print(f"  è®­ç»ƒ-éªŒè¯å·®å¼‚: {overfitting_analysis['final_gap']:.4f}")
    
    print(f"\nğŸ” è¿‡æ‹Ÿåˆåˆ†æ:")
    print(f"  è¿‡æ‹Ÿåˆç¨‹åº¦: {overfitting_analysis['overfitting_level']}")
    print(f"  æœ€å¤§åˆ†æ•°å·®å¼‚: {overfitting_analysis['max_gap']:.4f}")
    print(f"  å­¦ä¹ æ•ˆç‡: {overfitting_analysis['learning_efficiency']:.1%}")
    
    print(f"\nğŸ“ˆ è¶‹åŠ¿åˆ†æ:")
    if overfitting_analysis['val_improving']:
        print("  âœ… éªŒè¯æ€§èƒ½æŒç»­æ”¹å–„")
    else:
        print("  âš ï¸ éªŒè¯æ€§èƒ½æå‡æ”¾ç¼“")
        
    if overfitting_analysis['val_stable']:
        print("  ğŸ“Š éªŒè¯æ€§èƒ½è¶‹äºç¨³å®š")
    
    print(f"\nğŸ“Š æ–¹å·®åˆ†æ:")
    print(f"  è®­ç»ƒæ–¹å·®: {overfitting_analysis['train_variance']:.4f}")
    print(f"  éªŒè¯æ–¹å·®: {overfitting_analysis['val_variance']:.4f}")
    if overfitting_analysis['high_variance']:
        print("  âš ï¸ æ£€æµ‹åˆ°é«˜æ–¹å·®ï¼Œæ¨¡å‹ä¸å¤Ÿç¨³å®š")
    else:
        print("  âœ… æ–¹å·®é€‚ä¸­ï¼Œæ¨¡å‹ç›¸å¯¹ç¨³å®š")
    
    # å»ºè®®
    if overfitting_analysis['recommendations']:
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for i, rec in enumerate(overfitting_analysis['recommendations'], 1):
            print(f"  {i}. {rec}")
    else:
        print(f"\nâœ… æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œæ— éœ€ç‰¹æ®Šè°ƒæ•´")
    
    # æ€»ç»“
    print(f"\nğŸ“‹ æ€»ç»“:")
    if overfitting_analysis['overfitting_detected']:
        print("  âŒ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆï¼Œå»ºè®®æŒ‰ä¸Šè¿°å»ºè®®è°ƒæ•´æ¨¡å‹")
    else:
        print("  âœ… æ¨¡å‹æ‹Ÿåˆç¨‹åº¦è‰¯å¥½")
        
    if overfitting_analysis['high_variance']:
        print("  âš ï¸ æ¨¡å‹æ–¹å·®è¾ƒé«˜ï¼Œå»ºè®®å¢åŠ è®­ç»ƒæ•°æ®æˆ–æ­£åˆ™åŒ–")
    else:
        print("  âœ… æ¨¡å‹æ–¹å·®é€‚ä¸­")
    
    return overfitting_analysis


def save_learning_curve_results(
    lc_data: dict,
    overfitting_analysis: dict,
    save_path: str = None,
    model_name: str = "learning_curve"
):
    """
    ä¿å­˜å­¦ä¹ æ›²çº¿ç»“æœï¼ˆä¿å­˜éƒ¨åˆ†ï¼‰
    
    Parameters:
    -----------
    lc_data : dict
        ç”± compute_learning_curve è¿”å›çš„æ•°æ®å­—å…¸
    overfitting_analysis : dict
        ç”± plot_learning_curve è¿”å›çš„è¿‡æ‹Ÿåˆåˆ†æç»“æœ
    save_path : str, optional
        ä¿å­˜è·¯å¾„ï¼ˆç›®å½•ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä¿å­˜
    model_name : str
        æ¨¡å‹åç§°ï¼ˆç”¨äºæ–‡ä»¶åï¼‰
        
    Returns:
    --------
    dict: å®Œæ•´çš„ä¿å­˜ç»“æœå­—å…¸
    """
    print("=" * 60)
    print("ä¿å­˜å­¦ä¹ æ›²çº¿ç»“æœ")
    print("=" * 60)
    
    # æ„å»ºå®Œæ•´ç»“æœ
    complete_result = {
        # æ ¸å¿ƒCVæ•°æ®
        'train_sizes': lc_data['train_sizes'],
        'train_scores': lc_data['train_scores'],
        'val_scores': lc_data['val_scores'],
        
        # ç»Ÿè®¡æ±‡æ€»
        'train_scores_mean': lc_data['train_scores_mean'],
        'train_scores_std': lc_data['train_scores_std'],
        'val_scores_mean': lc_data['val_scores_mean'],
        'val_scores_std': lc_data['val_scores_std'],
        
        # é…ç½®ä¿¡æ¯
        'cv_config': lc_data['cv_config'],
        'model_config': lc_data['model_config'],
        'input_dim': lc_data['input_dim'],
        
        # è¿‡æ‹Ÿåˆåˆ†æ
        'overfitting_analysis': overfitting_analysis,
        
        # æ±‡æ€»æŒ‡æ ‡
        'final_performance': lc_data['val_scores_mean'][-1],
        'overfitting_detected': overfitting_analysis['overfitting_detected'],
        'high_variance': overfitting_analysis['high_variance'],
        
        # æ•°æ®å½¢çŠ¶è¯´æ˜
        'data_shapes': {
            'train_sizes_shape': lc_data['train_sizes'].shape,
            'train_scores_shape': lc_data['train_scores'].shape,
            'val_scores_shape': lc_data['val_scores'].shape,
            'train_mean_shape': lc_data['train_scores_mean'].shape,
            'val_mean_shape': lc_data['val_scores_mean'].shape
        },
        
        # å…ƒæ•°æ®
        'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
        'model_name': model_name
    }
    
    if save_path is not None:
        os.makedirs(save_path, exist_ok=True)
        timestamp = complete_result['timestamp']
        
        # ä¿å­˜ä¸ºJSONï¼ˆå¯åºåˆ—åŒ–ç‰ˆæœ¬ï¼‰
        json_result = {}
        for key, value in complete_result.items():
            if key in ['train_scores', 'val_scores', 'train_sizes']:
                # ä¿å­˜å®Œæ•´æ•°ç»„
                if isinstance(value, np.ndarray):
                    json_result[key] = value.tolist()
                else:
                    json_result[key] = value
            elif key in ['train_scores_mean', 'train_scores_std', 'val_scores_mean', 'val_scores_std']:
                # ä¿å­˜ç»Ÿè®¡æ±‡æ€»
                if isinstance(value, np.ndarray):
                    json_result[key] = value.tolist()
                else:
                    json_result[key] = value
            elif key == 'overfitting_analysis':
                # å¤„ç†è¿‡æ‹Ÿåˆåˆ†æ
                of_serializable = {}
                for k, v in value.items():
                    if isinstance(v, np.ndarray):
                        of_serializable[k] = v.tolist()
                    elif isinstance(v, (list, tuple)):
                        of_serializable[k] = [float(x) if isinstance(x, (np.floating, float)) else x for x in v]
                    elif isinstance(v, (np.floating, float, np.integer, int)):
                        of_serializable[k] = float(v)
                    else:
                        of_serializable[k] = v
                json_result[key] = of_serializable
            elif key == 'data_shapes':
                # å¤„ç†å½¢çŠ¶ä¿¡æ¯
                shapes_serializable = {}
                for k, v in value.items():
                    if isinstance(v, tuple):
                        shapes_serializable[k] = list(v)
                    else:
                        shapes_serializable[k] = v
                json_result[key] = shapes_serializable
            else:
                json_result[key] = value
        
        json_file = os.path.join(save_path, f"{model_name}_learning_curve_{timestamp}.json")
        with open(json_file, 'w') as f:
            json.dump(json_result, f, indent=2, default=str)
        
        print(f"âœ… å­¦ä¹ æ›²çº¿ç»“æœå·²ä¿å­˜:")
        print(f"  ğŸ“„ JSONæ–‡ä»¶: {json_file}")
    
    print(f"âœ… å­¦ä¹ æ›²çº¿ç»“æœå‡†å¤‡å®Œæˆ")
    return complete_result


def plot_learning_curve_nn(
    build_model_fn,
    X_raw: pd.DataFrame,
    y: np.ndarray,
    features_no_coords: list,
    preprocessor_class=None,
    preprocessor_instance=None,
    train_sizes=np.linspace(0.2, 1.0, 5),
    cv_splits: int = 5,
    epochs: int = 30,
    batch_size: int = 64,
    learning_rate: float = 0.001,
    hidden_layers=[128, 64, 32],
    dropout_rate: float = 0.3,
    scoring: str = "f1",
    random_state: int = 42,
    transformer_config=None,
    resnet_layers=None,
    save_path: str = None,
    model_name: str = "learning_curve"
):
    """
    ç”Ÿæˆç¥ç»ç½‘ç»œå­¦ä¹ æ›²çº¿ï¼ˆæ•´åˆç‰ˆæœ¬ï¼šè°ƒç”¨ä¸‰ä¸ªå­æ¨¡å—ï¼‰
    è¿™ä¸ªå‡½æ•°æ•´åˆäº†ï¼š
    1. compute_learning_curve - è®¡ç®—å­¦ä¹ æ›²çº¿æ•°æ®
    2. plot_learning_curve - ç»˜åˆ¶å­¦ä¹ æ›²çº¿
    3. save_learning_curve_results - ä¿å­˜ç»“æœ
    
    ä½¿ç”¨æ‰‹å†™CVå¾ªç¯ï¼Œé¿å…sklearnçš„learning_curveå‡½æ•°å¯¼è‡´çš„ç±»å‹æ£€æµ‹é—®é¢˜ã€‚
    """
    # 1. è®¡ç®—å­¦ä¹ æ›²çº¿æ•°æ®
    lc_data = compute_learning_curve(
        build_model_fn=build_model_fn,
        X_raw=X_raw,
        y=y,
        features_no_coords=features_no_coords,
        preprocessor_class=preprocessor_class,
        preprocessor_instance=preprocessor_instance,
        train_sizes=train_sizes,
        cv_splits=cv_splits,
        epochs=epochs,
        batch_size=batch_size,
        learning_rate=learning_rate,
        hidden_layers=hidden_layers,
        dropout_rate=dropout_rate,
        scoring=scoring,
        random_state=random_state,
        transformer_config=transformer_config,
        resnet_layers=resnet_layers
    )
    
    if lc_data is None:
        return None
    
    # 2. ç»˜åˆ¶å­¦ä¹ æ›²çº¿
    # å¦‚æœæä¾›äº†save_pathï¼Œæ„å»ºå›¾ç‰‡ä¿å­˜è·¯å¾„
    plot_save_path = None
    if save_path is not None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_save_path = os.path.join(save_path, f"{model_name}_learning_curve_{timestamp}.png")
    
    overfitting_analysis = plot_learning_curve(
        lc_data=lc_data,
        scoring=scoring,
        dropout_rate=dropout_rate,
        show_plot=True,
        save_path=plot_save_path
    )
    
    # 3. ä¿å­˜ç»“æœ
    complete_result = save_learning_curve_results(
        lc_data=lc_data,
        overfitting_analysis=overfitting_analysis,
        save_path=save_path,
        model_name=model_name
    )
    
    print(f"\nâœ… å¢å¼ºç‰ˆå­¦ä¹ æ›²çº¿åˆ†æå®Œæˆï¼")
    return complete_result
