# -*- coding: utf-8 -*-
"""
æ¨¡å—åŒ–è¯Šæ–­ç³»ç»Ÿ

è®¾è®¡ç†å¿µï¼š
- æ¯ä¸ªæ¨¡å‹æœ‰ç‹¬ç«‹çš„è¯Šæ–­å‡½æ•°
- å¯çµæ´»ç»„åˆå¯¹æ¯”ä¸åŒæ¨¡å‹
- æ”¯æŒç»Ÿä¸€è¯Šæ–­å…¥å£
- é›†æˆSHAPåˆ†è§£å’ŒPUå­¦ä¹ è¯„ä¼°

Author: 2024
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, List, Optional, Any, Tuple
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, confusion_matrix
)

try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("âš ï¸ SHAP not available, some features will be disabled")
except Exception as e:
    # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œä¸ä»…ä»…æ˜¯ ImportError
    SHAP_AVAILABLE = False
    print(f"âš ï¸ SHAP not available ({type(e).__name__}): {e}, some features will be disabled")


# ==========================================
# Level 1: ç‹¬ç«‹è¯Šæ–­å‡½æ•°ï¼ˆæ¨¡å‹ä¸“ç”¨ï¼‰
# ==========================================

def diagnose_transformer_model(
    model, X_test, y_test, feature_names,
    model_name="Transformer", show_plots=True):
    """
    Transformeræ¨¡å‹çš„ä¸“ç”¨è¯Šæ–­
    
    Parameters:
    -----------
    model: Keras/TensorFlowæ¨¡å‹
    X_test, y_test: æµ‹è¯•æ•°æ®
    feature_names: ç‰¹å¾åç§°åˆ—è¡¨
    model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
    show_plots: æ˜¯å¦æ˜¾ç¤ºå›¾è¡¨
    
    Returns:
    --------
    diagnostics: dict, åŒ…å«æ‰€æœ‰è¯Šæ–­æŒ‡æ ‡
    """
    print("=" * 60)
    print(f"è¯Šæ–­ {model_name} æ¨¡å‹")
    print("=" * 60)
    
    # 1. åŸºæœ¬æ€§èƒ½æŒ‡æ ‡
    y_pred_proba = model.predict(X_test, verbose=0).ravel()
    y_pred = (y_pred_proba > 0.5).astype(int)
    
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'auc': roc_auc_score(y_test, y_pred_proba)
    }
    
    print("\næ€§èƒ½æŒ‡æ ‡:")
    for key, value in metrics.items():
        print(f"  {key.upper():12s}: {value:.4f}")
    
    # 2. æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    
    # 3. å¯è§†åŒ–
    if show_plots:
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))
        
        # æ··æ·†çŸ©é˜µçƒ­å›¾
        im = axes[0].imshow(cm, cmap='Blues', aspect='auto')
        axes[0].set_title('Confusion Matrix')
        axes[0].set_xticks([0, 1])
        axes[0].set_yticks([0, 1])
        axes[0].set_xticklabels(['Pred: 0', 'Pred: 1'])
        axes[0].set_yticklabels(['True: 0', 'True: 1'])
        for i in range(2):
            for j in range(2):
                axes[0].text(j, i, str(cm[i, j]), ha='center', va='center', color='black', fontsize=12)
        plt.colorbar(im, ax=axes[0])
        
        # ROCæ›²çº¿
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        axes[1].plot(fpr, tpr, label=f'{model_name} (AUC={metrics["auc"]:.3f})', linewidth=2)
        axes[1].plot([0, 1], [0, 1], 'k--', label='Random')
        axes[1].set_xlabel('False Positive Rate')
        axes[1].set_ylabel('True Positive Rate')
        axes[1].set_title('ROC Curve')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    # 4. SHAPåˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
    shap_values = None
    if SHAP_AVAILABLE and show_plots:
        print("\nè®¡ç®—SHAPå€¼...")
        try:
            shap_values = _compute_shap_transformer(model, X_test, feature_names)
            print("  âœ… SHAPè®¡ç®—å®Œæˆ")
        except Exception as e:
            print(f"  âš ï¸ SHAPè®¡ç®—å¤±è´¥: {e}")
            shap_values = None
    
    # è¿”å›è¯Šæ–­ç»“æœ
    diagnostics = {
        'model_type': 'transformer',
        'metrics': metrics,
        'confusion_matrix': cm,
        'predictions': y_pred_proba,
        'y_pred': y_pred,
        'shap_values': shap_values
    }
    
    return diagnostics


def diagnose_mlp_model(
    model, X_test, y_test, feature_names,
    model_name="MLP", show_plots=True):
    """
    MLPæ¨¡å‹çš„ä¸“ç”¨è¯Šæ–­
    """
    return diagnose_transformer_model(model, X_test, y_test, feature_names, model_name, show_plots)


def diagnose_rf_model(
    model, X_test, y_test, feature_names,
    model_name="Random Forest", show_plots=True):
    """
    Random Forestæ¨¡å‹çš„ä¸“ç”¨è¯Šæ–­ï¼ˆå¢å¼ºç‰ˆï¼‰
    """
    print("=" * 60)
    print(f"è¯Šæ–­ {model_name} æ¨¡å‹")
    print("=" * 60)
    
    # 1. åŸºæœ¬æ€§èƒ½æŒ‡æ ‡
    y_pred_proba = model.predict(X_test).ravel()
    y_pred = (y_pred_proba > 0.5).astype(int)
    
    # åŸºç¡€åˆ†ç±»æŒ‡æ ‡
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, 
        roc_auc_score, log_loss, matthews_corrcoef, cohen_kappa_score, 
        balanced_accuracy_score
    )
    
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'auc': roc_auc_score(y_test, y_pred_proba)
    }
    
    # âœ… æ–°å¢ï¼šé¢å¤–çš„è¯Šæ–­æŒ‡æ ‡
    try:
        metrics['log_loss'] = log_loss(y_test, y_pred_proba)
    except Exception:
        metrics['log_loss'] = None
    
    try:
        metrics['mcc'] = matthews_corrcoef(y_test, y_pred)
    except Exception:
        metrics['mcc'] = None
    
    try:
        metrics['kappa'] = cohen_kappa_score(y_test, y_pred)
    except Exception:
        metrics['kappa'] = None
    
    try:
        metrics['balanced_accuracy'] = balanced_accuracy_score(y_test, y_pred)
    except Exception:
        metrics['balanced_accuracy'] = None
    
    print("\næ€§èƒ½æŒ‡æ ‡:")
    for key, value in metrics.items():
        if value is not None:
            print(f"  {key.upper():18s}: {value:.4f}")
        else:
            print(f"  {key.upper():18s}: N/A")
    
    # 2. ç‰¹å¾é‡è¦æ€§ï¼ˆRFç‰¹æœ‰çš„ï¼‰
    feature_importance = getattr(model.model, 'feature_importances_', None)
    
    if feature_importance is not None:
        total_importance = feature_importance.sum()
        metrics['total_feature_importance'] = float(total_importance)
        
        # Top 3 features contribution
        top_3_idx = np.argsort(feature_importance)[-3:][::-1]
        top_3_contrib = feature_importance[top_3_idx].sum() / total_importance
        metrics['top3_features_contribution'] = float(top_3_contrib)
        
        print(f"\nç‰¹å¾é‡è¦æ€§ç»Ÿè®¡:")
        print(f"  æ€»é‡è¦æ€§: {total_importance:.6f}")
        print(f"  Top-3ç‰¹å¾è´¡çŒ®åº¦: {top_3_contrib:.2%}")
        
        if show_plots:
            print("\nTop 10 æœ€é‡è¦ç‰¹å¾:")
            
            # âœ… ä¿®å¤ï¼šç¡®ä¿é•¿åº¦ä¸€è‡´
            min_len = min(len(feature_names), len(feature_importance))
            importance_df = pd.DataFrame({
                'feature': feature_names[:min_len],
                'importance': feature_importance[:min_len]
            }).sort_values('importance', ascending=False)
            
            for i, (_, row) in enumerate(importance_df.head(10).iterrows(), 1):
                print(f"  {i:2d}. {row['feature']:20s}: {row['importance']:.6f}")
    
    # 3. æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    
    print("\næ··æ·†çŸ©é˜µ:")
    print(f"  True Negative (TN):  {cm[0, 0]:5d}")
    print(f"  False Positive (FP): {cm[0, 1]:5d}")
    print(f"  False Negative (FN): {cm[1, 0]:5d}")
    print(f"  True Positive (TP):  {cm[1, 1]:5d}")
    
    # 4. å¯è§†åŒ–
    if show_plots:
        n_plots = 3 if feature_importance is not None else 2
        fig, axes = plt.subplots(1, n_plots, figsize=(6*n_plots, 4))
        if n_plots == 2:
            axes = [axes[0], axes[1]]
        
        # æ··æ·†çŸ©é˜µ
        im = axes[0].imshow(cm, cmap='Blues', aspect='auto')
        axes[0].set_title('Confusion Matrix')
        for i in range(2):
            for j in range(2):
                axes[0].text(j, i, str(cm[i, j]), ha='center', va='center', color='black')
        plt.colorbar(im, ax=axes[0])
        
        # ROCæ›²çº¿
        from sklearn.metrics import roc_curve
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        axes[1].plot(fpr, tpr, label=f'{model_name} (AUC={metrics["auc"]:.3f})', linewidth=2)
        axes[1].plot([0, 1], [0, 1], 'k--', label='Random')
        axes[1].set_xlabel('False Positive Rate')
        axes[1].set_ylabel('True Positive Rate')
        axes[1].set_title('ROC Curve')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        # ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæœ‰ï¼‰
        if feature_importance is not None and n_plots == 3:
            top_features = importance_df.head(10)
            axes[2].barh(range(len(top_features)), top_features['importance'].values)
            axes[2].set_yticks(range(len(top_features)))
            axes[2].set_yticklabels(top_features['feature'], fontsize=9)
            axes[2].set_xlabel('Importance')
            axes[2].set_title('Top 10 Feature Importance')
            axes[2].grid(True, alpha=0.3, axis='x')
        
        plt.tight_layout()
        plt.show()
    
    # 5. SHAPåˆ†æ
    shap_values = None
    if SHAP_AVAILABLE and show_plots:
        print("\nè®¡ç®—SHAPå€¼...")
        try:
            import shap
            explainer = shap.TreeExplainer(model.model)
            shap_values = explainer.shap_values(X_test[:min(100, len(X_test))])
            if isinstance(shap_values, list):
                shap_values = shap_values[1]  # äºŒåˆ†ç±»å–æ­£ç±»
            print("  âœ… SHAPè®¡ç®—å®Œæˆ")
        except Exception as e:
            print(f"  âš ï¸ SHAPè®¡ç®—å¤±è´¥: {e}")
    
    # âœ… è®¡ç®—æ ·æœ¬çº§åˆ«çš„ç»Ÿè®¡
    sample_stats = {
        'n_samples': len(y_test),
        'pos_proba_mean': float(y_pred_proba.mean()),
        'pos_proba_std': float(y_pred_proba.std()),
        'pos_proba_min': float(y_pred_proba.min()),
        'pos_proba_max': float(y_pred_proba.max()),
        'n_predicted_positive': int(y_pred.sum()),
        'n_predicted_negative': int((y_pred == 0).sum()),
        'positive_prediction_rate': float(y_pred.mean())
    }
    
    print("\né¢„æµ‹ç»Ÿè®¡:")
    print(f"  æµ‹è¯•æ ·æœ¬æ•°: {sample_stats['n_samples']}")
    print(f"  é¢„æµ‹æ¦‚ç‡å‡å€¼: {sample_stats['pos_proba_mean']:.4f} Â± {sample_stats['pos_proba_std']:.4f}")
    print(f"  é¢„æµ‹æ¦‚ç‡èŒƒå›´: [{sample_stats['pos_proba_min']:.4f}, {sample_stats['pos_proba_max']:.4f}]")
    print(f"  é¢„æµ‹ä¸ºæ­£ç±»æ•°: {sample_stats['n_predicted_positive']} ({sample_stats['positive_prediction_rate']:.2%})")
    print(f"  é¢„æµ‹ä¸ºè´Ÿç±»æ•°: {sample_stats['n_predicted_negative']}")
    
    diagnostics = {
        'model_type': 'random_forest',
        'metrics': metrics,
        'confusion_matrix': cm,
        'predictions': y_pred_proba,
        'y_pred': y_pred,
        'feature_importance': feature_importance,
        'shap_values': shap_values,
        'sample_stats': sample_stats
    }
    return diagnostics

# ==========================================
# Level 2: çµæ´»å¯¹æ¯”å‡½æ•°
# ==========================================

def compare_models(
    results: Dict[str, Dict],
    models: List[str] = None,
    show_detailed: bool = True):
    """
    çµæ´»å¯¹æ¯”æŒ‡å®šçš„æ¨¡å‹
    
    Parameters:
    -----------
    results: dict, åŒ…å«å„ä¸ªæ¨¡å‹çš„resultsï¼ˆæ¥è‡ªè®­ç»ƒï¼‰
            æ ¼å¼: {'transformer': {...}, 'mlp': {...}, 'random_forest': {...}}
    models: list, è¦å¯¹æ¯”çš„æ¨¡å‹åç§°ï¼Œå¦‚['transformer', 'mlp']
           å¦‚æœNoneï¼Œåˆ™å¯¹æ¯”æ‰€æœ‰æ¨¡å‹
    show_detailed: bool, æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†å¯¹æ¯”
    
    Returns:
    --------
    comparison: dict, å¯¹æ¯”ç»“æœ
    """
    # é€‰æ‹©è¦å¯¹æ¯”çš„æ¨¡å‹
    if models is None:
        models = list(results.keys())
    
    available_models = [m for m in models if m in results and results[m] is not None]
    
    if not available_models:
        print("âš ï¸ æ²¡æœ‰å¯å¯¹æ¯”çš„æ¨¡å‹")
        return None
    
    print("=" * 60)
    print(f"å¯¹æ¯”æ¨¡å‹: {', '.join([m.upper() for m in available_models])}")
    print("=" * 60)
    
    # æå–æŒ‡æ ‡
    comparison = {}
    for model_name in available_models:
        comparison[model_name] = results[model_name].get('metrics', {})
    
    # æ‰“å°å¯¹æ¯”è¡¨
    print("\næ€§èƒ½æŒ‡æ ‡å¯¹æ¯”:")
    print("-" * 60)
    
    # è¡¨å¤´
    print(f"{'æŒ‡æ ‡':<15s}", end="")
    for model_name in available_models:
        print(f"{model_name.upper():>15s}", end="")
    print()
    print("-" * 60)
    
    # å„é¡¹æŒ‡æ ‡
    for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc']:
        print(f"{metric.upper():<15s}", end="")
        for model_name in available_models:
            value = comparison[model_name].get(metric, 0)
            print(f"{value:>15.4f}", end="")
        print()
    
    # å¯è§†åŒ–å¯¹æ¯”
    if show_detailed and len(available_models) > 0:
        _plot_model_comparison(results, available_models)
    
    return comparison


# ==========================================
# Level 3: ç»¼åˆè¯Šæ–­ï¼ˆç»Ÿä¸€å…¥å£ï¼‰
# ==========================================

def diagnose_all_models(
    results: Dict[str, Dict],
    X_test, y_test, feature_names,
    models: List[str] = None,
    run_shap: bool = True):
    """
    ç»¼åˆè¯Šæ–­æ‰€æœ‰æ¨¡å‹
    
    è¿™æ˜¯ç»Ÿä¸€çš„è¯Šæ–­å…¥å£ï¼Œå†…éƒ¨ä¼šè°ƒç”¨å„ä¸ªç‹¬ç«‹è¯Šæ–­å‡½æ•°
    
    Parameters:
    -----------
    results: æ¨¡å‹è®­ç»ƒç»“æœ
    X_test, y_test: æµ‹è¯•æ•°æ®
    feature_names: ç‰¹å¾åç§°
    models: è¦è¯Šæ–­çš„æ¨¡å‹åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
    run_shap: æ˜¯å¦è¿è¡ŒSHAPåˆ†æ
    
    Returns:
    --------
    all_diagnostics: dict, æ‰€æœ‰è¯Šæ–­ç»“æœ
    """
    print("=" * 80)
    print("ç»¼åˆè¯Šæ–­ç³»ç»Ÿ - æ¨¡å—åŒ–æ¶æ„")
    print("=" * 80)
    
    if models is None:
        models = list(results.keys())
    
    # ç§»é™¤ä¸å­˜åœ¨çš„æ¨¡å‹
    available_models = [m for m in models if m in results and results[m] is not None]
    
    if not available_models:
        print("âš ï¸ æ²¡æœ‰å¯è¯Šæ–­çš„æ¨¡å‹")
        return None
    
    all_diagnostics = {}
    
    # å¯¹æ¯ä¸ªæ¨¡å‹è¿è¡Œç‹¬ç«‹è¯Šæ–­
    for model_name in available_models:
        print(f"\n{'='*80}")
        model_results = results[model_name]
        model = model_results.get('model')
        
        if model is None:
            continue
        
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©è¯Šæ–­å‡½æ•°
        if model_name == 'transformer':
            diagnostics = diagnose_transformer_model(
                model, X_test, y_test, feature_names,
                model_name="Transformer", show_plots=True)
        elif model_name == 'mlp':
            diagnostics = diagnose_mlp_model(
                model, X_test, y_test, feature_names,
                model_name="MLP", show_plots=True)
        elif model_name == 'random_forest':
            diagnostics = diagnose_rf_model(
                model, X_test, y_test, feature_names,
                model_name="Random Forest", show_plots=True)
        else:
            print(f"âš ï¸ æœªçŸ¥æ¨¡å‹ç±»å‹: {model_name}")
            continue
        
        all_diagnostics[model_name] = diagnostics
    
    # æ¨¡å‹é—´å¯¹æ¯”
    print("\n" + "=" * 80)
    print("æ¨¡å‹é—´æ€§èƒ½å¯¹æ¯”")
    print("=" * 80)
    comparison = compare_models(results, models=available_models, show_detailed=True)
    
    # å¤šæ¨¡å‹SHAPå¯¹æ¯”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if run_shap and SHAP_AVAILABLE:
        print("\n" + "=" * 80)
        print("å¤šæ¨¡å‹SHAPç»¼åˆåˆ†æ")
        print("=" * 80)
        _compare_shap_values(all_diagnostics, X_test, feature_names)
    
    return all_diagnostics


# ==========================================
# Level 4: PUå­¦ä¹ è¯„ä¼°
# ==========================================

def pu_evaluation_from_results(
    complete_results: Dict,
    thresholds: np.ndarray = None,
    pi: Optional[float] = None,
    max_f1_prime: float = float('inf'),
    min_detection_rate: float = 0.001,
    negative_ratio: Optional[float] = None,
    cost_fp: float = 1.0,
    cost_fn: float = 1.0) -> Dict:
    """
    å¢å¼ºçš„PUå­¦ä¹ è¯„ä¼° - è€ƒè™‘æ­£è´Ÿæ ·æœ¬èåˆã€é‡‡æ ·æ¯”ä¾‹å’Œé”™åˆ†ç±»ä»£ä»·
    
    Parameters:
    -----------
    complete_results : dict, å®Œæ•´çš„è®­ç»ƒç»“æœ
    thresholds : array, é˜ˆå€¼èŒƒå›´
    pi : float, æ­£ç±»å…ˆéªŒæ¦‚ç‡ï¼ˆå¯é€‰ï¼‰
    max_f1_prime : float, F1'æœ€å¤§å€¼é™åˆ¶ï¼Œé˜²æ­¢æ•°å€¼çˆ†ç‚¸
    min_detection_rate : float, æœ€å°æ£€æµ‹ç‡ï¼Œä½äºæ­¤å€¼æ ‡è®°ä¸ºä¸å¯é 
    negative_ratio : float, è®­ç»ƒæ—¶ä½¿ç”¨çš„è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼ˆç”¨äºé‡‡æ ·åå·®ä¿®æ­£ï¼‰
    cost_fp : float, False Positiveçš„ä»£ä»·æƒé‡
    cost_fn : float, False Negativeçš„ä»£ä»·æƒé‡
    
    Returns:
    --------
    dict : åŒ…å«å®Œæ•´è¯„ä¼°è¡¨æ ¼ã€æœ€ä½³ç»“æœå’Œå»ºè®®
    """
    if thresholds is None:
        thresholds = np.linspace(0.05, 0.95, 19)
    
    # å…¼å®¹å•æ¨¡å‹å’Œå¤šæ¨¡å‹æ¨¡å¼
    if "best_model" in complete_results:
        # å¤šæ¨¡å‹æ¨¡å¼
        training_results_all = complete_results["training_results"]
        best_model_result = training_results_all["results"][training_results_all["best_model"]]
        y_test = training_results_all["splits"]["y_test"]
        X_test = training_results_all["splits"]["X_test"]
        model = complete_results["model"]
    else:
        # å•æ¨¡å‹æ¨¡å¼
        y_test = complete_results["training_results"]["splits"]["y_test"]
        X_test = complete_results["training_results"]["splits"]["X_test"]
        model = complete_results["training_results"]["model"]
    
    # è·å–è®­ç»ƒé…ç½®
    if negative_ratio is None:
        negative_ratio = complete_results.get("config", {}).get("negative_ratio", 0.3)
    
    # è®¡ç®—è®­ç»ƒæ—¶çš„æ ·æœ¬æ¯”ä¾‹
    training_pos_ratio = 1 / (1 + negative_ratio)
    
    # ä½¿ç”¨å®Œæ•´æœªæ ‡æ³¨æ•°æ®è®¡ç®—Dï¼Œé¿å…è´Ÿæ ·æœ¬åå·®
    p_test = model.predict(X_test, verbose=0).ravel()
    p_unl = complete_results["prediction_results"]["predicted_prob"].values
    
    print(f"ğŸ“Š å¢å¼ºPUè¯„ä¼° - è€ƒè™‘é‡‡æ ·åå·®å’Œé”™åˆ†ç±»ä»£ä»·:")
    print(f"   - è®­ç»ƒæ—¶è´Ÿæ ·æœ¬æ¯”ä¾‹: {negative_ratio:.1f} (æ­£æ ·æœ¬æ¯”ä¾‹: {training_pos_ratio:.1%})")
    print(f"   - æµ‹è¯•é›†æ­£æ ·æœ¬: {(y_test==1).sum()}")
    print(f"   - æµ‹è¯•é›†è´Ÿæ ·æœ¬: {(y_test==0).sum()}")
    print(f"   - æœªæ ‡æ³¨æ ·æœ¬: {len(p_unl)}")
    print(f"   - é”™åˆ†ç±»ä»£ä»·æ¯” (FP:FN): {cost_fp}:{cost_fn}")

    # è¯„ä¼°ä¸åŒé˜ˆå€¼
    results = []
    pos_mask = (y_test == 1)
    neg_mask = (y_test == 0)
    
    for t in thresholds:
        TP = (p_test[pos_mask] >= t).sum()
        FN = (p_test[pos_mask] < t).sum()
        FP = (p_test[neg_mask] >= t).sum()
        TN = (p_test[neg_mask] < t).sum()
        
        # åŸºç¡€æŒ‡æ ‡
        R = TP / (TP + FN) if (TP + FN) > 0 else 0.0
        P_biased = TP / (TP + FP) if (TP + FP) > 0 else 0.0
        FPR = FP / (FP + TN) if (FP + TN) > 0 else 0.0
        
        # PUå­¦ä¹ æ ¸å¿ƒæŒ‡æ ‡
        D = (p_unl >= t).mean()
        D_safe = max(D, min_detection_rate)
        
        # é‡‡æ ·æ¯”ä¾‹ä¿®æ­£çš„ç²¾ç¡®åº¦
        if pi is not None:
            P_corrected = (R * pi) / (R * pi + FPR * (1 - pi)) if (R * pi + FPR * (1 - pi)) > 0 else 0.0
        else:
            P_corrected = P_biased
        
        # å¤šç§F1åˆ†æ•°
        F1_standard = 2 * P_biased * R / (P_biased + R) if (P_biased + R) > 0 else 0.0
        
        if (cost_fp + cost_fn) > 0:
            weighted_precision = P_biased * cost_fp / (cost_fp + cost_fn)
            weighted_recall = R * cost_fn / (cost_fp + cost_fn)
            F1_cost_sensitive = 2 * weighted_precision * weighted_recall / (weighted_precision + weighted_recall) if (weighted_precision + weighted_recall) > 0 else 0.0
        else:
            F1_cost_sensitive = F1_standard
        
        F1p_raw = (R * R) / D_safe
        F1_prime_original = min(F1p_raw, max_f1_prime)
        
        F1_prime_enhanced = (R * R) / max(D_safe + FPR * 0.1, min_detection_rate)
        F1_prime_enhanced = min(F1_prime_enhanced, max_f1_prime)
        
        sampling_bias_factor = abs(training_pos_ratio - 0.5) * 2
        F1_sampling_corrected = F1_standard * (1 - sampling_bias_factor * 0.2)
        
        # å¯é æ€§è¯„ä¼°
        if D >= 0.01 and FPR < 0.5:
            reliability = "High"
        elif D >= 0.001 and FPR < 0.7:
            reliability = "Medium"
        else:
            reliability = "Low"
        
        row = {
            "thr": float(t),
            "TP": int(TP), "FN": int(FN), "FP": int(FP), "TN": int(TN),
            "R": float(R), "P_biased": float(P_biased), "FPR": float(FPR),
            "P_corrected": float(P_corrected) if pi else None,
            "D": float(D),
            "F1_standard": float(F1_standard),
            "F1_cost_sensitive": float(F1_cost_sensitive),
            "F1_prime": float(F1_prime_original),
            "F1_prime_enhanced": float(F1_prime_enhanced),
            "F1_sampling_corrected": float(F1_sampling_corrected),
            "F1_prime_raw": float(F1p_raw),
            "reliability": reliability,
            "is_stable": D >= min_detection_rate,
            "sampling_bias_factor": float(sampling_bias_factor)
        }
        
        # PU-LearningæŒ‡æ ‡
        if pi is not None:
            alpha = 0.7
            constraint_met = D >= alpha * pi
            prec_pu = (R * pi) / D_safe
            f1_pu = 2 * prec_pu * R / max(prec_pu + R, 1e-12)
            
            row.update({
                "Prec_PU": float(prec_pu),
                "F1_PU": float(f1_pu),
                "constraint_D_ge_alpha_pi": constraint_met,
                "alpha_pi": alpha * pi
            })
        
        results.append(row)
    
    # æ™ºèƒ½é˜ˆå€¼é€‰æ‹©
        # æ™ºèƒ½é˜ˆå€¼é€‰æ‹©
    best_result = _select_optimal_threshold(results, pi, cost_fp, cost_fn)
    recommendation = _get_threshold_recommendation(results, best_result, pi, negative_ratio)
    
    # æ‰“å°è¯„ä¼°ç»“æœ
    _print_pu_evaluation_summary(results, best_result, pi, negative_ratio, cost_fp, cost_fn)

    
    return {
        "table": results,
        "best": best_result,
        "reliable_count": len([r for r in results if r["reliability"] != "Low"]),
        "recommendation": recommendation,
        "cost_ratio": f"FP:FN = {cost_fp}:{cost_fn}", 
        "config": {
            "negative_ratio": negative_ratio,
            "training_pos_ratio": training_pos_ratio,
            "cost_fp": cost_fp,
            "cost_fn": cost_fn,
            "sampling_bias_factor": best_result.get("sampling_bias_factor", 0)
        }
    }

# ==========================================
# è¾…åŠ©å‡½æ•° - SHAP
# ==========================================

def _compute_shap_transformer(model, X_test, feature_names, sample_size=1000):
    """ä¸ºTransformeræ¨¡å‹è®¡ç®—SHAPå€¼"""
    if not SHAP_AVAILABLE:
        return None
    
    import shap
    
    # é€‰æ‹©èƒŒæ™¯æ ·æœ¬
    background_size = min(50, len(X_test) // 10)
    background = X_test[:background_size]
    
    # åŒ…è£…é¢„æµ‹å‡½æ•°
    def predict_wrapper(X):
        return model.predict(X, verbose=0).ravel()
    
    # KernelExplainer (Transformeræœ€ç¨³å®š)
    explainer = shap.KernelExplainer(predict_wrapper, background)
    shap_values = explainer.shap_values(X_test[:sample_size], nsamples=100)
    
    if isinstance(shap_values, list):
        shap_values = shap_values[0]
    
    return shap_values


def _compute_shap_deep(model, X_test, feature_names, sample_size=1000):
    """ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹è®¡ç®—SHAPå€¼ï¼ˆé€šç”¨ï¼‰"""
    if not SHAP_AVAILABLE:
        return None
    
    import shap
    
    # é€‰æ‹©èƒŒæ™¯æ ·æœ¬
    background_size = min(20, len(X_test) // 2)
    background = X_test[:background_size]
    
    # åŒ…è£…é¢„æµ‹å‡½æ•°
    def predict_wrapper(X):
        return model.predict(X, verbose=0).ravel()
    
    # KernelExplainer
    explainer = shap.KernelExplainer(predict_wrapper, background)
    shap_values = explainer.shap_values(X_test[:sample_size], nsamples=50)
    
    return shap_values




def _select_optimal_threshold(results, pi=None, cost_fp=1.0, cost_fn=1.0):
    """æ™ºèƒ½é˜ˆå€¼é€‰æ‹©ç­–ç•¥"""
    reliable_results = [r for r in results if r["reliability"] != "Low"]
    
    if not reliable_results:
        print("âš ï¸ æ‰€æœ‰é˜ˆå€¼çš„æ£€æµ‹ç‡éƒ½è¿‡ä½ï¼Œé€‰æ‹©æ£€æµ‹ç‡æœ€é«˜çš„")
        return max(results, key=lambda r: r["D"])
    
    # é€‰æ‹©æœ€ä¼˜F1æ ‡å‡†
    if pi is not None:
        constraint_satisfied = [r for r in reliable_results
                              if r.get("constraint_D_ge_alpha_pi", True)]
        if constraint_satisfied:
            print("âœ… åŸºäºæ»¡è¶³çº¦æŸçš„PU-F1é€‰æ‹©æœ€ä½³é˜ˆå€¼")
            return max(constraint_satisfied, key=lambda r: r["F1_PU"])
        else:
            print("âš ï¸ æ— é˜ˆå€¼æ»¡è¶³çº¦æŸï¼Œä½¿ç”¨æœ€ä¼˜F1_PU")
            return max(reliable_results, key=lambda r: r.get("F1_PU", 0))
    elif cost_fp != cost_fn:
        print(f"âœ… åŸºäºä»£ä»·æ•æ„ŸF1é€‰æ‹©æœ€ä½³é˜ˆå€¼ (FP:FN = {cost_fp}:{cost_fn})")
        return max(reliable_results, key=lambda r: r["F1_cost_sensitive"])
    else:
        print("âœ… åŸºäºå¢å¼ºF1'é€‰æ‹©æœ€ä½³é˜ˆå€¼")
        return max(reliable_results, key=lambda r: r["F1_prime_enhanced"])


def _get_threshold_recommendation(results, best_result, pi=None, negative_ratio=None):
    """å¢å¼ºçš„é˜ˆå€¼å»ºè®® - è€ƒè™‘é‡‡æ ·åå·®"""
    best_d = best_result["D"]
    best_thresh = best_result["thr"]
    best_r = best_result["R"]
    best_fpr = best_result["FPR"]
    
    # åŸºç¡€å»ºè®®
    if best_d >= 0.05:
        base_msg = f"âœ… æ¨èé˜ˆå€¼ {best_thresh:.3f} (è¦†ç›–ç‡ {best_d:.1%}ï¼Œå¬å›ç‡ {best_r:.1%})"
    elif best_d >= 0.01:
        base_msg = f"âš ï¸ é˜ˆå€¼ {best_thresh:.3f} å¯ç”¨ä½†ä¿å®ˆ (è¦†ç›–ç‡ {best_d:.1%}ï¼Œå¬å›ç‡ {best_r:.1%})"
    else:
        base_msg = f"âŒ é˜ˆå€¼ {best_thresh:.3f} è¿‡äºä¿å®ˆ (è¦†ç›–ç‡ä»… {best_d:.1%})"
    
    # è¯¯æŠ¥ç‡è­¦å‘Š
    if best_fpr > 0.3:
        base_msg += f"\n   âš ï¸ è¯¯æŠ¥ç‡è¾ƒé«˜ ({best_fpr:.1%})"
    
    return base_msg


def _print_pu_evaluation_summary(results, best_result, pi, negative_ratio, cost_fp=1.0, cost_fn=1.0):
    """æ‰“å°PUè¯„ä¼°æ‘˜è¦"""
    df = pd.DataFrame(results)
    
    print("\n" + "=" * 60)
    print("PUè¯„ä¼°æ‘˜è¦")
    print("=" * 60)
    
    # æ˜¾ç¤ºå‰10ä¸ªæœ€é«˜F1_primeçš„ç»“æœ
    top_results = df.nlargest(10, 'F1_prime_enhanced')
    print("\nTop 10 é˜ˆå€¼ç»“æœ (æŒ‰F1_prime_enhanced):")
    print("-" * 80)
    print(f"{'Thr':<8s} {'R':<8s} {'D':<8s} {'FPR':<8s} {'F1':<8s} {'F1â€²':<8s} {'Rel':<8s}")
    print("-" * 80)
    
    for _, row in top_results.iterrows():
        print(f"{row['thr']:>6.3f} {row['R']:>6.3f} {row['D']:>6.3f} "
              f"{row['FPR']:>6.3f} {row['F1_standard']:>6.3f} "
              f"{row['F1_prime_enhanced']:>6.3f} {row['reliability']:>8s}")
    
    # æœ€ä½³ç»“æœ
    print("\n" + "=" * 60)
    print("æœ€ä½³é˜ˆå€¼è¯¦æƒ…")
    print("=" * 60)
    print(f"é˜ˆå€¼: {best_result['thr']:.3f}")
    print(f"å¬å›ç‡(R): {best_result['R']:.3f}")
    print(f"æ£€æµ‹ç‡(D): {best_result['D']:.3f}")
    print(f"F1æ ‡å‡†: {best_result['F1_standard']:.3f}")
    print(f"F1â€²å¢å¼º: {best_result['F1_prime_enhanced']:.3f}")
    print(f"âœ… é”™åˆ†ç±»ä»£ä»·æ¯”: FP:FN = {cost_fp}:{cost_fn}") 
    print(f"å¯é æ€§: {best_result['reliability']}")
    print(f"æ··æ·†çŸ©é˜µ: TP={best_result['TP']}, FN={best_result['FN']}, "
          f"FP={best_result['FP']}, TN={best_result['TN']}")
    print(f"F1ä»£ä»·æ•æ„Ÿ: {best_result['F1_cost_sensitive']:.3f}")


# ==========================================
# å¯è§†åŒ–è¾…åŠ©å‡½æ•°
# ==========================================

def _plot_model_comparison(results, model_names):
    """å¯è§†åŒ–æ¨¡å‹å¯¹æ¯”"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. æ€§èƒ½æŒ‡æ ‡æŸ±çŠ¶å›¾
    metrics_list = ['accuracy', 'precision', 'recall', 'f1', 'auc']
    x = np.arange(len(model_names))
    width = 0.15
    
    for i, metric in enumerate(metrics_list):
        values = [results[m].get('metrics', {}).get(metric, 0) for m in model_names]
        axes[0, 0].bar(x + i*width, values, width, label=metric.upper())
    
    axes[0, 0].set_ylabel('Score')
    axes[0, 0].set_title('Performance Metrics Comparison')
    axes[0, 0].set_xticks(x + width * 2)
    axes[0, 0].set_xticklabels([m.upper().replace('_', ' ') for m in model_names])
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    
    # 2. ROCæ›²çº¿å¯¹æ¯” (ç®€åŒ–ç‰ˆ)
    axes[0, 1].plot([0, 1], [0, 1], 'k--', label='Random')
    axes[0, 1].set_xlabel('False Positive Rate')
    axes[0, 1].set_ylabel('True Positive Rate')
    axes[0, 1].set_title('ROC Curves Comparison')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
    for model_name in model_names:
        preds = results[model_name].get('predictions', None)
        if preds is not None:
            axes[1, 0].hist(preds, bins=20, alpha=0.5, label=model_name.upper().replace('_', ' '))
    
    axes[1, 0].axvline(0.5, color='red', linestyle='--', linewidth=2, label='Threshold 0.5')
    axes[1, 0].set_xlabel('Predicted Probability')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].set_title('Prediction Distribution')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. æŒ‡æ ‡é›·è¾¾å›¾
    n_metrics = len(metrics_list)
    angles = np.linspace(0, 2*np.pi, n_metrics, endpoint=False).tolist()
    angles += angles[:1]
    
    axes[1, 1].remove()
    ax = fig.add_subplot(2, 2, 4, projection='polar')
    
    for model_name in model_names:
        values = [results[model_name].get('metrics', {}).get(m, 0) for m in metrics_list]
        values += values[:1]
        ax.plot(angles, values, 'o-', linewidth=2, label=model_name.upper().replace('_', ' '))
        ax.fill(angles, values, alpha=0.1)
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels([m.upper() for m in metrics_list])
    ax.set_ylim(0, 1)
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
    ax.set_title('Performance Radar Chart', pad=20)
    
    plt.tight_layout()
    plt.show()


def _compare_shap_values(all_diagnostics, X_test, feature_names, max_display=15):
    """å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„SHAPå€¼"""
    if not SHAP_AVAILABLE:
        return
    
    print("\nè®¡ç®—ç»¼åˆSHAPé‡è¦æ€§...")
    
    # æ”¶é›†å„æ¨¡å‹çš„SHAPå€¼
    shap_results = {}
    for model_name, diagnostics in all_diagnostics.items():
        if diagnostics.get('shap_values') is not None:
            shap_values = diagnostics['shap_values']
            # è®¡ç®—å¹³å‡ç»å¯¹SHAPå€¼
            feature_importance = np.abs(shap_values).mean(0)
            if len(feature_importance.shape) > 1:
                feature_importance = feature_importance.mean(0)
            
            shap_results[model_name] = feature_importance
    
    if not shap_results:
        print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„SHAPç»“æœ")
        return
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # 1. ç‰¹å¾é‡è¦æ€§å¯¹æ¯”
    df_importance = pd.DataFrame(shap_results, index=feature_names[:len(list(shap_results.values())[0])])
    df_importance['mean'] = df_importance.mean(axis=1)
    df_importance = df_importance.sort_values('mean', ascending=True)
    df_importance = df_importance.drop('mean', axis=1)
    
    df_importance.plot(kind='barh', ax=axes[0], legend=True)
    axes[0].set_title('Feature Importance Comparison')
    axes[0].set_xlabel('Mean |SHAP Value|')
    axes[0].grid(True, alpha=0.3, axis='x')
    axes[0].legend(title='Model', fontsize=8)
    
    # 2. ç»¼åˆé‡è¦æ€§
    overall = df_importance.mean(axis=1).tail(max_display)
    axes[1].barh(range(len(overall)), overall.values)
    axes[1].set_yticks(range(len(overall)))
    axes[1].set_yticklabels(overall.index, fontsize=9)
    axes[1].set_xlabel('Mean |SHAP Value|')
    axes[1].set_title(f'Top {max_display} Features (Consensus)')
    axes[1].grid(True, alpha=0.3, axis='x')
    
    plt.tight_layout()
    plt.show()
    
    print("\nTop 10 ç‰¹å¾ï¼ˆç»¼åˆï¼‰:")
    for i, (feat, imp) in enumerate(overall.tail(10).items(), 1):
        print(f"  {i:2d}. {feat:20s}: {imp:.6f}")