# æ–‡ä»¶: function/negative_sampling.py (å®Œæ•´ç‰ˆ)

# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€è´Ÿæ ·æœ¬ç”Ÿæˆæ¥å£

ç­–ç•¥ç±»å‹ï¼š
1. é€‰æ‹©å¼ (Selection-based): ä»ç°æœ‰æ ·æœ¬æ± ä¸­ç­›é€‰
2. ç”Ÿæˆå¼ (Generation-based): ä½¿ç”¨GMMç”Ÿæˆæ–°æ ·æœ¬
3. æ··åˆå¼ (Hybrid): ç»“åˆä¸¤ç§ç­–ç•¥

Author: 2024
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Literal
from abc import ABC, abstractmethod

# å¿…è¦çš„å¯¼å…¥ï¼ˆæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
# è¿™äº›å‡½æ•°å¦‚æœåœ¨æ–°æ–‡ä»¶ä¸­ï¼Œéœ€è¦ç¡®ä¿å¯¼å…¥è·¯å¾„æ­£ç¡®
try:
    from function.gmm_training import score_env, attach_env_calibration, visualize_sampling_results_pit
except ImportError:
    # å¦‚æœåœ¨notebookä¸­ï¼Œå¯èƒ½éœ€è¦ä»å…¨å±€å‘½åç©ºé—´å¯¼å…¥
    pass


# ==========================================
# ç­–ç•¥åŸºç±»
# ==========================================

class NegativeSamplingStrategy(ABC):
    """è´Ÿæ ·æœ¬ç­–ç•¥æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def generate(
        self, 
        df_positive: pd.DataFrame,
        df_prediction_pool: Optional[pd.DataFrame] = None,
        features: List[str] = None,
        negative_ratio: float = 0.3,
        random_state: int = 42
    ) -> tuple:
        """
        ç”Ÿæˆè´Ÿæ ·æœ¬
        
        Returns:
        --------
        (df_negative_samples, df_remaining, df_combined)
        """
        pass


# ==========================================
# ç­–ç•¥å®ç°ï¼ˆåŒ…è£…ç°æœ‰å‡½æ•°ï¼‰
# ==========================================

class SelectionBasedStrategy(NegativeSamplingStrategy):
    """é€‰æ‹©å¼ç­–ç•¥ï¼šåŒ…è£…ç°æœ‰çš„generate_negative_samples_from_abandon"""
    
    def __init__(
        self,
        gmm_pipeline,
        sampling_strategy: Literal["simple", "mixed", "hard"] = "mixed",
        difficulty_levels: int = 3
    ):
        self.gmm_pipeline = gmm_pipeline
        self.sampling_strategy = sampling_strategy
        self.difficulty_levels = difficulty_levels
    
    def generate(
        self,
        df_positive,
        df_prediction_pool,
        features,
        negative_ratio=0.3,
        random_state=42
    ):
        """è°ƒç”¨ç°æœ‰çš„generate_negative_samples_from_abandon"""
        print(f"\n{'='*60}")
        print(f"é€‰æ‹©å¼è´Ÿæ ·æœ¬ç­–ç•¥")
        print(f"  ç­–ç•¥: {self.sampling_strategy}")
        print(f"  éš¾åº¦å±‚çº§: {self.difficulty_levels}")
        print(f"{'='*60}")
        
        # ç›´æ¥è°ƒç”¨ç°æœ‰å‡½æ•°
        return generate_negative_samples_from_abandon(
            df_positive, 
            df_prediction_pool, 
            features, 
            self.gmm_pipeline,
            negative_ratio=negative_ratio,
            random_state=random_state,
            sampling_strategy=self.sampling_strategy,
            difficulty_levels=self.difficulty_levels
        )


class GenerationBasedStrategy(NegativeSamplingStrategy):
    """ç”Ÿæˆå¼ç­–ç•¥ï¼šçº¯ç²¹çš„GMMç”Ÿæˆå¼é‡‡æ ·"""
    
    def __init__(self, gmm_pipeline, augmentation_ratio=0.3):
        self.gmm_pipeline = gmm_pipeline
        self.augmentation_ratio = augmentation_ratio
    

    def generate(self, df_positive, df_prediction_pool=None, features=None,
                negative_ratio=0.3, random_state=42):
        """çº¯ç²¹ç”Ÿæˆå¼ï¼šä»GMMé‡‡æ ·ç”Ÿæˆè´Ÿæ ·æœ¬"""
        print(f"\n{'='*60}")
        print(f"ç”Ÿæˆå¼è´Ÿæ ·æœ¬ç­–ç•¥ï¼ˆçº¯ç²¹GMMé‡‡æ ·ï¼‰")
        print(f"{'='*60}")
        
        np.random.seed(random_state)
        
        gmm = self.gmm_pipeline.named_steps['gmm']
        preprocessor = self.gmm_pipeline.named_steps['preprocessor']
        
        # è®¡ç®—éœ€è¦ç”Ÿæˆçš„è´Ÿæ ·æœ¬æ•°
        n_pos = len(df_positive)
        n_neg = int(n_pos * negative_ratio)
        
        print(f"\nç”Ÿæˆé…ç½®:")
        print(f"  æ­£æ ·æœ¬æ•°: {n_pos}")
        print(f"  ç›®æ ‡è´Ÿæ ·æœ¬æ•°: {n_neg}")
        
        # âœ… åŠ¨æ€é‡‡æ ·ç­–ç•¥ï¼šé€æ­¥å¢åŠ é‡‡æ ·é‡ï¼Œä¿æŒé˜ˆå€¼ä¸¥æ ¼
        X_pos_processed = preprocessor.transform(df_positive[features])
        ref_logp = np.mean(gmm.score_samples(X_pos_processed))
        ref_std = np.std(gmm.score_samples(X_pos_processed))
        threshold = ref_logp - ref_std  # ä¿æŒä¸¥æ ¼é˜ˆå€¼
        
        print(f"\nç­›é€‰æ ‡å‡†:")
        print(f"  æ­£æ ·æœ¬å¹³å‡logæ¦‚ç‡: {ref_logp:.3f}")
        print(f"  é˜ˆå€¼ (å‡å€¼ - 1std): {threshold:.3f}")
        
        max_attempts = 8  # æœ€å¤šå°è¯•8æ¬¡
        sampling_multiplier = 2  # åˆå§‹é‡‡æ ·å€æ•°
        candidate_samples = None
        total_sampled = 0
        
        for attempt in range(max_attempts):
            sample_count = n_neg * sampling_multiplier
            print(f"\nå°è¯• {attempt + 1}/{max_attempts}: é‡‡æ · {sample_count} ä¸ª...")
            
            # ä»GMMé‡‡æ ·
            generated_samples_highdim, _ = gmm.sample(sample_count)
            total_sampled += sample_count
            
            # ç­›é€‰ä½æ¦‚ç‡æ ·æœ¬
            generated_logps = gmm.score_samples(generated_samples_highdim)
            low_density_mask = generated_logps < threshold
            n_valid = low_density_mask.sum()
            
            print(f"  å®é™…é‡‡æ ·: {sample_count} ä¸ª")
            print(f"  ç¬¦åˆé˜ˆå€¼: {n_valid} ä¸ª ({n_valid/sample_count:.1%})")
            
            if candidate_samples is None:
                candidate_samples = generated_samples_highdim[low_density_mask].copy()
            else:
                # åˆå¹¶æ–°é‡‡æ ·çš„æœ‰æ•ˆæ ·æœ¬
                new_candidates = generated_samples_highdim[low_density_mask]
                candidate_samples = np.vstack([candidate_samples, new_candidates])
            
            n_available = len(candidate_samples)
            print(f"  ç´¯è®¡æœ‰æ•ˆæ ·æœ¬: {n_available} ä¸ª")
            
            if n_available >= n_neg:
                # æœ‰è¶³å¤Ÿçš„æ ·æœ¬ï¼Œæˆªå–ç›®æ ‡æ•°é‡
                candidate_samples = candidate_samples[:n_neg]
                print(f"  âœ… å·²è·å¾—è¶³å¤Ÿæ ·æœ¬ ({len(candidate_samples)} ä¸ª)")
                break
            elif attempt < max_attempts - 1:
                # ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•ï¼Œå¢åŠ é‡‡æ ·å€æ•°
                sampling_multiplier = int(sampling_multiplier * 1.5)  # å¢åŠ 50%
                print(f"  â³ ç»§ç»­å¢åŠ é‡‡æ ·é‡ (ä¸‹æ¬¡: {n_neg * sampling_multiplier} ä¸ª)...")
            else:
                print(f"  âš ï¸ å·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨æ ·æœ¬ ({n_available} ä¸ª)")
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\né‡‡æ ·ç»Ÿè®¡:")
        print(f"  æ€»é‡‡æ ·æ¬¡æ•°: {total_sampled} ä¸ª")
        print(f"  æœ‰æ•ˆæ ·æœ¬ç‡: {len(candidate_samples)/total_sampled:.2%}")
        print(f"  æœ€ç»ˆè´Ÿæ ·æœ¬æ•°: {len(candidate_samples)} ä¸ª")
        
        # âœ… é€†å˜æ¢ï¼šæœ€è¿‘é‚»æ˜ å°„ï¼ˆä¼˜åŒ–ç‰ˆ - æ‰¹é‡å¤„ç†ï¼‰
        from scipy.spatial import cKDTree
        tree = cKDTree(X_pos_processed)
        
        print(f"\né€†å˜æ¢æ˜ å°„åˆ°åŸå§‹ç‰¹å¾ç©ºé—´ï¼ˆæ‰¹é‡å¤„ç†ï¼‰...")
        
        # æ‰¹é‡æŸ¥æ‰¾æœ€è¿‘é‚»
        distances, nearest_indices = tree.query(candidate_samples, k=1)
        
        if nearest_indices.ndim > 1:
            nearest_indices = nearest_indices.ravel()

        # æ‰¹é‡è·å–åŸºç¡€ç‰¹å¾ï¼ˆæ·»åŠ  int() è½¬æ¢ç¡®ä¿ç´¢å¼•æ˜¯æ•´æ•°ï¼‰
        base_features_list = [df_positive[features].iloc[int(idx)].copy() for idx in nearest_indices]
        df_base = pd.DataFrame(base_features_list).reset_index(drop=True)
        
        # å‘é‡åŒ–æ·»åŠ å¾®æ‰°
        numeric_cols = df_positive[features].select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col in df_base.columns:
                std_val = df_positive[col].std()
                perturbations = np.random.normal(0, 0.15 * std_val, size=len(df_base))
                df_base[col] += perturbations
        
        # æ‰¹é‡è®¡ç®—log-density
        df_base_features = df_base[features].copy()
        X_generated_processed = preprocessor.transform(df_base_features)
        generated_logps = gmm.score_samples(X_generated_processed)

        # æ·»åŠ å…ƒæ•°æ®
        df_generated = df_base.copy()
        df_generated['gmm_logp'] = generated_logps
        df_generated['sample_type'] = 'generated'
        df_generated['label'] = 0

        print(f"é€†å˜æ¢å®Œæˆ: {len(df_generated)} ä¸ªè´Ÿæ ·æœ¬")
        print(f"å¹³å‡log-density: {df_generated['gmm_logp'].mean():.3f}")

        # è®­ç»ƒæ•°æ®
        df_combined = pd.concat([
            df_positive[features].assign(label=1, sample_type='positive'),
            df_generated[features + ['gmm_logp', 'sample_type', 'label']]
        ], ignore_index=True)

        print(f"ç”Ÿæˆå®Œæˆ: {len(df_generated)} ä¸ªè´Ÿæ ·æœ¬")
        print(f"å¹³å‡log-density: {df_generated['gmm_logp'].mean():.3f}")

        # ç”Ÿæˆå¼ç­–ç•¥è¿”å›å¤„ç†
        if df_prediction_pool is not None and len(df_prediction_pool) > 0:
            print(f"ä¿ç•™æ•´ä¸ªé¢„æµ‹æ± ç”¨äºé¢„æµ‹: {len(df_prediction_pool)} ä¸ªæ ·æœ¬")
            df_remaining = df_prediction_pool.copy()

            # å¯é€‰ï¼šæ·»åŠ GMMè¯„åˆ†åˆ°remainingï¼ˆä¸é€‰æ‹©å¼ç­–ç•¥ä¿æŒä¸€è‡´çš„æ•°æ®æ ¼å¼ï¼‰
            X_remaining = preprocessor.transform(df_remaining[features])
            remaining_logp = gmm.score_samples(X_remaining)
            df_remaining['gmm_logp'] = remaining_logp
            df_remaining['gmm_score'] = None  # åç»­å¯ä»¥è®¡ç®—sigmoidåˆ†æ•°

        else:
            print("âš ï¸ æœªæä¾›é¢„æµ‹æ± ï¼Œè¿”å›ç©ºDataFrame")
            df_remaining = pd.DataFrame()

        return df_generated, df_remaining, df_combined

class HybridStrategy(NegativeSamplingStrategy):
    """æ··åˆç­–ç•¥ï¼šé€‰æ‹©å¼ + ç”Ÿæˆå¼"""
    
    def __init__(
        self,
        selection_strategy: SelectionBasedStrategy,
        generation_strategy: GenerationBasedStrategy,
        selection_weight: float = 0.7
    ):
        self.selection_strategy = selection_strategy
        self.generation_strategy = generation_strategy
        self.selection_weight = selection_weight
    
    def generate(
        self,
        df_positive,
        df_prediction_pool,
        features,
        negative_ratio=0.3,
        random_state=42
    ):
        """æ··åˆç­–ç•¥ç”Ÿæˆ"""
        print(f"\n{'='*60}")
        print(f"æ··åˆè´Ÿæ ·æœ¬ç­–ç•¥")
        print(f"  é€‰æ‹©å¼æƒé‡: {self.selection_weight:.1%}")
        print(f"  ç”Ÿæˆå¼æƒé‡: {1-self.selection_weight:.1%}")
        print(f"{'='*60}")
        
        # åˆ†é…æ¯”ä¾‹
        n_pos = len(df_positive)
        n_total_neg = int(n_pos * negative_ratio)
        n_selection = int(n_total_neg * self.selection_weight)
        n_generation = n_total_neg - n_selection
        
        # 1. é€‰æ‹©å¼
        if n_selection > 0 and df_prediction_pool is not None and len(df_prediction_pool) > 0:
            df_neg_sel, df_remaining, _ = self.selection_strategy.generate(
                df_positive, df_prediction_pool, features,
                negative_ratio=n_selection / n_pos,
                random_state=random_state
            )
        else:
            df_neg_sel = pd.DataFrame()
            df_remaining = df_prediction_pool if df_prediction_pool is not None else pd.DataFrame()
        
        # 2. ç”Ÿæˆå¼ï¼ˆåŸºäºé€‰æ‹©å¼çš„è´Ÿæ ·æœ¬ï¼‰
        if n_generation > 0 and len(df_neg_sel) > 0:
            # ä½¿ç”¨augment_negative_samples_with_generationå¢å¼º
            df_neg_gen = augment_negative_samples_with_generation(
                df_neg_sel.sample(min(len(df_neg_sel), n_generation // 2)),
                df_positive,
                features,
                self.selection_strategy.gmm_pipeline,
                augmentation_ratio=1.0,
                random_state=random_state + 1
            )
            df_neg_gen['sample_type'] = 'generation'
        else:
            df_neg_gen = pd.DataFrame()
        
        # åˆå¹¶æ‰€æœ‰è´Ÿæ ·æœ¬
        if len(df_neg_sel) > 0 and len(df_neg_gen) > 0:
            all_neg = pd.concat([df_neg_sel, df_neg_gen], ignore_index=True)
        elif len(df_neg_sel) > 0:
            all_neg = df_neg_sel
        elif len(df_neg_gen) > 0:
            all_neg = df_neg_gen
        else:
            raise ValueError("No negative samples generated")
        
        # ç¡®ä¿ä¸è¶…è¿‡ç›®æ ‡æ•°é‡
        if len(all_neg) > n_total_neg:
            all_neg = all_neg.sample(n=n_total_neg, random_state=random_state)
        
        # æ ‡è®°æ¥æº
        n_from_selection = min(len(df_neg_sel), n_selection)
        n_from_generation = len(all_neg) - n_from_selection
        
        print(f"\næ··åˆç»“æœ:")
        print(f"  é€‰æ‹©å¼: {n_from_selection} ä¸ª")
        print(f"  ç”Ÿæˆå¼: {n_from_generation} ä¸ª")
        print(f"  æ€»è®¡: {len(all_neg)} ä¸ª")
        
        # è®­ç»ƒæ•°æ®
        df_combined = pd.concat([
            df_positive[features].assign(label=1, sample_type='positive'),
            all_neg
        ], ignore_index=True)
        
        return all_neg, df_remaining, df_combined


# ==========================================
# ç»Ÿä¸€æ¥å£
# ==========================================

def generate_negative_samples_unified(
    strategy_type: Literal["selection", "generation", "hybrid"],
    df_positive: pd.DataFrame,
    df_prediction_pool: Optional[pd.DataFrame] = None,
    features: List[str] = None,
    gmm_pipeline=None,
    **kwargs
) -> tuple:
    """
    ç»Ÿä¸€çš„è´Ÿæ ·æœ¬ç”Ÿæˆæ¥å£
    
    Parameters:
    -----------
    strategy_type: str, ç­–ç•¥ç±»å‹
        - "selection": é€‰æ‹©å¼ï¼ˆéœ€è¦df_prediction_poolï¼‰
        - "generation": ç”Ÿæˆå¼ï¼ˆä¸éœ€è¦df_prediction_poolï¼‰
        - "hybrid": æ··åˆå¼
    df_positive: DataFrame, æ­£æ ·æœ¬
    df_prediction_pool: DataFrame, é¢„æµ‹æ ·æœ¬æ± ï¼ˆé€‰æ‹©å¼å’Œæ··åˆå¼éœ€è¦ï¼‰
    features: List, ç‰¹å¾åˆ—è¡¨
    gmm_pipeline: Pipeline, GMMæ¨¡å‹
    **kwargs: ç­–ç•¥ç‰¹å®šå‚æ•°
        - sampling_strategy: "simple"/"mixed"/"hard"
        - difficulty_levels: int
        - augmentation_ratio: float
        - selection_weight: float
        - negative_ratio: float
        - random_state: int
    
    Returns:
    --------
    (df_negative_samples, df_remaining, df_combined_training)
    """
    
    print(f"\n{'='*80}")
    print(f"ç»Ÿä¸€è´Ÿæ ·æœ¬ç”Ÿæˆæ¥å£ - ç­–ç•¥: {strategy_type}")
    print(f"{'='*80}")
    
    # åˆ›å»ºç­–ç•¥å¯¹è±¡
    if strategy_type == "selection":
        strategy = SelectionBasedStrategy(
            gmm_pipeline=gmm_pipeline,
            sampling_strategy=kwargs.get('sampling_strategy', 'mixed'),
            difficulty_levels=kwargs.get('difficulty_levels', 3)
        )
    
    elif strategy_type == "generation":
        strategy = GenerationBasedStrategy(
            gmm_pipeline=gmm_pipeline,
            augmentation_ratio=kwargs.get('augmentation_ratio', 0.3)
        )
    
    elif strategy_type == "hybrid":
        selection_strategy = SelectionBasedStrategy(
            gmm_pipeline=gmm_pipeline,
            sampling_strategy=kwargs.get('sampling_strategy', 'mixed'),
            difficulty_levels=kwargs.get('difficulty_levels', 3)
        )
        generation_strategy = GenerationBasedStrategy(
            gmm_pipeline=gmm_pipeline,
            augmentation_ratio=kwargs.get('augmentation_ratio', 0.3)
        )
        strategy = HybridStrategy(
            selection_strategy,
            generation_strategy,
            selection_weight=kwargs.get('selection_weight', 0.7)
        )
    
    else:
        raise ValueError(f"æœªçŸ¥ç­–ç•¥ç±»å‹: {strategy_type}")
    
    # æ‰§è¡Œç”Ÿæˆ
    return strategy.generate(
        df_positive=df_positive,
        df_prediction_pool=df_prediction_pool,
        features=features,
        negative_ratio=kwargs.get('negative_ratio', 0.3),
        random_state=kwargs.get('random_state', 42)
    )




# é€‰æ‹©å¼è´Ÿæ ·æœ¬ç­–ç•¥
def generate_negative_samples_from_abandon(df_positive, df_prediction_pool, features_no_coords, 
                                           gmm_pipeline, negative_ratio=0.1, random_state=42,
                                           sampling_strategy="mixed", difficulty_levels=3):
    """
    åŸºäºå·²è®­ç»ƒGMM + å‚è€ƒåˆ†ä½æ ¡å‡†çš„è´Ÿæ ·æœ¬é‡‡æ ·ï¼ˆPIT åˆ†å±‚ï¼‰
    
    æ”¹è¿›ç‚¹ï¼š
    - ä½¿ç”¨å·²è®­ç»ƒçš„GMMï¼Œä¸é‡æ–°è®­ç»ƒ
    - åŸºäºå‚è€ƒé›†ï¼ˆæ­£æ ·æœ¬ï¼‰è®¡ç®—log-densityåˆ†ä½é˜ˆå€¼
    - ä½¿ç”¨PIT (percentile-in-training) è¿›è¡Œåˆ†å±‚é‡‡æ ·
    - é‡‡æ ·ä»…åŸºäºlog-density/PITï¼Œsigmoidåˆ†æ•°ä»…ç”¨äºå¯è§†åŒ–
    - è®­ç»ƒæ•°æ®ä¸åŒ…å«ä»»ä½•GMMæ‰“åˆ†åˆ—ï¼Œé¿å…æ³„éœ²
    
    Parameters:
    -----------
    df_positive : æ­£æ ·æœ¬æ•°æ®ï¼ˆä½œä¸ºå‚è€ƒé›†ï¼‰
    df_prediction_pool : é¢„æµ‹æ ·æœ¬æ± ï¼ˆä»ä¸­æŠ½å–è´Ÿæ ·æœ¬ï¼‰
    features_no_coords : ç‰¹å¾åˆ—è¡¨
    gmm_pipeline : å·²è®­ç»ƒå¥½çš„GMMç®¡é“
    negative_ratio : è´Ÿæ ·æœ¬æ¯”ä¾‹
    random_state : éšæœºç§å­
    sampling_strategy : str, é‡‡æ ·ç­–ç•¥ {"simple", "mixed", "hard"}
    difficulty_levels : int, éš¾åº¦åˆ†å±‚æ•°ï¼ˆé»˜è®¤3å±‚ï¼‰
    
    Returns:
    --------
    df_negative_samples : è¢«é€‰ä¸­çš„è´Ÿæ ·æœ¬
    df_remaining_prediction : å‰©ä½™çš„é¢„æµ‹æ ·æœ¬
    df_combined_training : ç»„åˆçš„è®­ç»ƒæ•°æ®ï¼ˆæ­£æ ·æœ¬+è´Ÿæ ·æœ¬ï¼‰
    """
    print("=" * 60)
    print("åŸºäºå·²è®­ç»ƒGMM + å‚è€ƒåˆ†ä½æ ¡å‡†çš„è´Ÿæ ·æœ¬é‡‡æ ·ï¼ˆPIT åˆ†å±‚ï¼‰")
    print("=" * 60)
    
    try:
        np.random.seed(random_state)
        rng = np.random.RandomState(random_state)
        
        # 0) Validate GMM pipeline
        if (gmm_pipeline is None or 
            not hasattr(gmm_pipeline, 'named_steps') or
            'preprocessor' not in gmm_pipeline.named_steps or 
            'gmm' not in gmm_pipeline.named_steps):
            raise ValueError("Invalid gmm_pipeline: must contain both 'preprocessor' and 'gmm' steps")

        pre = gmm_pipeline.named_steps['preprocessor']
        gmm = gmm_pipeline.named_steps['gmm']
        
        print(f"Using trained GMM model: {gmm.n_components} components, {gmm.covariance_type} covariance")
        
        # 1) Compute log-density for reference (positive) and prediction pool
        print("Computing log-density for reference set and prediction pool...")
        X_ref = pre.transform(df_positive[features_no_coords])
        logp_ref = gmm.score_samples(X_ref)
        
        X_pool = pre.transform(df_prediction_pool[features_no_coords])
        logp_pool = gmm.score_samples(X_pool)
        
        print(f"Reference set log-density: mean={np.mean(logp_ref):.3f}, std={np.std(logp_ref):.3f}")
        print(f"Prediction pool log-density: mean={np.mean(logp_pool):.3f}, std={np.std(logp_pool):.3f}")
        
        # 2) Compute PIT (Percentile-In-Training)
        # PIT(x) = P_ref(logp <= logp(x)), using sorted reference logp as empirical distribution
        print("Building reference empirical distribution and computing PIT...")
        ref_sorted = np.sort(logp_ref)
        
        def compute_pit(logp_values):
            """Compute PIT for given log-density values"""
            indices = np.searchsorted(ref_sorted, logp_values, side='right')
            return indices.astype(float) / len(ref_sorted)
        
        pit_pool = compute_pit(logp_pool)
        
        # 3) (Optional) Attach robust calibration parameters to pipeline
        try:
            gmm_pipeline = attach_env_calibration(
                gmm_pipeline, df_positive[features_no_coords], robust=True
            )
            print("âœ… Robust calibration parameters attached to pipeline")
        except Exception as e:
            print(f"Calibration attachment failed (does not affect sampling): {e}")
        
        # 4) Compute sigmoid scores and density (for visualization/audit only, not for sampling)
        print("Computing sigmoid scores (for visualization only)...")
        dens_pool = np.exp(logp_pool)
        
        # Use unified scoring function to compute sigmoid scores
        _, env_scores_pool, stats_pool = score_env(
            gmm_pipeline,
            df_prediction_pool[features_no_coords],
            method='sigmoid', 
            sigmoid_alpha=0.2,
            return_logdens=False
        )
        
        # 5) Add all scoring info to prediction pool
        df_pool_scored = df_prediction_pool.copy()
        df_pool_scored['original_id'] = np.arange(len(df_pool_scored))
        df_pool_scored['gmm_logp'] = logp_pool
        df_pool_scored['gmm_pit'] = pit_pool
        df_pool_scored['gmm_density'] = dens_pool
        df_pool_scored['gmm_score'] = env_scores_pool  # for visualization only
        
        # 6) Diagnostic statistics
        ref_quantiles = np.percentile(logp_ref, [5, 20, 40, 60, 80, 95])
        pit_quantiles = np.percentile(pit_pool, [5, 10, 25, 50, 75, 90])
        
        print(f"Reference log-density quantiles: Q5={ref_quantiles[0]:.3f}, Q20={ref_quantiles[1]:.3f}, Q40={ref_quantiles[2]:.3f}")
        print(f"Prediction pool PIT quantiles: Q5={pit_quantiles[0]:.3f}, Q10={pit_quantiles[1]:.3f}, Q25={pit_quantiles[2]:.3f}, Q50={pit_quantiles[3]:.3f}, Q75={pit_quantiles[4]:.3f}")

        # 7) Define PIT bin boundaries and strategy weights
        # ä»¥5%å’Œ25%åˆ†ä½æ•°ä¸ºä¸»è¦è´Ÿæ ·æœ¬é‡‡æ ·æºå¤´ï¼Œ5%æœ€é‡è¦ï¼Œ25%æ¬¡ä¹‹ï¼Œä¿ç•™diffcultyçš„åŠ¨æ€åˆ¤åˆ«æ¥å£
        if difficulty_levels == 3:
            # [0,5%)=æœ€å®¹æ˜“, [5%,25%)=æ¬¡å®¹æ˜“, [25%,60%)=éš¾
            pit_bins = [0.0, 0.05, 0.25, 0.60]
        elif difficulty_levels == 2:
            # [0,5%)=æœ€å®¹æ˜“, [5%,30%)=éš¾
            pit_bins = [0.0, 0.05, 0.30]
        else:
            # åŠ¨æ€åˆ†å±‚ï¼Œå‰5%å’Œ25%åˆ†ä½æ•°ä¸ºä¸»ï¼Œå‰©ä¸‹å‡åˆ†
            # å…ˆç”¨5%å’Œ25%åˆ†ä½æ•°ï¼Œå†å‡åˆ†å‰©ä½™åŒºé—´
            pit_bins = [0.0, 0.05, 0.25]
            if difficulty_levels > 2:
                # å‰©ä½™åŒºé—´ [0.25, 0.6] å‡åˆ†
                extra_bins = list(np.linspace(0.25, 0.6, difficulty_levels - 2 + 1))[1:]  
                pit_bins += extra_bins
        pit_bins = np.asarray(pit_bins, dtype=float)

        # Strategy weights definition
        # ä¸»è¦æƒé‡åˆ†é…ç»™5%å’Œ25%åˆ†ä½æ•°
        if difficulty_levels == 3:
            strategy_weights = {
                "simple": [1.0, 0.0, 0.0],      # 5%æœ€å¤šï¼Œ25%æ¬¡ä¹‹
                "mixed":  [0.6, 0.3, 0.1],
                "hard":   [0.5, 0.4, 0.1]
            }
        elif difficulty_levels == 2:
            strategy_weights = {
                "simple": [0.8, 0.2],
                "mixed":  [0.6, 0.4],
                "hard":   [0.4, 0.6]
            }
        else:
            # åŠ¨æ€åˆ†é…ï¼Œå‰ä¸¤ä¸ªbinæƒé‡é«˜ï¼Œå‰©ä¸‹å‡åˆ†
            def dynamic_weights(levels):
                w = np.ones(levels)
                w[0] = 0.5  # 5%åˆ†ä½æ•°
                w[1] = 0.3  # 25%åˆ†ä½æ•°
                if levels > 2:
                    w[2:] = 0.2 / (levels - 2)
                return w / w.sum()
            strategy_weights = {
                "simple": dynamic_weights(difficulty_levels),
                "mixed":  dynamic_weights(difficulty_levels),
                "hard":   dynamic_weights(difficulty_levels)
            }

        if sampling_strategy not in strategy_weights:
            print(f"âš ï¸ Unknown sampling strategy: {sampling_strategy}, using default: mixed")
            sampling_strategy = "mixed"

        base_weights = strategy_weights[sampling_strategy]
        layer_weights = np.asarray(base_weights[:difficulty_levels], dtype=float)
        layer_weights = layer_weights / layer_weights.sum()  # normalize
        
        # 8) Calculate required number of negative samples
        n_pos = len(df_positive)
        n_neg_total = int(max(1, round(n_pos * negative_ratio)))
        
        print(f"\nSampling configuration:")
        print(f"  Number of positive samples: {n_pos}")
        print(f"  Target negative samples: {n_neg_total} (ratio={negative_ratio:.2f})")
        print(f"  PIT bin boundaries: {pit_bins}")
        print(f"  Layer weights: {np.round(layer_weights, 3)}")
        print(f"  Sampling strategy: {sampling_strategy}")
        
        # 9) Perform PIT-based stratified sampling
        print("\nPerforming PIT stratified sampling...")
        layer_samples = []
        layer_info = []
        layer_names = ["Easy", "Medium", "Hard", "Very Hard", "Extremely Hard"]
        
        for i in range(difficulty_levels):
            lo, hi = pit_bins[i], pit_bins[i+1]
            layer_name = layer_names[i] if i < len(layer_names) else f"Layer {i+1}"
            
            # Define PIT range for current layer
            if i == difficulty_levels - 1:  # last layer uses closed interval
                mask = (df_pool_scored['gmm_pit'] >= lo) & (df_pool_scored['gmm_pit'] <= hi)
            else:
                mask = (df_pool_scored['gmm_pit'] >= lo) & (df_pool_scored['gmm_pit'] < hi)
            
            candidates = df_pool_scored[mask]
            need = int(round(n_neg_total * layer_weights[i]))
            
            print(f"  {layer_name} layer: PIT range [{lo:.3f},{hi:.3f}], candidates {len(candidates):,}, needed {need:,}")
            
            if len(candidates) == 0:
                print(f"    âš ï¸ No candidates in {layer_name} layer, skipping")
                continue
            
            # Random sampling within this layer
            if need >= len(candidates):
                print(f"    âš ï¸ Requested more than available in {layer_name} layer, using all")
                sampled = candidates
            else:
                sampled = candidates.sample(n=need, random_state=random_state + i)
            
            layer_samples.append(sampled)
            layer_info.append({
                'layer_name': layer_name,
                'pit_range': (float(lo), float(hi)),
                'n_candidates': int(len(candidates)),
                'n_sampled': int(len(sampled)),
                'weight': layer_weights[i],
                'mean_logp': float(sampled['gmm_logp'].mean()),
                'std_logp': float(sampled['gmm_logp'].std()),
                'mean_pit': float(sampled['gmm_pit'].mean()),
                'mean_score': float(sampled['gmm_score'].mean())  # for display only
            })
        
        # 10) Concatenate all sampled layers
        if not layer_samples:
            raise ValueError("No negative samples could be drawn from any layer. Please check bin boundaries or pool size.")
        
        df_negative_samples = pd.concat(layer_samples, ignore_index=True)
        
        # 11) Build remaining prediction pool
        picked_ids = set(df_negative_samples['original_id'])
        remaining_mask = ~df_pool_scored['original_id'].isin(picked_ids)
        df_remaining_prediction = df_pool_scored[remaining_mask].copy()
        
        # Data integrity check
        total_expected = len(df_prediction_pool)
        total_actual = len(df_negative_samples) + len(df_remaining_prediction)
        if total_actual != total_expected:
            print(f"âŒ Data inconsistency: expected {total_expected}, got {total_actual}")
            raise ValueError("Data integrity check failed")
        
        print(f"\nSampling results:")
        print(f"  Total negative samples drawn: {len(df_negative_samples):,}")
        print(f"  Remaining prediction samples: {len(df_remaining_prediction):,}")
        print(f"  âœ… Data integrity check passed")
        
        # 12) Build training data (without any GMM scoring columns to avoid leakage)
        print("\nBuilding training dataset (no GMM scoring columns, to avoid leakage)...")
        
        df_pos_train = df_positive[features_no_coords].copy()
        df_pos_train['label'] = 1
        df_pos_train['sample_type'] = 'positive'
        
        df_neg_train = df_negative_samples[features_no_coords].copy()
        df_neg_train['label'] = 0
        df_neg_train['sample_type'] = 'negative_sample'
        
        df_combined_training = pd.concat([df_pos_train, df_neg_train], ignore_index=True)
        
        print(f"Training set built:")
        print(f"  Positives: {len(df_pos_train):,} | Negatives: {len(df_neg_train):,}")
        print(f"  Total: {len(df_combined_training):,} | Positive ratio: {df_combined_training['label'].mean():.3f}")
        
        # 13) Detailed stratified statistics
        print(f"\nğŸ“Š PIT stratified sampling statistics:")
        print("-" * 60)
        for info in layer_info:
            print(f"{info['layer_name']} layer:")
            print(f"  PIT range: [{info['pit_range'][0]:.3f}, {info['pit_range'][1]:.3f}]")
            print(f"  Candidates/Sampled: {info['n_candidates']:,} / {info['n_sampled']:,}")
            print(f"  Mean log-density: {info['mean_logp']:.3f} Â± {info['std_logp']:.3f}")
            print(f"  Mean PIT: {info['mean_pit']:.3f}")
            print(f"  Mean sigmoid score: {info['mean_score']:.3f} (for reference only)")
        
        # 14) Quick quality validation
        print(f"\nValidating sampling quality...")
        try:
            from sklearn.model_selection import cross_val_score
            from sklearn.linear_model import LogisticRegression
            from sklearn.preprocessing import StandardScaler
            from sklearn.pipeline import Pipeline as SkPipeline
            
            # Prepare validation data (use training data, no GMM scores)
            X_pos = df_pos_train[features_no_coords]
            X_neg = df_neg_train[features_no_coords]
            X_combined = pd.concat([X_pos, X_neg], ignore_index=True)
            y_combined = np.concatenate([np.ones(len(X_pos)), np.zeros(len(X_neg))])
            
            # Simple validation pipeline
            val_pipeline = SkPipeline([
                ('scaler', StandardScaler()),
                ('lr', LogisticRegression(random_state=random_state, max_iter=1000))
            ])
            
            cv_scores = cross_val_score(val_pipeline, X_combined, y_combined, 
                                      cv=3, scoring='f1', n_jobs=-1)
            
            quality_score = np.mean(cv_scores)
            print(f"  Sampling quality (3-fold CV F1): {quality_score:.3f} Â± {np.std(cv_scores):.3f}")
            
            if quality_score > 0.8:
                print("  âœ… Good sampling quality")
            elif quality_score > 0.6:
                print("  âš ï¸ Moderate sampling quality, consider adjusting strategy")
            else:
                print("  âŒ Poor sampling quality, consider resampling")
                
        except Exception as e:
            print(f"  âš ï¸ Quality validation failed: {e}")
        
        # 15) Visualize sampling results
        try:
            visualize_sampling_results_pit(
                logp_pool, pit_pool, env_scores_pool, 
                df_negative_samples, pit_bins, layer_info
            )
        except Exception as e:
            print(f"â„¹ï¸ Sampling visualization failed (ignored): {e}")
        
        return df_negative_samples, df_remaining_prediction, df_combined_training
        
    except Exception as e:
        print(f"âŒ Error in generate_negative_samples_from_abandon: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None



# ç”Ÿæˆå¼è´Ÿæ ·æœ¬
def augment_negative_samples_with_generation(
    df_negative_samples, df_positive, features_no_coords, gmm_pipeline,
    augmentation_ratio=0.3, random_state=42):
    """
    ä½¿ç”¨GMMç”Ÿæˆå¼é‡‡æ ·å¢å¼ºè´Ÿæ ·æœ¬
    
    æ ¸å¿ƒé€»è¾‘ï¼š
    1. GMMå·²ç»å­¦åˆ°äº†æ­£æ ·æœ¬çš„åˆ†å¸ƒ
    2. ä»ä½æ¦‚ç‡åŒºåŸŸéšæœºé‡‡æ ·ç”Ÿæˆæ–°æ ·æœ¬
    3. ä½¿ç”¨æœ€è¿‘é‚»+å¾®æ‰°å°†é«˜ç»´æ ·æœ¬æ˜ å°„å›åŸå§‹ç‰¹å¾ç©ºé—´
    
    Parameters:
    -----------
    df_negative_samples : pd.DataFrame, å·²æœ‰è´Ÿæ ·æœ¬ï¼ˆä»é€‰æ‹©ç­–ç•¥è·å¾—ï¼‰
    df_positive : pd.DataFrame, æ­£æ ·æœ¬ï¼ˆç”¨ä½œå‚è€ƒï¼‰
    features_no_coords : List[str], ç‰¹å¾åˆ—è¡¨
    gmm_pipeline : Pipeline, å·²è®­ç»ƒçš„GMM
    augmentation_ratio : float, å¢å¹¿æ¯”ä¾‹
    random_state : int, éšæœºç§å­
    """
    print("\n" + "=" * 60)
    print("GMMç”Ÿæˆå¼è´Ÿæ ·æœ¬å¢å¹¿")
    print("=" * 60)
    
    np.random.seed(random_state)
    
    gmm = gmm_pipeline.named_steps['gmm']
    preprocessor = gmm_pipeline.named_steps['preprocessor']
    
    # 1) è®¡ç®—å‚è€ƒlog-densityï¼ˆæ­£æ ·æœ¬çš„å¯†åº¦ï¼‰
    X_pos_processed = preprocessor.transform(df_positive[features_no_coords])
    ref_logp = np.mean(gmm.score_samples(X_pos_processed))
    ref_std = np.std(gmm.score_samples(X_pos_processed))
    
    # 2) ä»GMMé‡‡æ ·ï¼ˆç”Ÿæˆé«˜ç»´æ ·æœ¬ï¼‰
    n_generate = int(len(df_negative_samples) * augmentation_ratio)
    generated_samples_highdim, _ = gmm.sample(n_generate * 3)  # å¤šç”Ÿæˆä¸€äº›ç”¨äºç­›é€‰
    
    # 3) ç­›é€‰ä½æ¦‚ç‡æ ·æœ¬ï¼ˆè¿œç¦»æ­£æ ·æœ¬åˆ†å¸ƒï¼‰
    generated_logps = gmm.score_samples(generated_samples_highdim)
    threshold = ref_logp - ref_std  # ä½äºå¹³å‡å¯†åº¦1ä¸ªæ ‡å‡†å·®
    
    low_density_mask = generated_logps < threshold
    candidate_samples = generated_samples_highdim[low_density_mask][:n_generate]
    
    print(f"\nç”Ÿæˆè¿‡ç¨‹:")
    print(f"  å‚è€ƒlog-density: {ref_logp:.3f} Â± {ref_std:.3f}")
    print(f"  ç­›é€‰é˜ˆå€¼: {threshold:.3f}")
    print(f"  ç”Ÿæˆå€™é€‰: {len(candidate_samples)}")
    
    if len(candidate_samples) < n_generate:
        print(f"âš ï¸ ç”Ÿæˆæ ·æœ¬ä¸è¶³ï¼Œä½¿ç”¨æ‰€æœ‰ {len(candidate_samples)} ä¸ª")
        n_generate = len(candidate_samples)
        candidate_samples = generated_samples_highdim[low_density_mask][:n_generate]
    
    # 4) é€†å˜æ¢ï¼šå°†é«˜ç»´æ ·æœ¬æ˜ å°„å›åŸå§‹ç‰¹å¾ç©ºé—´
    from scipy.spatial import cKDTree
    
    # æ„å»ºKDæ ‘
    tree = cKDTree(X_pos_processed)
    df_augmented = []
    
    for i, sample_highdim in enumerate(candidate_samples[:n_generate]):
        # æ‰¾æœ€è¿‘é‚»
        _, nearest_idx = tree.query(sample_highdim.reshape(1, -1), k=1)
        
        # ä½¿ç”¨æœ€è¿‘é‚»ç‰¹å¾ï¼Œæ·»åŠ æ§åˆ¶æ€§æ‰°åŠ¨
        base_features = df_positive[features_no_coords].iloc[nearest_idx[0]].copy()
        
        # æ·»åŠ å®šå‘æ‰°åŠ¨ï¼ˆå‘ä½å¯†åº¦åŒºåŸŸåç§»ï¼‰
        perturbation_scale = 0.15
        for col in base_features.index:
            if col in df_positive[features_no_coords].select_dtypes(include=[np.number]).columns:
                std_val = df_positive[col].std()
                # æ ¹æ®ç‰¹å¾é‡è¦æ€§è°ƒæ•´æ‰°åŠ¨æ–¹å‘
                perturbation = np.random.normal(0, perturbation_scale * std_val)
                base_features[col] += perturbation
        
        # éªŒè¯ç”Ÿæˆæ ·æœ¬çš„è´¨é‡
        sample_df = pd.DataFrame([base_features])[features_no_coords]
        sample_processed = preprocessor.transform(sample_df)
        sample_logp = gmm.score_samples(sample_processed)[0]
        
        base_features['gmm_logp'] = sample_logp
        base_features['generated'] = True  # æ ‡è®°ä¸ºç”Ÿæˆæ ·æœ¬
        
        df_augmented.append(base_features)
    
    df_augmented = pd.DataFrame(df_augmented)
    
    # 5) éªŒè¯å¢å¹¿è´¨é‡
    X_aug_processed = preprocessor.transform(df_augmented[features_no_coords])
    aug_logps = gmm.score_samples(X_aug_processed)
    
    print(f"\nå¢å¹¿æ ·æœ¬è´¨é‡:")
    print(f"  å¢å¹¿æ•°é‡: {len(df_augmented)}")
    print(f"  å¹³å‡log-density: {np.mean(aug_logps):.3f} (åº”ä½äºå‚è€ƒ: {ref_logp:.3f})")
    print(f"  å¯†åº¦æ¯”: {np.exp(np.mean(aug_logps) - ref_logp):.3f}")
    
    return df_augmented