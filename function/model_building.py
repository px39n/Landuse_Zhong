# -*- coding: utf-8 -*-
"""
æ¨¡å‹æ„å»ºæ¨¡å—
åŒ…å«æ‰€æœ‰æ¨¡å‹æ„å»ºå‡½æ•°ï¼šMLPã€Transformer+ResNetã€Random Forest
"""

from __future__ import annotations

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score
)
import joblib


_tf_model_building = None
_keras_model_building = None
_layers_model_building = None
TENSORFLOW_AVAILABLE = False

def _ensure_tensorflow_model_building():
    """ç¡®ä¿ TensorFlow å·²å¯¼å…¥ï¼ˆç”¨äº model_building æ¨¡å—ï¼‰"""
    global _tf_model_building, _keras_model_building, _layers_model_building, TENSORFLOW_AVAILABLE
    if TENSORFLOW_AVAILABLE and _tf_model_building is not None:
        return True
    
    try:
        import sys
        # ä¸´æ—¶å¢åŠ é€’å½’æ·±åº¦é™åˆ¶ï¼ˆTensorFlow 2.15 å¯èƒ½éœ€è¦ï¼‰
        original_recursion_limit = sys.getrecursionlimit()
        sys.setrecursionlimit(max(original_recursion_limit, 3000))
        
        import tensorflow as tf
        print(f"âœ… [model_building] TensorFlow å¯¼å…¥æˆåŠŸï¼Œç‰ˆæœ¬: {tf.__version__}")
        

        keras = tf.keras
        layers = keras.layers
        
        # æ¢å¤é€’å½’æ·±åº¦é™åˆ¶
        sys.setrecursionlimit(original_recursion_limit)
        
        # éªŒè¯å¯¼å…¥æ˜¯å¦æˆåŠŸ
        if keras is None:
            raise ImportError("keras is None after import")
        if layers is None:
            raise ImportError("layers is None after import")
        
        _tf_model_building = tf
        _keras_model_building = keras
        _layers_model_building = layers
        TENSORFLOW_AVAILABLE = True
        print(f"âœ… [model_building] Keras å’Œ Layers å¯¼å…¥æˆåŠŸ")
        return True
    except RecursionError as e:
        TENSORFLOW_AVAILABLE = False
        print(f"âŒ [model_building] TensorFlow å¯¼å…¥é€’å½’é”™è¯¯: {e}")
        print("ğŸ’¡ æç¤º: è¿™å¯èƒ½æ˜¯ TensorFlow 2.15 çš„å·²çŸ¥é—®é¢˜ï¼Œå°è¯•ä½¿ç”¨ tf.keras")
        import traceback
        traceback.print_exc()
        return False
    except Exception as e:
        TENSORFLOW_AVAILABLE = False
        print(f"âŒ [model_building] TensorFlow å¯¼å…¥å¤±è´¥: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return False

# å»¶è¿Ÿå¯¼å…¥ï¼šä¸åœ¨æ¨¡å—çº§åˆ«ç«‹å³è°ƒç”¨ï¼Œé¿å…å¾ªç¯å¯¼å…¥é—®é¢˜
# _ensure_tensorflow_model_building()  # æ³¨é‡Šæ‰ï¼Œæ”¹ä¸ºåœ¨éœ€è¦æ—¶è°ƒç”¨

# ä¸ºäº†å‘åå…¼å®¹ï¼Œè®¾ç½®å…¨å±€å˜é‡
if TENSORFLOW_AVAILABLE:
    tf = _tf_model_building
    keras = _keras_model_building
    layers = _layers_model_building


def build_deep_learning_model(input_dim, hidden_layers=[128, 64, 32],
                              dropout_rate=0.3, learning_rate=0.001):
    """æ„å»ºæ·±åº¦å­¦ä¹ MLPæ¨¡å‹"""
    # ç¡®ä¿ TensorFlow å¯ç”¨ï¼ˆè¿è¡Œæ—¶é‡è¯•ï¼‰
    if not _ensure_tensorflow_model_building():
        raise ImportError("TensorFlow not available, cannot build deep learning model")
    
    keras = _keras_model_building
    layers = _layers_model_building

    print("Building deep learning model...")
    
    inputs = keras.Input(shape=(input_dim,))
    x = layers.BatchNormalization()(inputs)
    for units in hidden_layers:
        x = layers.Dense(units, activation='relu',
                         kernel_regularizer=keras.regularizers.l2(1e-2))(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(dropout_rate)(x)
    outputs = layers.Dense(1, activation='sigmoid')(x)
    model = keras.Model(inputs=inputs, outputs=outputs)
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss='binary_crossentropy',
        metrics=[
            keras.metrics.AUC(name='auc'),
            keras.metrics.Precision(name='precision'),
            keras.metrics.Recall(name='recall'),
            'accuracy'
        ]
    )
    print(f"Input dim: {input_dim} | Hidden: {hidden_layers} | Params: {model.count_params():,}")
    return model


def build_transformer_resnet_model(
    input_dim, d_model=64, num_heads=4, num_transformer_layers=2,
    resnet_layers=[128, 128, 64], dropout_rate=0.3, learning_rate=0.001):
    """
    æ„å»º Transformer + ResNet æ··åˆæ¨¡å‹
    
    æ¶æ„è®¾è®¡ï¼š
    - Transformer Encoder: å­¦ä¹ ç‰¹å¾é—´çš„é•¿è·ç¦»ä¾èµ–
    - ResNet Branch: æ®‹å·®è¿æ¥ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±
    - åŒåˆ†æ”¯èåˆ: ç»“åˆä¸¤ç§æ¶æ„ä¼˜åŠ¿
    """
    # ç¡®ä¿ TensorFlow å¯ç”¨ï¼ˆè¿è¡Œæ—¶é‡è¯•ï¼‰
    if not _ensure_tensorflow_model_building():
        # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        error_msg = (
            "TensorFlow not available in model_building. "
            "Please check the error messages above for details. "
            "This may be due to: "
            "1) TensorFlow not installed in the environment, "
            "2) TensorFlow version incompatibility, "
            "3) Missing dependencies (e.g., CUDA libraries)."
        )
        raise ImportError(error_msg)
    
    keras = _keras_model_building
    layers = _layers_model_building
    
    print("Building Transformer+ResNet Hybrid Model...")
    
    # è¾“å…¥å±‚
    inputs = keras.Input(shape=(input_dim,), name='features')
    
    # === 1) Transformeråˆ†æ”¯ ===
    # é‡å¡‘ä¸ºåºåˆ—ï¼š(batch, seq_len, d_model)
    seq_len = input_dim
    x_tf = layers.Reshape((seq_len, 1))(inputs)
    x_tf = layers.Dense(d_model)(x_tf)
    
    # ä½ç½®ç¼–ç ï¼ˆå¯å­¦ä¹ çš„ï¼‰
    x_tf_norm = layers.LayerNormalization()(x_tf)
    
    # Transformerç¼–ç å™¨å±‚
    for i in range(num_transformer_layers):
        # Multi-Head Self-Attention
        attn_output = layers.MultiHeadAttention(
            num_heads=num_heads,
            key_dim=d_model // num_heads,
            dropout=dropout_rate,
            name=f'transformer_attn_{i}'
        )(x_tf_norm, x_tf_norm)
        
        # Add & Norm
        x_tf_norm = layers.LayerNormalization()(x_tf_norm + attn_output)
        
        # Feed Forward Network
        ffn_out = keras.Sequential([
            layers.Dense(d_model * 4, activation='relu'),
            layers.Dropout(dropout_rate),
            layers.Dense(d_model)
        ], name=f'transformer_ffn_{i}')(x_tf_norm)
        
        # Add & Norm
        x_tf_norm = layers.LayerNormalization()(x_tf_norm + ffn_out)
    
    # å…¨å±€æ± åŒ–
    x_tf = layers.GlobalAveragePooling1D()(x_tf_norm)
    
    # === 2) ResNetåˆ†æ”¯ ===
    x_resnet = inputs
    x_resnet = layers.BatchNormalization()(x_resnet)
    
    for i, units in enumerate(resnet_layers):
        residual = x_resnet  

        # ä¸»è·¯å¾„
        x_resnet = layers.Dense(
            units,
            activation='relu',
            kernel_regularizer=keras.regularizers.l2(1e-4),
        )(x_resnet)
        x_resnet = layers.BatchNormalization()(x_resnet)
        x_resnet = layers.Dropout(dropout_rate)(x_resnet)

        # æŠŠ residual ä¹Ÿæ˜ å°„åˆ° units ç»´åº¦
        if residual.shape[-1] != units:
            residual = layers.Dense(
                units,
                use_bias=False,
                kernel_regularizer=keras.regularizers.l2(1e-4),
            )(residual)
            residual = layers.BatchNormalization()(residual)

        x_resnet = layers.Add()([residual, x_resnet])
        x_resnet = layers.Activation('relu')(x_resnet)
    
    # === 3) èåˆä¸¤ä¸ªåˆ†æ”¯ ===
    x_fused = layers.Concatenate()([x_tf, x_resnet])
    x_fused = layers.Dense(32, activation='relu')(x_fused)
    x_fused = layers.BatchNormalization()(x_fused)
    x_fused = layers.Dropout(dropout_rate)(x_fused)
    
    # è¾“å‡ºå±‚
    outputs = layers.Dense(1, activation='sigmoid', name='prediction')(x_fused)
    
    model = keras.Model(inputs=inputs, outputs=outputs)
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss='binary_crossentropy',
        metrics=[
            keras.metrics.AUC(name='auc'),
            keras.metrics.Precision(name='precision'),
            keras.metrics.Recall(name='recall'),
            'accuracy'
        ]
    )
    
    print(f"  Input dim: {input_dim} | Transformer layers: {num_transformer_layers} | Params: {model.count_params():,}")
    return model


class RandomForestWrapper:
    """åŒ…è£…Random Forestä»¥ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥å£ä¸€è‡´"""
    def __init__(self, n_estimators=100, max_depth=15, random_state=42):
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=random_state,
            n_jobs=-1,
            verbose=0
        )
        self.random_state = random_state
        
    def fit(self, X, y, verbose=0):
        self.model.fit(X, y)
        return self
    
    def predict(self, X, verbose=0):
        # è¿”å›æ¦‚ç‡ä»¥åŒ¹é…æ·±åº¦å­¦ä¹ æ–¹æ³•
        return self.model.predict_proba(X)[:, 1:2]  # shape: (n, 1)
    
    def evaluate(self, X_test, y_test):
        """è¿”å›ä¸Kerasæ¨¡å‹ä¸€è‡´çš„metrics"""
        y_pred_proba = self.predict(X_test).ravel()
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        return {
            'loss': 1 - accuracy_score(y_test, y_pred),  # ä¸kerasçš„lossæ¥å£ä¸€è‡´
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred),
            'recall': recall_score(y_test, y_pred),
            'f1': f1_score(y_test, y_pred),
            'auc': roc_auc_score(y_test, y_pred_proba)
        }
    
    def save(self, filepath):
        joblib.dump(self.model, filepath)
        
    def load(self, filepath):
        self.model = joblib.load(filepath)

