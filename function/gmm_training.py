# -*- coding: utf-8 -*-
"""
æ•´åˆç‰ˆæœ¬çš„GMM+æ·±åº¦å­¦ä¹ è®­ç»ƒç®¡é“
- ç»Ÿä¸€çš„GMMè®­ç»ƒå’Œæ·±åº¦å­¦ä¹ æµç¨‹
- è§£å†³æ•°æ®æ³„éœ²ï¼šé¢„å¤„ç†å™¨åœ¨Pipelineä¸­
- å®Œæ•´çš„è´Ÿæ ·æœ¬é‡‡æ ·å’Œæ¨¡å‹è¯„ä¼°
- ç®€åŒ–çš„APIè®¾è®¡

Author: you + ChatGPT 
"""
from __future__ import annotations

import os
import warnings
from typing import List, Tuple, Dict, Any, Optional, Sequence

from sklearn.pipeline import Pipeline as SkPipeline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib
from sklearn.base import BaseEstimator, TransformerMixin, clone
from sklearn.compose import make_column_transformer
from sklearn.impute import SimpleImputer
from sklearn.mixture import GaussianMixture
from sklearn.model_selection import GridSearchCV, train_test_split, learning_curve, StratifiedKFold
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import (
    OrdinalEncoder,
    QuantileTransformer,
    RobustScaler,
    StandardScaler,
    PowerTransformer,
    FunctionTransformer,
    OneHotEncoder,
)
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_curve, auc, mean_squared_error, mean_absolute_error,
    roc_auc_score
)
from tqdm.auto import tqdm
from typing import Dict, List, Optional, Tuple


SkPipeline = Pipeline

# æ·±åº¦å­¦ä¹ åº“
try:
    import tensorflow as tf
    # ä½¿ç”¨ tf.keras è€Œä¸æ˜¯ from tensorflow import kerasï¼ˆé¿å…é€’å½’é”™è¯¯ï¼‰
    # TensorFlow 2.15 å…¼å®¹æ–¹å¼
    keras = tf.keras
    layers = keras.layers  # ä½¿ç”¨ keras.layers è€Œä¸æ˜¯ tensorflow.keras.layers
    TENSORFLOW_AVAILABLE = True
    print("[OK] TensorFlow available")
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("[WARN] TensorFlow not available")
except RecursionError as e:
    # æ•è·é€’å½’é”™è¯¯ï¼ˆTensorFlow 2.15 çš„å·²çŸ¥é—®é¢˜ï¼‰
    TENSORFLOW_AVAILABLE = False
    print(f"[WARN] TensorFlow import recursion error: {e}")
    print("[INFO] This may be a TensorFlow 2.15 compatibility issue")
except Exception as e:
    # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œä¸ä»…ä»…æ˜¯ ImportError
    TENSORFLOW_AVAILABLE = False
    print(f"[WARN] TensorFlow not available: {type(e).__name__}: {e}")

# scikeras å»¶è¿Ÿå¯¼å…¥ï¼ˆåœ¨ TensorFlow æˆåŠŸå¯¼å…¥åï¼Œé¿å… keras.api å…¼å®¹æ€§é—®é¢˜ï¼‰
SCIKERAS_AVAILABLE = False
KerasClassifier = None

def _ensure_scikeras():
    """ç¡®ä¿ scikeras å·²å¯¼å…¥ï¼Œå¤„ç† keras.api å…¼å®¹æ€§é—®é¢˜"""
    global SCIKERAS_AVAILABLE, KerasClassifier
    if SCIKERAS_AVAILABLE and KerasClassifier is not None:
        return True
    
    if not TENSORFLOW_AVAILABLE:
        return False
    
    try:
        # åœ¨ TensorFlow å·²å¯¼å…¥çš„æƒ…å†µä¸‹ï¼Œå»¶è¿Ÿå¯¼å…¥ scikeras
        from scikeras.wrappers import KerasClassifier as _KerasClassifier
        KerasClassifier = _KerasClassifier
        SCIKERAS_AVAILABLE = True
        print("[OK] scikeras available")
        return True
    except ModuleNotFoundError as e:
        if 'keras.api' in str(e):
            print(f"[WARN] scikeras version incompatible with TensorFlow 2.11: {e}")
            print("[INFO] Try upgrading scikeras: pip install --upgrade scikeras>=0.12.0")
        SCIKERAS_AVAILABLE = False
        return False
    except ImportError:
        SCIKERAS_AVAILABLE = False
        print("[WARN] scikeras not available (pip install scikeras)")
        return False
    except Exception as e:
        SCIKERAS_AVAILABLE = False
        print(f"[WARN] scikeras import failed: {type(e).__name__}: {e}")
        return False

# å¦‚æœ TensorFlow å·²å¯ç”¨ï¼Œå°è¯•å¯¼å…¥ scikeras
if TENSORFLOW_AVAILABLE:
    _ensure_scikeras()

# SHAPï¼ˆå¯é€‰ï¼‰
try:
    import shap
    SHAP_AVAILABLE = True
    print("[OK] SHAP available")
except ImportError:
    SHAP_AVAILABLE = False
    print("[WARN] SHAP not available")
except Exception as e:
    # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œä¸ä»…ä»…æ˜¯ ImportError
    SHAP_AVAILABLE = False
    print(f"[WARN] SHAP not available: {type(e).__name__}: {e}")

# å¯è§†åŒ–è®¾ç½®
plt.style.use('default')
plt.rcParams['font.size'] = 10
plt.rcParams['axes.grid'] = True
plt.rcParams['grid.alpha'] = 0.3

# ------------------------------
# ä¿®å¤ï¼šç»„åˆé¢„å¤„ç†å™¨ï¼ˆè§£å†³æ•°æ®æ³„éœ²ï¼‰
# ------------------------------


class CombinedPreprocessor(BaseEstimator, TransformerMixin):
    """
    ç»„åˆé¢„å¤„ç†å™¨ï¼šæ•°å€¼ + ç±»åˆ«ï¼ˆOne-Hotï¼‰ï¼Œç¡®ä¿ç‰¹å¾ç»´åº¦ä¸€è‡´æ€§
    """
    def __init__(self, numeric_features: List[str], categorical_features: List[str]):
        self.numeric_features = numeric_features
        self.categorical_features = categorical_features
        self.numeric_preprocessor = None
        self.categorical_preprocessor = None
        self.feature_names_out_ = None
        
        # âœ… å…³é”®ä¿®å¤ï¼šé¢„å®šä¹‰landcoverçš„æ‰€æœ‰å¯èƒ½ç±»åˆ«
        self.known_landcover_categories = [1, 2, 3, 4, 5, 6, 7, 8, 9]  # å›ºå®š1-9ç±»åˆ«
        
        # æ•°å€¼ç‰¹å¾é¢„å¤„ç†å™¨
        if self.numeric_features:
            self.numeric_preprocessor = Pipeline([
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler())
            ])
        
        # âœ… ç±»åˆ«ç‰¹å¾é¢„å¤„ç†å™¨ - å›ºå®šç±»åˆ«
        if self.categorical_features:
            self.categorical_preprocessor = OneHotEncoder(
                categories=[self.known_landcover_categories],  # å›ºå®šç±»åˆ«
                sparse_output=False, 
                handle_unknown='ignore',  # å¿½ç•¥æœªçŸ¥ç±»åˆ«
                drop=None  # ä¿ç•™æ‰€æœ‰ç±»åˆ«
            )

    def fit(self, X, y=None):
        """æ‹Ÿåˆé¢„å¤„ç†å™¨"""
        # ç¡®ä¿è¾“å…¥æ˜¯DataFrameå¹¶ä¸”åˆ—å­˜åœ¨
        if not isinstance(X, pd.DataFrame):
            raise ValueError("è¾“å…¥å¿…é¡»æ˜¯pandas DataFrame")
        
        # æ£€æŸ¥ç‰¹å¾åˆ—æ˜¯å¦å­˜åœ¨
        missing_features = [f for f in (self.numeric_features + self.categorical_features) if f not in X.columns]
        if missing_features:
            raise ValueError(f"ä»¥ä¸‹ç‰¹å¾åˆ—åœ¨è¾“å…¥æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_features}")
        
        # æ‹Ÿåˆæ•°å€¼ç‰¹å¾é¢„å¤„ç†å™¨
        if self.numeric_features:
            self.numeric_preprocessor.fit(X[self.numeric_features])
        
        # æ‹Ÿåˆç±»åˆ«ç‰¹å¾é¢„å¤„ç†å™¨
        if self.categorical_features:
            # âœ… ç¡®ä¿landcoveråˆ—æ˜¯æ•´æ•°ç±»å‹
            X_cat = X[self.categorical_features].copy()
            for col in self.categorical_features:
                if col == 'landcover':
                    X_cat[col] = X_cat[col].astype(int)
            
            # âœ… å…³é”®ä¿®å¤ï¼šé‡æ–°åˆå§‹åŒ–OneHotEncoderå¹¶å¼ºåˆ¶è®¾ç½®categories
            self.categorical_preprocessor = OneHotEncoder(
                categories=[self.known_landcover_categories],  # é‡æ–°è®¾ç½®å›ºå®šç±»åˆ«
                sparse_output=False, 
                handle_unknown='ignore',
                drop=None
            )
            self.categorical_preprocessor.fit(X_cat)
            
        # âœ… ç”Ÿæˆå›ºå®šçš„ç‰¹å¾åç§°
        self._generate_feature_names()
        return self

    def transform(self, X):
        """è½¬æ¢æ•°æ®"""
        if not isinstance(X, pd.DataFrame):
            raise ValueError("è¾“å…¥å¿…é¡»æ˜¯pandas DataFrame")
        
        # æ£€æŸ¥æ˜¯å¦å·²æ‹Ÿåˆ
        if ((self.numeric_features and self.numeric_preprocessor is None) or 
            (self.categorical_features and self.categorical_preprocessor is None)):
            raise ValueError("å¿…é¡»å…ˆè°ƒç”¨fitæ–¹æ³•")
        
        results = []
        
        # è½¬æ¢æ•°å€¼ç‰¹å¾
        if self.numeric_features:
            X_num_transformed = self.numeric_preprocessor.transform(X[self.numeric_features])
            results.append(X_num_transformed)
        
        # è½¬æ¢ç±»åˆ«ç‰¹å¾
        if self.categorical_features:
            X_cat = X[self.categorical_features].copy()
            for col in self.categorical_features:
                if col == 'landcover':
                    X_cat[col] = X_cat[col].astype(int)
            
            X_cat_transformed = self.categorical_preprocessor.transform(X_cat)
            results.append(X_cat_transformed)
        
        # åˆå¹¶ç»“æœ
        if results:
            return np.hstack(results)
        else:
            return np.array([]).reshape(X.shape[0], 0)

    def _generate_feature_names(self):
        """ç”Ÿæˆç‰¹å¾åç§°"""
        feature_names = []
        
        # æ•°å€¼ç‰¹å¾åç§°
        if self.numeric_features:
            feature_names.extend(self.numeric_features)
        
        # âœ… ç±»åˆ«ç‰¹å¾åç§° - å›ºå®šç”Ÿæˆ
        if self.categorical_features:
            for col in self.categorical_features:
                cat_names = self.categorical_preprocessor.get_feature_names_out([col])
                feature_names.extend(cat_names)
        
        self.feature_names_out_ = np.array(feature_names)

    def get_feature_names_out(self, input_features=None):
        """è·å–è¾“å‡ºç‰¹å¾åç§°"""
        if self.feature_names_out_ is None:
            raise ValueError("å¿…é¡»å…ˆè°ƒç”¨fitæ–¹æ³•")
        return self.feature_names_out_

    def get_params(self, deep=True):
        """è·å–å‚æ•°"""
        return {
            'numeric_features': self.numeric_features,
            'categorical_features': self.categorical_features
        }

    def set_params(self, **params):
        """è®¾ç½®å‚æ•°"""
        for key, value in params.items():
            setattr(self, key, value)
        return self


#---------------------------
# Top-level, picklable transformers (no lambdas!)
# ------------------------------
class ReplaceInfWithNaN(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        X = np.asarray(X, dtype=float)
        X[~np.isfinite(X)] = np.nan
        return X

class SafeLog1p(BaseEstimator, TransformerMixin):
    def __init__(self, lower_bound: float = -1 + 1e-6):
        self.lower_bound = lower_bound
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        X = np.asarray(X, dtype=float)
        X = np.where(X <= self.lower_bound, self.lower_bound, X)
        return np.log1p(X)

# ------------------------------
# Utilities
# ------------------------------
def get_adaptive_n_quantiles(n_samples: int) -> int:
    """Adaptive n_quantiles for QuantileTransformer: 10..1000 and â‰¤ n_samples."""
    return int(max(10, min(1000, n_samples)))

# ------------------------------
# Light-weight quality check helpers
# ------------------------------
def _cov_condition_number(X: np.ndarray) -> Dict[str, float]:
    X = np.asarray(X, dtype=float)
    C = np.cov(X, rowvar=False)
    C += np.eye(C.shape[0]) * 1e-12
    try:
        w = np.linalg.eigvalsh(C)
        w = np.clip(w, 0.0, None)
        w_min = float(np.min(w))
        w_max = float(np.max(w))
        cond = float(w_max / (w_min + 1e-18))
        return {"min_eig": w_min, "max_eig": w_max, "condition_number": cond, "eigen_ratio": cond}
    except np.linalg.LinAlgError:
        return {"min_eig": np.nan, "max_eig": np.nan, "condition_number": np.inf, "eigen_ratio": np.inf}

def comprehensive_data_quality_check(
    X: np.ndarray, feature_names: List[str] | None = None, verbose: bool = True
) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
    X = np.asarray(X)
    n, d = X.shape
    var = np.nanvar(X, axis=0)
    zero_thr = 1e-12
    low_thr = 1e-4

    report: Dict[str, Any] = {
        "shape": (n, d),
        "nan_count": int(np.isnan(X).sum()),
        "inf_count": int(np.isinf(X).sum()),
        "zero_count": int((X == 0).sum()),
        "variance_range": (float(np.nanmin(var)), float(np.nanmax(var))),
        "zero_variance_count": int(np.sum(var <= zero_thr)),
        "low_variance_count": int(np.sum(var <= low_thr)),
    }
    cov_info = _cov_condition_number(X)
    report["covariance_analysis"] = cov_info

    if verbose:
        print("æ•°æ®å½¢çŠ¶:", report["shape"])
        print("NaNå€¼æ•°é‡:", report["nan_count"])
        print("Infå€¼æ•°é‡:", report["inf_count"])
        print("é›¶å€¼æ•°é‡:", report["zero_count"])
        print("æ–¹å·®èŒƒå›´:", "[%.2e, %.2e]" % report["variance_range"])
        print("é›¶æ–¹å·®ç‰¹å¾æ•°:", report["zero_variance_count"], "ï¼›ä½æ–¹å·®ç‰¹å¾æ•°:", report["low_variance_count"])
        print("åæ–¹å·®çŸ©é˜µæ¡ä»¶æ•°: %.2e" % cov_info["condition_number"])
        print("æœ€å°/æœ€å¤§ç‰¹å¾å€¼: %.2e / %.2e" % (cov_info["min_eig"], cov_info["max_eig"]))

    recs: List[Dict[str, Any]] = []
    return report, recs



def select_and_train_gmm(df_pos: pd.DataFrame, bandwidths=None, use_bic=False):

    """
    ä¿®å¤ç‰ˆæœ¬çš„GMMè®­ç»ƒå‡½æ•°ï¼ˆå‡çº§ç‰ˆï¼‰ï¼š
    - è§£å†³æ•°æ®æ³„éœ²ï¼ˆé¢„å¤„ç†å™¨è¿›Pipelineï¼‰ï¼›
    - æœç´¢ n_init ä¸ reg_covarï¼›
    - æ‰“å°å•ç»„åˆæ’è¡Œæ¦œï¼›
    - åˆ†ç±»å‹ç»˜å›¾å¹¶å¸¦è¯¯å·®æ¡ã€‚
    """
    print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {df_pos.shape}")

    if "landcover" in df_pos.columns:
        landcover_values = df_pos["landcover"].value_counts().sort_index()
        print("\næ£€æŸ¥ landcover åˆ†å¸ƒ:")
        print(f"å”¯ä¸€å€¼: {sorted(df_pos['landcover'].unique())}")
        print(f"åˆ†å¸ƒ: {dict(landcover_values)}")

    LOG = ['GDPpc', 'GDPtot', 'Population', 'Powerdist']
    DEM_SLOPE = ['DEM', 'Slope']
    DIST = ['GURdist', 'PrimaryRoad', 'SecondaryRoad', 'TertiaryRoad']
    NORMAL = ['tas', 'gdmp', 'rsds', 'wind']
    CAT = ['landcover']

    all_numeric_features = LOG + DEM_SLOPE + DIST + NORMAL
    available_numeric = [f for f in all_numeric_features if f in df_pos.columns]
    available_categorical = [f for f in CAT if f in df_pos.columns]
    if not available_numeric and not available_categorical:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ç”¨çš„ç‰¹å¾åˆ—")

    # é¢„å¤„ç†å™¨
    print("\n åˆ›å»ºç»„åˆé¢„å¤„ç†å™¨...")
    combined_preprocessor = CombinedPreprocessor(available_numeric, available_categorical)

    # é¢„å¤„ç†æ¢æŸ¥
    print("æµ‹è¯•é¢„å¤„ç†å™¨...")
    test_preprocessor = CombinedPreprocessor(available_numeric, available_categorical)
    X_test = test_preprocessor.fit_transform(df_pos)
    print(f"é¢„å¤„ç†åç‰¹å¾å½¢çŠ¶: {X_test.shape}")

    # è´¨é‡æ£€æŸ¥
    quality_report, _ = comprehensive_data_quality_check(
        X_test, feature_names=[f"f{i}" for i in range(X_test.shape[1])], verbose=True
    )
    # print("\næ•°æ®è´¨é‡æ¦‚è¦ï¼š")
    # print(f"NaN: {quality_report['nan_count']}  |  Inf: {quality_report['inf_count']}")
    # print("æ–¹å·®èŒƒå›´: [%.2e, %.2e]" % quality_report["variance_range"])

    cond = quality_report["covariance_analysis"]["condition_number"]
    if cond > 1e12:
        # print("âš ï¸ æ•°å€¼è¾ƒä¸ç¨³å®šï¼Œä½¿ç”¨ä¿å®ˆå‚æ•°")
        gmm_params = dict(n_components=1, covariance_type="diag", reg_covar=1e-3, random_state=0)
    else:
        print("âœ… æ•°å€¼ç¨³å®šï¼Œä½¿ç”¨æ ‡å‡†å‚æ•°")
        gmm_params = dict(n_components=1, covariance_type="full", reg_covar=1e-6, random_state=0)

    # å®Œæ•´Pipeline
    print("\næ„å»ºå®Œæ•´Pipelineï¼ˆåŒ…å«é¢„å¤„ç†å™¨ï¼‰...")
    full_pipe = Pipeline([
        ("preprocessor", combined_preprocessor),
        ("gmm", GaussianMixture(**gmm_params)),
    ])

    # å‚æ•°ç½‘æ ¼
    if bandwidths is not None and np.size(bandwidths) > 0:
        comps = sorted({int(max(1, round(float(b)))) for b in np.ravel(bandwidths)})
    else:
        n_samples = len(df_pos)
        max_k = min(35, n_samples // 50)  
        comps = list(range(15, max_k + 1, 2))
    
    cov_types = ["diag", "full"]
    n_init_list = [20]
    reg_list = [1e-7, 1e-6]
    
    param_grid = {
        "gmm__n_components": comps,
        "gmm__covariance_type": cov_types,
        "gmm__n_init": n_init_list,
        "gmm__reg_covar": reg_list,
    }
    
    if use_bic:
        def gmm_bic_scorer(estimator, X, y=None):
            Xt = estimator[:-1].transform(X)
            return -estimator[-1].bic(Xt)
        scoring = gmm_bic_scorer
        print("ä½¿ç”¨BICä½œä¸ºè¯„åˆ†æ ‡å‡†")
    else:
        scoring = None  # ä½¿ç”¨ Pipeline.score -> GMM çš„å¹³å‡å¯¹æ•°ä¼¼ç„¶
        print("ä½¿ç”¨å¯¹æ•°ä¼¼ç„¶ä½œä¸ºè¯„åˆ†æ ‡å‡†")

    
    grid = GridSearchCV(
        estimator=full_pipe,
        param_grid=param_grid,
        cv=5,
        scoring=scoring,  # âœ… å¯é€‰çš„è¯„åˆ†æ ‡å‡†
        n_jobs=-1,
        refit=True,
        verbose=1,
        error_score="raise"
    )

    # è¿›åº¦æ¡ï¼ˆæŒ‰ç½‘æ ¼è§„æ¨¡ç²—ç•¥ä¼°è®¡ï¼‰
    total_iters = len(comps) * len(cov_types) * len(n_init_list) * len(reg_list)
    print("\nå¼€å§‹è®­ç»ƒ...")
    with tqdm(total=total_iters, desc="GMMè®­ç»ƒ") as pbar:
        grid.fit(df_pos)
        pbar.update(total_iters)

    # ç»“æœ
    best_params = grid.best_params_
    best_score  = grid.best_score_
    best_pipe   = grid.best_estimator_

    print("\n" + "=" * 60)
    print("è®­ç»ƒå®Œæˆï¼æœ€ä½³å‚æ•°:")
    print("=" * 60)
    print(f"n_components   : {best_params['gmm__n_components']}")
    print(f"covariance_type: {best_params['gmm__covariance_type']}")
    print(f"n_init         : {best_params['gmm__n_init']}")
    print(f"reg_covar      : {best_params['gmm__reg_covar']:.1e}")
    print(f"æœ€ä½³CVå‡å€¼å¯¹æ•°ä¼¼ç„¶: {best_score:.6f}")

    # --- æ‰“å°æ’è¡Œæ¦œï¼ˆå•ç»„åˆç²’åº¦ï¼‰ ---
    res = pd.DataFrame(grid.cv_results_)
    cols = [
        "param_gmm__n_components",
        "param_gmm__covariance_type",
        "param_gmm__n_init",
        "param_gmm__reg_covar",
        "mean_test_score",
        "std_test_score",
        "rank_test_score",
    ]
    leaderboard = res[cols].sort_values("mean_test_score", ascending=False)
    print("\nTop-15 å•ç»„åˆæ’è¡Œæ¦œï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰:")
    print(leaderboard.head(15).to_string(index=False))

    # --- ä¸¤æ¡æ›²çº¿ + è¯¯å·®æ¡ï¼ˆæ¯ä¸ª covã€K é€‰å–è¯¥ç»„æœ€ä½³ç»„åˆçš„ stdï¼‰ ---
    try:
        plot_cv_by_covariance_with_errorbars(grid.cv_results_, best_params)
    except Exception as e:
        print(f"âš ï¸ åˆ†ç±»å‹è¯¯å·®æ¡ç»˜å›¾å¤±è´¥: {e}")

    # ç»å…¸çš„ï¼ˆè·¨ç±»å‹å¹³å‡ï¼‰çš„æ€»è§ˆå›¾ï¼ˆå¯é€‰ï¼‰
    try:
        plot_loglik_vs_components(grid.cv_results_, best_params)
    except Exception as e:
        print(f"âš ï¸ æ€»è§ˆå›¾ç»˜åˆ¶å¤±è´¥: {e}")

    # è¾¹ç•Œæé†’
    if (best_params["gmm__n_components"] == max(comps)
        and best_params["gmm__covariance_type"] == "full"):
        print("\nğŸ’¡ æç¤ºï¼šæœ€ä½³æ¨¡å‹åœ¨ K ä¸Šè§¦åˆ°ä¸Šç•Œï¼Œåç»­å¯æ‰©å¤§ K æˆ–ç»§ç»­ç»†åŒ– reg_covar ç½‘æ ¼ã€‚")

    # ä¿å­˜Pipeline
    model_filename = f"gmm_model_{best_params['gmm__n_components']}c_fixed.pkl"
    try:
        joblib.dump(best_pipe, model_filename)
        print(f"\nâœ… å®Œæ•´Pipelineå·²ä¿å­˜åˆ°: {model_filename}")
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")

    return best_pipe


# ------------------------------
# ä¿®å¤ï¼šè¯„åˆ†APIï¼ˆæå‡ä¾¿æ·æ€§ï¼‰
# ------------------------------
def score_env(
    gmm_pipeline: Pipeline,
    df_query: pd.DataFrame,
    method: str = 'sigmoid',
    sigmoid_alpha: float = 1.0,
    reference_stats: Dict | None = None,
    return_logdens: bool = False,
):
    """
    ä¸€è‡´æ€§è¯„åˆ†å‡½æ•°ï¼ˆæ”¯æŒå›ºå®šæ ‡å®šï¼‰ï¼š
    - ä¼˜å…ˆä½¿ç”¨ gmm_pipeline.calibration_ï¼ˆè‹¥å­˜åœ¨ï¼‰ï¼Œå¦åˆ™ç”¨ reference_statsï¼Œ
      å†å¦åˆ™é€€å›å½“å‰æ‰¹æ¬¡è‡ªé€‚åº”ã€‚
    - æ”¯æŒ method âˆˆ {'sigmoid','minmax','zscore'}ã€‚
    - å¯è¿”å› logdens ä»¥ä¾¿åç»­åˆ†æã€‚

    reference_stats å¯åŒ…å«çš„é”®ï¼š
      - å¯¹ sigmoid/zscoreï¼š'mu' æˆ– 'mean'ï¼Œä»¥åŠå¯é€‰ 'std'
      - å¯¹ minmaxï¼š'min','max'
    """
    # 1) é¢„å¤„ç† + GMMæ‰“åˆ†ï¼ˆlogåŸŸæ›´ç¨³å®šï¼‰
    Xp = gmm_pipeline.named_steps['preprocessor'].transform(df_query)
    gmm: GaussianMixture = gmm_pipeline.named_steps['gmm']
    logdens = gmm.score_samples(Xp)
    dens = np.exp(logdens)

    # 2) é€‰æ‹©æ ‡å®šå‚æ•°æ¥æºï¼špipeline.calibration_ > reference_stats > å½“å‰æ‰¹æ¬¡
    calib = getattr(gmm_pipeline, "calibration_", None)
    ref = reference_stats or {}
    # ç»Ÿä¸€å–å€¼
    def pick(keys, default=None):
        for k in keys:
            if calib and k in calib:
                return calib[k]
            if k in ref:
                return ref[k]
        return default

    mu  = pick(['mu', 'mean'], float(logdens.mean()))
    std = pick(['std'], float(logdens.std()))
    vmin = pick(['min'], float(logdens.min()))
    vmax = pick(['max'], float(logdens.max()))

    # 3) è®¡ç®—åˆ†æ•°ï¼ˆå¸¦æ•°å€¼ä¿æŠ¤ï¼‰
    if method == 'sigmoid':
        # æ•°å€¼è£å‰ªï¼Œé¿å… exp æº¢å‡º
        x = np.clip(sigmoid_alpha * (logdens - mu), -50.0, 50.0)
        scores = 1.0 / (1.0 + np.exp(-x))
    elif method == 'minmax':
        rng = max(vmax - vmin, 1e-12)
        scores = (logdens - vmin) / rng
        # é˜²æ­¢è½»å¾®è¶Šç•Œ
        scores = np.clip(scores, 0.0, 1.0)
    elif method == 'zscore':
        s = std if std and std > 1e-12 else 1.0
        scores = (logdens - mu) / s
    else:
        raise ValueError(f"Unknown method: {method}")

    # 4) è¾“å‡ºç»Ÿè®¡ï¼ˆç”¨å½“å‰æ‰¹æ¬¡çš„ï¼Œç”¨äºæ—¥å¿—/å›å†™ï¼‰
    stats_out = {
        'mean': float(logdens.mean()),
        'mu': float(mu),
        'std': float(std),
        'min': float(logdens.min()),
        'max': float(logdens.max()),
    }

    if return_logdens:
        return dens, scores, stats_out, logdens
    else:
        return dens, scores, stats_out





def split_pos_for_calibration(df_pos: pd.DataFrame, calib_frac: float = 0.2, random_state: int = 42):
    """æŠŠæ­£æ ·æœ¬æ‹†æˆè®­ç»ƒ(1-calib_frac)ä¸æ ‡å®š(calib_frac)ã€‚è¿”å› df_train, df_calibã€‚"""
    idx = np.arange(len(df_pos))
    rs = np.random.RandomState(random_state)
    rs.shuffle(idx)
    cut = int(len(idx) * (1 - calib_frac))
    return df_pos.iloc[idx[:cut]].copy(), df_pos.iloc[idx[cut:]].copy()



def attach_env_calibration(gmm_pipeline: Pipeline, df_calib: pd.DataFrame, robust: bool = True):
    """
    åœ¨ç‹¬ç«‹çš„æ ‡å®šé›†ä¸Šä¼°è®¡ log-density çš„ç»Ÿè®¡é‡å¹¶æŒ‚åˆ° pipeline.calibration_ã€‚
    robust=True ç”¨ median/MADï¼Œé‡å°¾æ›´ç¨³ã€‚
    """
    Xp = gmm_pipeline.named_steps['preprocessor'].transform(df_calib)
    gmm: GaussianMixture = gmm_pipeline.named_steps['gmm']
    logp = gmm.score_samples(Xp)
    if robust:
        med = float(np.median(logp))
        mad = float(np.median(np.abs(logp - med)) + 1e-12)
        std = 1.4826 * mad  # æŠŠ MAD è½¬æˆè¿‘ä¼¼æ ‡å‡†å·®
        mu = med
    else:
        mu = float(np.mean(logp))
        std = float(np.std(logp) + 1e-12)
    gmm_pipeline.calibration_ = {
        "mu": mu, "std": std,
        "min": float(np.min(logp)),
        "max": float(np.max(logp))
    }
    return gmm_pipeline



def logdensity_reference_stats(gmm_pipeline: Pipeline, df_ref: pd.DataFrame, qs=(0.01, 0.05, 0.10)):
    Xp = gmm_pipeline.named_steps['preprocessor'].transform(df_ref)
    gmm: GaussianMixture = gmm_pipeline.named_steps['gmm']
    logp = gmm.score_samples(Xp)
    stats = {
        "mu": float(np.mean(logp)),
        "std": float(np.std(logp) + 1e-12),
        "quantiles": {f"Q{int(q*100)}": float(np.quantile(logp, q)) for q in qs}
    }
    return stats, logp

def assess_similarity_by_logdensity(gmm_pipeline: Pipeline,
                                    df_ref_pos: pd.DataFrame,
                                    df_query: pd.DataFrame,
                                    q_cut: float = 0.05):
    """
    è¿”å›ï¼šæ¯ä¸ªæŸ¥è¯¢æ ·æœ¬æ˜¯å¦â€œç›¸ä¼¼â€ï¼ˆlogp >= å‚è€ƒé›† q_cut åˆ†ä½é˜ˆå€¼ï¼‰ã€å…¶ z-score ä¸ logpã€‚
    """
    ref_stats, ref_logp = logdensity_reference_stats(gmm_pipeline, df_ref_pos, qs=(q_cut,))
    Xq = gmm_pipeline.named_steps['preprocessor'].transform(df_query)
    gmm: GaussianMixture = gmm_pipeline.named_steps['gmm']
    logp_q = gmm.score_samples(Xq)
    z = (logp_q - ref_stats["mu"]) / ref_stats["std"]
    thr = list(ref_stats["quantiles"].values())[0]
    is_similar = logp_q >= thr
    return {
        "threshold": thr,
        "ref_mu": ref_stats["mu"], "ref_std": ref_stats["std"],
        "similar_mask": is_similar,
        "logp_query": logp_q,
        "z_query": z
    }



def plot_loglik_vs_components(grid_results: Dict[str, Any], best_params: Dict[str, Any]) -> None:
    """ç»˜åˆ¶å¯¹æ•°ä¼¼ç„¶vsç»„ä»¶æ•°ï¼ˆé£æ ¼ä»¿ Figure5ï¼‰"""
    import matplotlib as mpl
    mpl.rcParams['font.family'] = 'Arial'
    mpl.rcParams['axes.titlesize'] = 16
    mpl.rcParams['axes.labelsize'] = 14
    mpl.rcParams['xtick.labelsize'] = 12
    mpl.rcParams['ytick.labelsize'] = 12
    mpl.rcParams['legend.fontsize'] = 12

    df = pd.DataFrame(grid_results)
    if "param_gmm__n_components" not in df.columns:
        warnings.warn("Grid results do not contain 'param_gmm__n_components'. Skipping plot.")
        return
    series = df.groupby("param_gmm__n_components")["mean_test_score"].mean()
    plt.figure(figsize=(10, 6))
    plt.plot(series.index, series.values, marker="o", color='#1F78B4', linewidth=2)
    plt.axvline(x=best_params["gmm__n_components"], linestyle="--", color='#E31A1C', linewidth=2, label=f"Best: {best_params['gmm__n_components']}")
    plt.xlabel("n_components")
    plt.ylabel("CV mean log-likelihood (higher is better)")
    plt.title("Log-likelihood vs n_components (CV)")
    plt.grid(True, alpha=0.3, linestyle='--')
    plt.legend(frameon=False)
    plt.tight_layout()
    plt.show()



def plot_cv_by_covariance_with_errorbars(cv_results: Dict[str, Any], best_params: Dict[str, Any]) -> None:
    """æŒ‰åæ–¹å·®ç±»å‹åˆ†ç»„ç»˜åˆ¶CVç»“æœï¼ˆé£æ ¼ä»¿ Figure5ï¼‰"""
    import matplotlib as mpl
    mpl.rcParams['font.family'] = 'Arial'
    mpl.rcParams['axes.titlesize'] = 16
    mpl.rcParams['axes.labelsize'] = 14
    mpl.rcParams['xtick.labelsize'] = 12
    mpl.rcParams['ytick.labelsize'] = 12
    mpl.rcParams['legend.fontsize'] = 12

    df = pd.DataFrame(cv_results)
    need_cols = [
        "param_gmm__n_components",
        "param_gmm__covariance_type",
        "param_gmm__n_init",
        "param_gmm__reg_covar",
        "mean_test_score",
        "std_test_score",
    ]
    for c in need_cols:
        if c not in df.columns:
            warnings.warn(f"cv_results_ ç¼ºå°‘åˆ—: {c}ï¼Œè·³è¿‡åˆ†ç±»å‹è¯¯å·®å›¾")
            return
    idx = df.groupby(["param_gmm__covariance_type", "param_gmm__n_components"])["mean_test_score"].idxmax()
    best_per_k_cov = df.loc[idx].sort_values(["param_gmm__covariance_type", "param_gmm__n_components"])
    fig, ax = plt.subplots(figsize=(9, 5))
    color_map = {
        'full': '#1F78B4',
        'tied': '#33A02C',
        'diag': '#E31A1C',
        'spherical': '#FDBF6F'
    }
    for cov, sub in best_per_k_cov.groupby("param_gmm__covariance_type"):
        color = color_map.get(cov, None)
        ax.errorbar(
            sub["param_gmm__n_components"],
            sub["mean_test_score"],
            yerr=sub["std_test_score"],
            marker="o",
            capsize=3,
            label=f"{cov}",
            color=color,
            linewidth=2
        )
    ax.axvline(best_params["gmm__n_components"], linestyle="--", color='#E31A1C', linewidth=2, label=f"Best K={best_params['gmm__n_components']}")
    ax.set_xlabel("n_components (K)")
    ax.set_ylabel("CV mean log-likelihood (â†‘)")
    ax.set_title("CV score by K (split by covariance_type) with error bars")
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.legend(frameon=False)
    plt.tight_layout()
    plt.show()

def _ecdf(values: np.ndarray):
    v = np.sort(values)
    y = np.linspace(0, 1, len(v), endpoint=False)
    return v, y

def _hist_bins_clip(x: np.ndarray, pct_lo=0.5, pct_hi=99.5, max_bins=60):
    lo, hi = np.percentile(x, [pct_lo, pct_hi])
    xs = np.clip(x, lo, hi)
    return xs, max_bins, (lo, hi)

def _get_component_covs(gmm: GaussianMixture):
    """è¿”å›æ¯ä¸ªç»„ä»¶çš„åæ–¹å·®çŸ©é˜µåˆ—è¡¨ï¼ˆfull/diag/tied ç»Ÿä¸€ä¸º full çŸ©é˜µï¼‰"""
    cov_type = gmm.covariance_type
    n_comp = gmm.n_components
    covs = []
    if cov_type == "full":
        covs = [gmm.covariances_[k] for k in range(n_comp)]
    elif cov_type == "diag":
        for k in range(n_comp):
            covs.append(np.diag(gmm.covariances_[k]))
    elif cov_type == "tied":
        covs = [gmm.covariances_ for _ in range(n_comp)]
    else:
        # å…œåº•ï¼šæŒ‰ diag å¤„ç†
        for k in range(n_comp):
            covs.append(np.diag(gmm.covariances_[k]))
    return covs

def _mahalanobis2_per_sample(Xp: np.ndarray, gmm: GaussianMixture):
    """å¯¹æ¯ä¸ªæ ·æœ¬ï¼šé€‰è´£ä»»åº¦æœ€å¤§çš„ç»„ä»¶ï¼Œè®¡ç®—è¯¥ç»„ä»¶ä¸‹çš„é©¬æ°è·ç¦»å¹³æ–¹ï¼ˆå…¨ç»´ï¼‰"""
    resp = gmm.predict_proba(Xp)           # (n, K)
    assign = resp.argmax(axis=1)           # (n,)
    means = gmm.means_                     # (K, d)
    covs = _get_component_covs(gmm)        # list of (d,d)

    md2 = np.empty(len(Xp), dtype=float)
    eps = 1e-9
    for i in range(len(Xp)):
        k = assign[i]
        delta = Xp[i] - means[k]
        C = covs[k] + np.eye(covs[k].shape[0]) * eps
        # ç”¨solveæ¯”æ˜¾å¼é€†æ›´ç¨³
        md2[i] = float(delta @ np.linalg.solve(C, delta))
    return md2, assign




def visualize_similarity_diagnostics(gmm_pipeline: Pipeline,
                                     df_ref_pos: pd.DataFrame,
                                     df_query: pd.DataFrame,
                                     q_cut: float = 0.05,
                                     max_points_pca: int = 8000,
                                     random_state: int = 42):
    """
    å¢å¼ºç‰ˆç›¸ä¼¼æ€§è¯Šæ–­å‡½æ•° - åŒ…å«è¯¦ç»†çš„ç»Ÿè®¡åˆ†æ
    æ–°å¢åŠŸèƒ½ï¼š
    1. 5ä¸ªç›¸ä¼¼æ€§å±‚çº§çš„è¯¦ç»†ç»Ÿè®¡
    2. PITå‡åŒ€æ€§æµ‹è¯•å’Œåˆ†å¸ƒåé‡åˆ†æ
    3. é©¬æ°è·ç¦»çš„å¤šä¸ªåˆ†ä½æ•°ç»Ÿè®¡
    4. ç»¼åˆç›¸ä¼¼æ€§è¯„åˆ†ç³»ç»Ÿ
    5. å®é™…åº”ç”¨æ„ä¹‰çš„è§£é‡Š
    6. PITåˆ†å¸ƒçš„ä¸Šä¸‹å››åˆ†ä½æ•°å’Œå‡å€¼ç¨³å¥æ€§ä¼°è®¡ï¼ˆattach_envï¼‰
    """
    from sklearn.decomposition import PCA

    pre = gmm_pipeline.named_steps['preprocessor']
    gmm: GaussianMixture = gmm_pipeline.named_steps['gmm']

    # ----- 1) è®¡ç®— log-density -----
    Xr = pre.transform(df_ref_pos)
    Xq = pre.transform(df_query)
    logp_ref = gmm.score_samples(Xr)
    logp_q = gmm.score_samples(Xq)

    # å‚è€ƒåˆ†å¸ƒçš„é˜ˆå€¼ï¼ˆé»˜è®¤5%ï¼‰
    thr = float(np.quantile(logp_ref, q_cut))
    mu, std = float(np.mean(logp_ref)), float(np.std(logp_ref) + 1e-12)
    z_q = (logp_q - mu) / std

    # ----- 2) ç”»å›¾ -----
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    ax1, ax2, ax3, ax4 = axes.ravel()

    # (a) log-density é‡å ç›´æ–¹å›¾
    ref_clip, bins_ref, _ = _hist_bins_clip(logp_ref)
    q_clip,   bins_q,   _ = _hist_bins_clip(logp_q)
    bins = max(bins_ref, bins_q)
    ax1.hist(ref_clip, bins=bins, alpha=0.6, label=f"Ref pos (n={len(logp_ref)})")
    ax1.hist(q_clip,   bins=bins, alpha=0.6, label=f"Query (n={len(logp_q)})")
    ax1.axvline(thr, color="red", linestyle="--", label=f"{int(q_cut*100)}% threshold")
    ax1.set_title("Log-density overlap (clipped to [0.5%, 99.5%])")
    ax1.set_xlabel("log p(x)  (transformed feature space)")
    ax1.set_ylabel("count")
    ax1.legend(frameon=False)

    # (b) PIT/åˆ†ä½ç›´æ–¹å›¾ï¼šq çš„ logp åœ¨ ref çš„ ECDF ä¸­çš„ç™¾åˆ†ä½
    v_ref, y_ref = _ecdf(logp_ref)
    ranks = np.searchsorted(v_ref, logp_q, side="left")
    pit = ranks / max(1, len(v_ref))  # âˆˆ[0,1)
    ax2.hist(pit, bins=20, range=(0, 1), alpha=0.85)
    ax2.set_title("PIT of query w.r.t ref (Uniformâ‰ˆwell-matched; Left-heavyâ‰ˆOOD)")
    ax2.set_xlabel("percentile")
    ax2.set_ylabel("count")

    # (c) PCA-2D + GMM æ¤­åœ†ï¼ˆ95% ç­‰æ¦‚ç‡è½®å»“ï¼‰
    rs = np.random.RandomState(random_state)
    idx_r = np.arange(len(Xr))
    idx_q = np.arange(len(Xq))
    if len(idx_r) > max_points_pca:
        idx_r = rs.choice(idx_r, size=max_points_pca, replace=False)
    if len(idx_q) > max_points_pca:
        idx_q = rs.choice(idx_q, size=max_points_pca, replace=False)
    Xr_s = Xr[idx_r]
    Xq_s = Xq[idx_q]

    pca = PCA(n_components=2, random_state=random_state)
    Zr = pca.fit_transform(Xr_s)
    Zq = pca.transform(Xq_s)

    ax3.scatter(Zr[:,0], Zr[:,1], s=6, alpha=0.25, label="Ref pos (PCA2)")
    ax3.scatter(Zq[:,0], Zq[:,1], s=8, alpha=0.5, label="Query (PCA2)")

    # ç”» GMM ç»„ä»¶åœ¨ PCA-2D ä¸‹çš„95%æ¤­åœ†
    comps = pca.components_[:2]
    means = gmm.means_
    covs = _get_component_covs(gmm)
    chi2_95 = 5.991
    for k in range(gmm.n_components):
        m_full = means[k][None, :]
        m_2d = pca.transform(m_full)[0]
        C = covs[k]
        C2 = comps @ C @ comps.T
        w, V = np.linalg.eigh(C2)
        w = np.maximum(w, 1e-12)
        width, height = 2*np.sqrt(chi2_95*w)
        angle = np.degrees(np.arctan2(V[1,0], V[0,0]))
        from matplotlib.patches import Ellipse
        ell = Ellipse(xy=m_2d, width=width, height=height, angle=angle,
                      edgecolor='k', facecolor='none', lw=1.5, alpha=0.8)
        ax3.add_patch(ell)

    ax3.set_title("PCA-2D with GMM 95% ellipses (for intuition only)")
    ax3.set_xlabel("PC1"); ax3.set_ylabel("PC2")
    ax3.legend(frameon=False)

    # (d) é©¬æ°è·ç¦»ï¼ˆå…¨ç»´ã€æŒ‰è´£ä»»åº¦æœ€è¿‘ç°‡ï¼‰
    md2_ref, _ = _mahalanobis2_per_sample(Xr, gmm)
    md2_q,   _ = _mahalanobis2_per_sample(Xq, gmm)
    thr_md2 = float(np.quantile(md2_ref, 0.95))
    xr_clip, _, _ = _hist_bins_clip(md2_ref)
    xq_clip, _, _ = _hist_bins_clip(md2_q)
    ax4.hist(xr_clip, bins=60, alpha=0.6, label="Ref pos")
    ax4.hist(xq_clip, bins=60, alpha=0.6, label="Query")
    ax4.axvline(thr_md2, color='red', linestyle='--', label="Ref 95% MDÂ²")
    ax4.set_title("Mahalanobis distanceÂ² by assigned component (full-dim)")
    ax4.set_xlabel("MDÂ²"); ax4.set_ylabel("count")
    ax4.legend(frameon=False)

    plt.tight_layout()
    plt.show()

    # =====================================================
    # è®¡ç®—è¯¦ç»†çš„ç›¸ä¼¼æ€§ç»Ÿè®¡æŒ‡æ ‡
    # =====================================================

    # 1. åŸºäº log-density çš„ç›¸ä¼¼æ€§åˆ†æ
    # 5%åˆ†ä½æ•°ä»¥ä¸‹ï¼ˆæåº¦ç›¸ä¼¼ï¼‰
    below_5pct = np.sum(logp_q >= thr)
    pct_below_5 = 100 * below_5pct / len(logp_q)

    # ä¸åŒç›¸ä¼¼æ€§å±‚æ¬¡çš„åˆ’åˆ†
    ref_quantiles = np.percentile(logp_ref, [5, 25, 50, 75, 95])
    q5, q25, q50, q75, q95 = ref_quantiles

    # æŸ¥è¯¢æ ·æœ¬åœ¨ä¸åŒå±‚æ¬¡çš„åˆ†å¸ƒ
    extremely_similar = np.sum(logp_q >= q95)  # å‰5%
    highly_similar = np.sum((logp_q >= q75) & (logp_q < q95))  # 75%-95%
    moderately_similar = np.sum((logp_q >= q25) & (logp_q < q75))  # 25%-75%
    poorly_similar = np.sum((logp_q >= q5) & (logp_q < q25))  # 5%-25%
    outliers = np.sum(logp_q < q5)  # å5%

    similarity_levels = {
        'extremely_similar': (extremely_similar, 100 * extremely_similar / len(logp_q)),
        'highly_similar': (highly_similar, 100 * highly_similar / len(logp_q)),
        'moderately_similar': (moderately_similar, 100 * moderately_similar / len(logp_q)),
        'poorly_similar': (poorly_similar, 100 * poorly_similar / len(logp_q)),
        'outliers': (outliers, 100 * outliers / len(logp_q))
    }

    # 2. PIT åˆ†æï¼ˆæ¦‚ç‡ç§¯åˆ†å˜æ¢ï¼‰
    pit_uniform_test = np.abs(np.histogram(pit, bins=10, range=(0,1))[0] - len(pit)/10).mean()
    pit_left_heavy = np.sum(pit < 0.2) / len(pit)  # å·¦åé‡ï¼ˆOODæŒ‡æ ‡ï¼‰
    pit_right_heavy = np.sum(pit > 0.8) / len(pit)  # å³åé‡

    # ==== PITåˆ†å¸ƒçš„ä¸Šä¸‹å››åˆ†ä½æ•°ã€å‡å€¼åŠå…¶ç¨³å¥æ€§ä¼°è®¡ ====
    def attach_env(arr):
        arr = np.asarray(arr)
        q25, q50, q75 = np.percentile(arr, [25, 50, 75])
        mean = np.mean(arr)
        std = np.std(arr, ddof=1)
        n = len(arr)
        sem = std / np.sqrt(n)
        # å››åˆ†ä½æ•°çš„æ ‡å‡†è¯¯ä¼°è®¡ï¼ˆIQR/1.349/sqrt(n) è¿‘ä¼¼ï¼Œé€‚ç”¨äºå¤§æ ·æœ¬ï¼‰
        iqr = q75 - q25
        q25_sem = iqr / 1.349 / np.sqrt(n)
        q75_sem = iqr / 1.349 / np.sqrt(n)
        q25_ci95 = (q25 - 1.96*q25_sem, q25 + 1.96*q25_sem)
        q75_ci95 = (q75 - 1.96*q75_sem, q75 + 1.96*q75_sem)
        median_sem = 1.253 * sem  # æ­£æ€è¿‘ä¼¼ä¸‹ä¸­ä½æ•°æ ‡å‡†è¯¯
        median_ci95 = (q50 - 1.96*median_sem, q50 + 1.96*median_sem)
        return {
            "q25": q25,
            "q25_sem": q25_sem,
            "q25_ci95": q25_ci95,
            "median": q50,
            "median_sem": median_sem,
            "median_ci95": median_ci95,
            "q75": q75,
            "q75_sem": q75_sem,
            "q75_ci95": q75_ci95,
            "mean": mean,
            "mean_sem": sem,
            "mean_ci95": (mean - 1.96*sem, mean + 1.96*sem)
        }
    pit_env = attach_env(pit)

    # 3. é©¬æ°è·ç¦»åˆ†æ
    md2_ref_quantiles = np.percentile(md2_ref, [50, 90, 95, 99])
    md2_q50, md2_q90, md2_q95, md2_q99 = md2_ref_quantiles

    md2_within_median = np.sum(md2_q <= md2_q50) / len(md2_q)
    md2_within_90pct = np.sum(md2_q <= md2_q90) / len(md2_q)
    md2_within_95pct = np.sum(md2_q <= md2_q95) / len(md2_q)
    md2_outliers = np.sum(md2_q > md2_q99) / len(md2_q)

    # 4. ç»¼åˆç›¸ä¼¼æ€§è¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰
    from scipy import stats
    logp_score = 100 * (1 - stats.percentileofscore(logp_ref, np.median(logp_q), kind='weak') / 100)
    pit_score = 100 * (1 - pit_uniform_test / (len(pit)/10))  # è¶Šæ¥è¿‘å‡åŒ€åˆ†å¸ƒè¶Šå¥½
    md2_score = 100 * md2_within_90pct  # 90%ä»¥å†…çš„æ¯”ä¾‹

    overall_similarity = (logp_score * 0.4 + pit_score * 0.3 + md2_score * 0.3)

    # =====================================================
    # æ‰“å°è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š
    # =====================================================
    print("\n" + "="*80)
    print("                    ç›¸ä¼¼æ€§è¯Šæ–­è¯¦ç»†æŠ¥å‘Š")
    print("="*80)

    print(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    print(f"  å‚è€ƒæ­£æ ·æœ¬: {len(logp_ref):,} ä¸ª")
    print(f"  æŸ¥è¯¢æ ·æœ¬: {len(logp_q):,} ä¸ª")

    print(f"\nğŸ¯ åŸºäºLog-Densityçš„ç›¸ä¼¼æ€§åˆ†å±‚:")
    for level, (count, pct) in similarity_levels.items():
        level_names = {
            'extremely_similar': 'æåº¦ç›¸ä¼¼ (>95%åˆ†ä½)',
            'highly_similar': 'é«˜åº¦ç›¸ä¼¼ (75%-95%)',
            'moderately_similar': 'ä¸­ç­‰ç›¸ä¼¼ (25%-75%)',
            'poorly_similar': 'ä½åº¦ç›¸ä¼¼ (5%-25%)',
            'outliers': 'å¼‚å¸¸å€¼ (<5%åˆ†ä½)'
        }
        emoji = "ğŸ”¥" if level == 'extremely_similar' else "âœ¨" if level == 'highly_similar' else "ğŸ“ˆ" if level == 'moderately_similar' else "âš ï¸" if level == 'poorly_similar' else "âŒ"
        print(f"  {emoji} {level_names[level]}: {count:,} ä¸ª ({pct:.1f}%)")

    print(f"\nğŸ² æ¦‚ç‡ç§¯åˆ†å˜æ¢(PIT)åˆ†æ:")
    print(f"  å‡åŒ€æ€§åç¦»åº¦: {pit_uniform_test:.3f} (è¶Šå°è¶Šå¥½, <{len(pit)/20:.1f}ä¸ºè‰¯å¥½)")
    print(f"  å·¦åé‡æ¯”ä¾‹: {pit_left_heavy:.1%} (é«˜å€¼è¡¨ç¤ºæŸ¥è¯¢æ ·æœ¬åç¦»å‚è€ƒåˆ†å¸ƒ)")
    print(f"  å³åé‡æ¯”ä¾‹: {pit_right_heavy:.1%}")
    print(f"  PITåˆ†å¸ƒä¸Šä¸‹å››åˆ†ä½æ•°: Q1={pit_env['q25']:.3f} Â± {pit_env['q25_sem']:.3f} (95%CI: {pit_env['q25_ci95'][0]:.3f} ~ {pit_env['q25_ci95'][1]:.3f})")
    print(f"                      Q2/ä¸­ä½æ•°={pit_env['median']:.3f} Â± {pit_env['median_sem']:.3f} (95%CI: {pit_env['median_ci95'][0]:.3f} ~ {pit_env['median_ci95'][1]:.3f})")
    print(f"                      Q3={pit_env['q75']:.3f} Â± {pit_env['q75_sem']:.3f} (95%CI: {pit_env['q75_ci95'][0]:.3f} ~ {pit_env['q75_ci95'][1]:.3f})")
    print(f"  PITå‡å€¼: {pit_env['mean']:.3f} Â± {pit_env['mean_sem']:.3f} (95%CI: {pit_env['mean_ci95'][0]:.3f} ~ {pit_env['mean_ci95'][1]:.3f})")

    print(f"\nğŸ“ é©¬æ°è·ç¦»ç»Ÿè®¡:")
    print(f"  ä¸­ä½æ•°ä»¥å†…: {md2_within_median:.1%}")
    print(f"  90%åˆ†ä½ä»¥å†…: {md2_within_90pct:.1%}")
    print(f"  95%åˆ†ä½ä»¥å†…: {md2_within_95pct:.1%}")
    print(f"  å¼‚å¸¸å€¼(>99%): {md2_outliers:.1%}")

    print(f"\nğŸ† ç»¼åˆç›¸ä¼¼æ€§è¯„åˆ†:")
    print(f"  Log-densityå¾—åˆ†: {logp_score:.1f}/100")
    print(f"  PITå‡åŒ€æ€§å¾—åˆ†: {pit_score:.1f}/100") 
    print(f"  é©¬æ°è·ç¦»å¾—åˆ†: {md2_score:.1f}/100")
    print(f"  â­ æ€»ä½“ç›¸ä¼¼æ€§: {overall_similarity:.1f}/100")

    # è§£é‡Šè¯„åˆ†æ„ä¹‰
    if overall_similarity >= 80:
        interpretation = "ğŸŸ¢ ä¼˜ç§€ - æŸ¥è¯¢æ ·æœ¬ä¸å‚è€ƒåˆ†å¸ƒé«˜åº¦ä¸€è‡´"
    elif overall_similarity >= 60:
        interpretation = "ğŸŸ¡ è‰¯å¥½ - æŸ¥è¯¢æ ·æœ¬ä¸å‚è€ƒåˆ†å¸ƒè¾ƒä¸ºä¸€è‡´"
    elif overall_similarity >= 40:
        interpretation = "ğŸŸ  ä¸€èˆ¬ - æŸ¥è¯¢æ ·æœ¬ä¸å‚è€ƒåˆ†å¸ƒå­˜åœ¨ä¸€å®šå·®å¼‚"
    else:
        interpretation = "ğŸ”´ è¾ƒå·® - æŸ¥è¯¢æ ·æœ¬æ˜æ˜¾åç¦»å‚è€ƒåˆ†å¸ƒ"

    print(f"  ğŸ“ è§£é‡Š: {interpretation}")

    print(f"\nğŸ’¡ å®é™…åº”ç”¨æ„ä¹‰:")
    if pct_below_5 >= 30:
        print(f"  âœ… {pct_below_5:.1f}%çš„æ’‚è’åœ°å…·æœ‰ä¸å·²å»ºå…‰ä¼é«˜åº¦ç›¸ä¼¼çš„ç¯å¢ƒç‰¹å¾")
    elif pct_below_5 >= 15:
        print(f"  âš ï¸ {pct_below_5:.1f}%çš„æ’‚è’åœ°å…·æœ‰ç›¸ä¼¼ç‰¹å¾ï¼Œå»ºè®®è¿›ä¸€æ­¥ç­›é€‰")
    else:
        print(f"  âŒ ä»…{pct_below_5:.1f}%çš„æ’‚è’åœ°ç›¸ä¼¼ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é€‰å€ç­–ç•¥")

    extreme_count = similarity_levels['extremely_similar'][0]
    high_count = similarity_levels['highly_similar'][0]
    print(f"  ğŸ¯ å»ºè®®ä¼˜å…ˆå¼€å‘: {extreme_count:,}ä¸ªæåº¦ç›¸ä¼¼ + {high_count:,}ä¸ªé«˜åº¦ç›¸ä¼¼åœ°ç‚¹")

    print("="*80)

    # è¿”å›è¯¦ç»†çš„ç»Ÿè®¡æ•°æ®
    return {
        # åŸæœ‰æ•°æ®
        "logp_ref_mean": mu, "logp_ref_std": std,
        "logp_threshold_q": q_cut, "logp_threshold_value": thr,
        "z_query": z_q,
        "pit_histogram": np.histogram(pit, bins=20, range=(0,1))[0],
        "md2_ref_q95": thr_md2,

        # æ–°å¢è¯¦ç»†ç»Ÿè®¡
        "similarity_levels": similarity_levels,
        "logp_quantiles": ref_quantiles,
        "pit_uniformity_deviation": pit_uniform_test,
        "pit_left_heavy_ratio": pit_left_heavy,
        "pit_right_heavy_ratio": pit_right_heavy,
        "pit_env": pit_env,
        "md2_quantiles": md2_ref_quantiles,
        "md2_within_ratios": {
            "median": md2_within_median,
            "90pct": md2_within_90pct, 
            "95pct": md2_within_95pct,
            "outliers": md2_outliers
        },
        "scores": {
            "logp_score": logp_score,
            "pit_score": pit_score,
            "md2_score": md2_score,
            "overall_similarity": overall_similarity,
            "interpretation": interpretation
        }
    }

