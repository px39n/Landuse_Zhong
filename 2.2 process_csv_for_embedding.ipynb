{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Sequence, Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from dask.distributed import Client\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Worker‐local cache\n",
    "_ds_abandon = None\n",
    "_ds_feat    = None\n",
    "\n",
    "def extract_single_point(\n",
    "    lat: float,\n",
    "    lon: float,\n",
    "    year: int,\n",
    "    p_area: float,\n",
    "    capacity_m: float,\n",
    "    unique_id: Any,\n",
    "    country: str,\n",
    "    abandon_pattern: str,\n",
    "    feature_pattern: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    在每个 worker 进程中首次打开并缓存两个 Dataset，然后针对单点抽特征。\n",
    "    \"\"\"\n",
    "    global _ds_abandon, _ds_feat\n",
    "\n",
    "    # 第一次调用时打开并 chunk\n",
    "    if _ds_abandon is None:\n",
    "        ds = xr.open_mfdataset(\n",
    "            abandon_pattern, combine='by_coords',\n",
    "            engine='netcdf4', parallel=False\n",
    "        )\n",
    "        # 先加载 metadata，再 chunk\n",
    "        tlen = ds.sizes['time']\n",
    "        _ds_abandon = ds.chunk({'time': tlen, 'lat': 500, 'lon': 500})\n",
    "\n",
    "    if _ds_feat is None:\n",
    "        ds = xr.open_mfdataset(\n",
    "            feature_pattern, combine='by_coords',\n",
    "            engine='netcdf4', parallel=False\n",
    "        )\n",
    "        tlen = ds.sizes['time']\n",
    "        _ds_feat = ds.chunk({'time': tlen, 'lat': 1000, 'lon': 1000})\n",
    "\n",
    "    # 最近邻抽取\n",
    "    env_pt     = _ds_feat.sel(time=str(year), method='nearest') \\\n",
    "                         .sel(lat=lat, lon=lon, method='nearest')\n",
    "    abandon_pt = _ds_abandon.sel(lat=lat, lon=lon, method='nearest')\n",
    "\n",
    "    feat = []\n",
    "    # 环境/社会经济特征\n",
    "    for var in _ds_feat.data_vars:\n",
    "        arr = env_pt[var].load().values\n",
    "        feat.append(float(arr) if arr.ndim == 0 else float(arr.flat[0]))\n",
    "\n",
    "    # 撂荒属性\n",
    "    for var in (\"current_abandonment\", \"abandonment_year\",\n",
    "                \"abandonment_duration\", \"recultivation\"):\n",
    "        if var in _ds_abandon.data_vars:\n",
    "            arr = abandon_pt[var].load().values\n",
    "            feat.append(float(arr))\n",
    "        else:\n",
    "            feat.append(np.nan)\n",
    "\n",
    "    # landcover 序列 embedding\n",
    "    if \"landcover\" in _ds_abandon.data_vars:\n",
    "        lc = abandon_pt[\"landcover\"].load().values  # shape (time,)\n",
    "        seq = np.nan_to_num(lc, 0).astype(int)\n",
    "        seq = np.clip(seq, 1, 9) - 1\n",
    "        onehot = np.eye(9)[seq]\n",
    "        feat.extend(onehot.mean(axis=0).tolist())\n",
    "\n",
    "    return {\n",
    "        'lat': lat, 'lon': lon, 'year': year,\n",
    "        'unique_id': unique_id,\n",
    "        'p_area': p_area, 'capacity_m': capacity_m,\n",
    "        'country': country,\n",
    "        **{f'f{i}': v for i, v in enumerate(feat)}\n",
    "    }\n",
    "\n",
    "\n",
    "def load_pv_sites(\n",
    "    csv_path: str,\n",
    "    years: Sequence[int] = (2018, 2020)\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    加载并标准化 PV 站点数据，过滤指定年份。\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"CSV 文件为空: {csv_path}\")\n",
    "\n",
    "    # 经纬度列映射\n",
    "    rename_map = {}\n",
    "    for src in ('latitude','lat_deg','LAT','Lat'):\n",
    "        if src in df.columns:\n",
    "            rename_map[src] = 'lat'\n",
    "    for src in ('longitude','lon_deg','LON','Lon'):\n",
    "        if src in df.columns:\n",
    "            rename_map[src] = 'lon'\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # 类型强制\n",
    "    df['lat']  = pd.to_numeric(df['lat'], errors='raise')\n",
    "    df['lon']  = pd.to_numeric(df['lon'], errors='raise')\n",
    "    df['year'] = pd.to_numeric(df['year'], downcast='integer', errors='raise')\n",
    "\n",
    "    required = {'lat','lon','year','unique_id','p_area','capacity_m','country'}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV 文件缺少必要列: {sorted(missing)}\")\n",
    "\n",
    "    df = df[df['year'].isin(years)]\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"没有符合年份 {years} 的记录\")\n",
    "\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据采样\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取通用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顶部已经定义好了：\n",
    "# _nc_lock = threading.Lock()\n",
    "\n",
    "def extract_all_features(\n",
    "    ds_feat: xr.Dataset,\n",
    "    ds_abandon: xr.Dataset,\n",
    "    pv_df: pd.DataFrame,\n",
    "    years: Sequence[int]\n",
    ") -> pd.DataFrame:\n",
    "    records = []\n",
    "\n",
    "    for yr in tqdm(years, desc=\"处理年份\"):\n",
    "        sub = pv_df[pv_df.year == yr].reset_index(drop=True)\n",
    "        lats = xr.DataArray(sub.lat.values, dims=\"point\")\n",
    "        lons = xr.DataArray(sub.lon.values, dims=\"point\")\n",
    "\n",
    "        ds_fy = ds_feat.sel(time=str(yr), method=\"nearest\")\n",
    "\n",
    "        # —— 环境特征\n",
    "        env_dict = {}\n",
    "        for var in tqdm(ds_feat.data_vars, desc=f\"提取环境变量 {yr}\"):\n",
    "            da_sel = ds_fy[var].sel(lat=lats, lon=lons, method=\"nearest\")\n",
    "\n",
    "            # ← 关键修改：在 compute 之前加全局锁\n",
    "            with _nc_lock:\n",
    "                arr = da_sel.compute(scheduler=\"single-threaded\")\n",
    "\n",
    "            env_dict[var] = arr  # numpy array\n",
    "\n",
    "        # —— 撂荒属性\n",
    "        aband_dict = {}\n",
    "        for var in (\"current_abandonment\",\"abandonment_year\",\n",
    "                    \"abandonment_duration\",\"recultivation\"):\n",
    "            if var in ds_abandon.data_vars:\n",
    "                da_sel = ds_abandon[var].sel(lat=lats, lon=lons, method=\"nearest\")\n",
    "                with _nc_lock:\n",
    "                    arr = da_sel.compute(scheduler=\"single-threaded\")\n",
    "                aband_dict[var] = arr\n",
    "            else:\n",
    "                aband_dict[var] = np.full(len(sub), np.nan)\n",
    "\n",
    "        # —— landcover 序列 embedding\n",
    "        if \"landcover\" in ds_abandon.data_vars:\n",
    "            lc_sel = ds_abandon.landcover.sel(lat=lats, lon=lons, method=\"nearest\")\n",
    "            with _nc_lock:\n",
    "                lc_arr = lc_sel.compute(scheduler=\"single-threaded\")\n",
    "            lc_vals = np.nan_to_num(lc_arr, nan=0).astype(int)\n",
    "            lc_vals = np.clip(lc_vals, 1, 9) - 1\n",
    "            onehot = np.eye(9)[lc_vals]\n",
    "            mean_onehot = onehot.mean(axis=0)\n",
    "            for i in range(9):\n",
    "                aband_dict[f\"lc_cls_{i}\"] = mean_onehot[:, i]\n",
    "\n",
    "        df_feat = pd.DataFrame({**env_dict, **aband_dict})\n",
    "        df_out  = pd.concat([sub, df_feat], axis=1)\n",
    "        records.append(df_out)\n",
    "\n",
    "    return pd.concat(records, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM负样本增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_candidate_negatives(ds_abandon: xr.Dataset,\n",
    "                            coords_pos: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    候选负样本：current_abandonment=1 且不在 coords_pos。\n",
    "    返回 DataFrame ['lat','lon']。\n",
    "    \"\"\"\n",
    "    mask = ds_abandon['current_abandonment'] == 1\n",
    "    idx = np.column_stack(np.where(mask.values))\n",
    "    cand = pd.DataFrame({\n",
    "        'lat': ds_abandon['lat'].values[idx[:,0]],\n",
    "        'lon': ds_abandon['lon'].values[idx[:,1]]\n",
    "    })\n",
    "    pos_set = set(zip(coords_pos['lat'].round(6), coords_pos['lon'].round(6)))\n",
    "    cand = cand[~cand.apply(lambda r: (round(r.lat,6), round(r.lon,6)) in pos_set, axis=1)]\n",
    "    return cand.reset_index(drop=True)\n",
    "\n",
    "def sample_negative(ds_abandon: xr.Dataset,\n",
    "                    ds_feature: xr.Dataset,\n",
    "                    coords_pos: pd.DataFrame,\n",
    "                    year: int = 2020,\n",
    "                    sample_size: int = 100_000,\n",
    "                    n_clusters: int = 10,\n",
    "                    quantile: float = 0.7):\n",
    "    \"\"\"\n",
    "    强负样本采样：\n",
    "      1) 从候选负样本中随机抽 sample_size，\n",
    "         用 extract_features(year) 提取特征并 GMM 聚类；\n",
    "      2) 选与“正样本集”最远的簇；\n",
    "      3) 对所有候选分批预测，筛出这些强负簇对应点；\n",
    "      4) 提取它们的特征并返回 X_neg, y_neg, coords_neg, scaler, gmm。\n",
    "    \"\"\"\n",
    "    cand = get_candidate_negatives(ds_abandon, coords_pos)\n",
    "    samp = cand.sample(min(sample_size, len(cand)), random_state=0).reset_index(drop=True)\n",
    "    feats = np.vstack([\n",
    "        extract_features(r.lat, r.lon, year, ds_abandon, ds_feature)\n",
    "        for _, r in samp.iterrows()\n",
    "    ])\n",
    "    scaler = StandardScaler().fit(feats)\n",
    "    fs = scaler.transform(feats)\n",
    "    gmm = GaussianMixture(n_components=n_clusters, random_state=0).fit(fs)\n",
    "\n",
    "    # 近似正样本 pos_fs\n",
    "    pos_fs = fs[:n_clusters]\n",
    "    dists = np.array([np.min(np.linalg.norm(pos_fs - c, axis=1)) for c in gmm.means_])\n",
    "    thr = np.quantile(dists, quantile)\n",
    "    strong_cls = np.where(dists >= thr)[0]\n",
    "\n",
    "    strong_list = []\n",
    "    batch = 10_000\n",
    "    for i in range(0, len(cand), batch):\n",
    "        block = cand.iloc[i:i+batch]\n",
    "        feats_blk = np.vstack([\n",
    "            extract_features(r.lat, r.lon, year, ds_abandon, ds_feature)\n",
    "            for _, r in block.iterrows()\n",
    "        ])\n",
    "        labels = gmm.predict(scaler.transform(feats_blk))\n",
    "        strong_list.append(block.iloc[np.isin(labels, strong_cls)])\n",
    "    strong_df = pd.concat(strong_list).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    X_neg = np.vstack([\n",
    "        extract_features(r.lat, r.lon, year, ds_abandon, ds_feature)\n",
    "        for _, r in strong_df.iterrows()\n",
    "    ])\n",
    "    y_neg = np.zeros(X_neg.shape[0], dtype=int)\n",
    "    coords_neg = strong_df[['lat','lon']].reset_index(drop=True)\n",
    "\n",
    "    return X_neg, y_neg, coords_neg, scaler, gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理... 2025-05-18 16:26:21\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unrecognized engine h5netcdf must be one of: ['netcdf4', 'scipy', 'rasterio', 'store']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 183\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m结束时间:\u001b[39m\u001b[38;5;124m\"\u001b[39m, datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 183\u001b[0m     main(test_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[1], line 152\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(test_mode, test_n)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始处理...\u001b[39m\u001b[38;5;124m\"\u001b[39m, datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# 1. 打开并 rechunk\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m ds_abandon, ds_feat \u001b[38;5;241m=\u001b[39m load_datasets(\n\u001b[0;32m    153\u001b[0m     PATHS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabandonment\u001b[39m\u001b[38;5;124m'\u001b[39m], PATHS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m )\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# 2. 读取 PV 站点并切片（测试模式）\u001b[39;00m\n\u001b[0;32m    157\u001b[0m pv_df \u001b[38;5;241m=\u001b[39m load_pv_sites(PATHS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m], years\u001b[38;5;241m=\u001b[39mYEARS)\n",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m, in \u001b[0;36mload_datasets\u001b[1;34m(abandon_pattern, feature_pattern)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m找不到文件\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 用 h5netcdf 引擎打开\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m ds_abandon \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_mfdataset(\n\u001b[0;32m     23\u001b[0m     files_abandon,\n\u001b[0;32m     24\u001b[0m     combine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby_coords\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     25\u001b[0m     engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh5netcdf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     26\u001b[0m     parallel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m          \u001b[38;5;66;03m# 还是用单线程模式\u001b[39;00m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m ds_feat \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_mfdataset(\n\u001b[0;32m     29\u001b[0m     files_feature,\n\u001b[0;32m     30\u001b[0m     combine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby_coords\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m     engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh5netcdf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     parallel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 一次性 rechunk（保持你原先的尺寸）\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\xarray\\backends\\api.py:1056\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[1;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     open_ \u001b[38;5;241m=\u001b[39m open_dataset\n\u001b[0;32m   1054\u001b[0m     getattr_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m\n\u001b[1;32m-> 1056\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [open_(p, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[0;32m   1057\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\xarray\\backends\\api.py:1056\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     open_ \u001b[38;5;241m=\u001b[39m open_dataset\n\u001b[0;32m   1054\u001b[0m     getattr_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m\n\u001b[1;32m-> 1056\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [open_(p, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[0;32m   1057\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\xarray\\backends\\api.py:559\u001b[0m, in \u001b[0;36mopen_dataset\u001b[1;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_array_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    557\u001b[0m     from_array_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 559\u001b[0m backend \u001b[38;5;241m=\u001b[39m plugins\u001b[38;5;241m.\u001b[39mget_backend(engine)\n\u001b[0;32m    561\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[0;32m    562\u001b[0m     decode_cf,\n\u001b[0;32m    563\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    569\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[0;32m    570\u001b[0m )\n\u001b[0;32m    572\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\xarray\\backends\\plugins.py:205\u001b[0m, in \u001b[0;36mget_backend\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m    203\u001b[0m     engines \u001b[38;5;241m=\u001b[39m list_engines()\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m engines:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munrecognized engine \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be one of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(engines)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m         )\n\u001b[0;32m    208\u001b[0m     backend \u001b[38;5;241m=\u001b[39m engines[engine]\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(engine, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(engine, BackendEntrypoint):\n",
      "\u001b[1;31mValueError\u001b[0m: unrecognized engine h5netcdf must be one of: ['netcdf4', 'scipy', 'rasterio', 'store']"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm.auto import tqdm\n",
    "from dask.diagnostics import ProgressBar\n",
    "import threading\n",
    "import glob\n",
    "\n",
    "def load_datasets(abandon_pattern: str, feature_pattern: str):\n",
    "    \"\"\"\n",
    "    打开 NetCDF，用 h5netcdf 替代 netcdf4，避免底层 HDF5 并发错误。\n",
    "    \"\"\"\n",
    "    files_abandon = glob.glob(abandon_pattern)\n",
    "    files_feature = glob.glob(feature_pattern)\n",
    "    if not files_abandon or not files_feature:\n",
    "        raise FileNotFoundError(\"找不到文件\")\n",
    "\n",
    "    # 用 h5netcdf 引擎打开\n",
    "    ds_abandon = xr.open_mfdataset(\n",
    "        files_abandon,\n",
    "        combine='by_coords',\n",
    "        engine='h5netcdf',\n",
    "        parallel=False          # 还是用单线程模式\n",
    "    )\n",
    "    ds_feat = xr.open_mfdataset(\n",
    "        files_feature,\n",
    "        combine='by_coords',\n",
    "        engine='h5netcdf',\n",
    "        parallel=False\n",
    "    )\n",
    "\n",
    "    # 一次性 rechunk（保持你原先的尺寸）\n",
    "    t_ab = ds_abandon.sizes['time']\n",
    "    ds_abandon = ds_abandon.chunk({'time': t_ab, 'lat': 500, 'lon': 500})\n",
    "\n",
    "    t_ft = ds_feat.sizes['time']\n",
    "    ds_feat = ds_feat.chunk({'time': t_ft, 'lat': 1000, 'lon': 1000})\n",
    "\n",
    "    return ds_abandon, ds_feat\n",
    "\n",
    "\n",
    "def load_pv_sites(\n",
    "    csv_path: str,\n",
    "    years: Sequence[int] = (2018, 2020)\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    加载并标准化 PV 站点数据，过滤指定年份。\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"CSV 文件为空: {csv_path}\")\n",
    "\n",
    "    # 经纬度列映射\n",
    "    rename_map = {}\n",
    "    for src in ('latitude', 'lat_deg', 'LAT', 'Lat'):\n",
    "        if src in df.columns:\n",
    "            rename_map[src] = 'lat'\n",
    "    for src in ('longitude', 'lon_deg', 'LON', 'Lon'):\n",
    "        if src in df.columns:\n",
    "            rename_map[src] = 'lon'\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # 强制类型转换\n",
    "    df['lat'] = pd.to_numeric(df['lat'], errors='raise')\n",
    "    df['lon'] = pd.to_numeric(df['lon'], errors='raise')\n",
    "    df['year'] = pd.to_numeric(df['year'], downcast='integer', errors='raise')\n",
    "\n",
    "    required = {'lat', 'lon', 'year', 'unique_id', 'p_area', 'capacity_m', 'country'}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV 文件缺少必要列: {sorted(missing)}\")\n",
    "\n",
    "    df = df[df['year'].isin(years)]\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"没有符合年份 {years} 的记录\")\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def extract_all_features(\n",
    "    ds_feat: xr.Dataset,\n",
    "    ds_abandon: xr.Dataset,\n",
    "    pv_df: pd.DataFrame,\n",
    "    years: Sequence[int]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    向量化地一次性对每个变量和年份 sel + load，\n",
    "    并用 _io_lock 串行化所有底层 NetCDF4 读操作。\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    for yr in years:\n",
    "        sub = pv_df[pv_df.year == yr].reset_index(drop=True)\n",
    "        lats = xr.DataArray(sub.lat.values, dims=\"point\")\n",
    "        lons = xr.DataArray(sub.lon.values, dims=\"point\")\n",
    "\n",
    "        ds_fy = ds_feat.sel(time=str(yr), method=\"nearest\")\n",
    "\n",
    "        # 环境变量\n",
    "        env_dict = {}\n",
    "        for var in tqdm(ds_feat.data_vars, desc=f\"提取环境变量 {yr}\"):\n",
    "            da_sel = ds_fy[var].sel(lat=lats, lon=lons, method=\"nearest\")\n",
    "            arr = da_sel.load(scheduler=\"single-threaded\").values\n",
    "            env_dict[var] = arr\n",
    "\n",
    "        # 撂荒属性\n",
    "        aband_dict = {}\n",
    "        for var in (\"current_abandonment\", \"abandonment_year\",\n",
    "                    \"abandonment_duration\", \"recultivation\"):\n",
    "            if var in ds_abandon.data_vars:\n",
    "                da_sel = ds_abandon[var].sel(lat=lats, lon=lons, method=\"nearest\")\n",
    "                arr = da_sel.load(scheduler=\"single-threaded\").values\n",
    "                aband_dict[var] = arr\n",
    "            else:\n",
    "                aband_dict[var] = np.full(len(sub), np.nan)\n",
    "\n",
    "        # landcover 序列 embedding\n",
    "        if \"landcover\" in ds_abandon.data_vars:\n",
    "            lc_sel = ds_abandon.landcover.sel(lat=lats, lon=lons, method=\"nearest\")\n",
    "\n",
    "            lc_arr = lc_sel.load(scheduler=\"single-threaded\").values\n",
    "            seq = np.nan_to_num(lc_arr, nan=0).astype(int)\n",
    "            seq = np.clip(seq, 1, 9) - 1\n",
    "            onehot = np.eye(9)[seq]\n",
    "            mean_onehot = onehot.mean(axis=0)\n",
    "            for i in range(9):\n",
    "                aband_dict[f\"lc_cls_{i}\"] = mean_onehot[:, i]\n",
    "\n",
    "        df_feat = pd.DataFrame({**env_dict, **aband_dict})\n",
    "        df_out  = pd.concat([sub, df_feat], axis=1)\n",
    "        records.append(df_out)\n",
    "\n",
    "    return pd.concat(records, ignore_index=True)\n",
    "\n",
    "\n",
    "def main(test_mode: bool = False, test_n: int = 500):\n",
    "    PATHS = {\n",
    "        'abandonment': \"D:/xarray/abandonment_chunkall/*.nc\",\n",
    "        'feature':     \"D:/xarray/aligned2/Feature_all/*.nc\",\n",
    "        'csv':         \"aligned_for_training.csv\",\n",
    "        'test_output': \"positive_samples_test_500.csv\",\n",
    "        'output':      \"positive_samples_full_with_features.csv\"\n",
    "    }\n",
    "    YEARS = [2018, 2020]\n",
    "\n",
    "    print(\"开始处理...\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    # 1. 打开并 rechunk\n",
    "    ds_abandon, ds_feat = load_datasets(\n",
    "        PATHS['abandonment'], PATHS['feature']\n",
    "    )\n",
    "\n",
    "    # 2. 读取 PV 站点并切片（测试模式）\n",
    "    pv_df = load_pv_sites(PATHS['csv'], years=YEARS)\n",
    "    if test_mode:\n",
    "        pv_df = pv_df.iloc[:test_n].reset_index(drop=True)\n",
    "        print(f\"⚠️ 测试模式：仅前 {test_n} 条记录\")\n",
    "\n",
    "    # 3. 向量化抽取\n",
    "    print(\"批量抽取特征 …\")\n",
    "    with ProgressBar():\n",
    "        df_all = extract_all_features(ds_feat, ds_abandon, pv_df, YEARS)\n",
    "\n",
    "    # 4. 去重 & 保存\n",
    "    df_unique = (\n",
    "        df_all\n",
    "        .sort_values(['year','lat','lon'])\n",
    "        .drop_duplicates(subset=['lat','lon'], keep='last')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    out_path = PATHS['test_output'] if test_mode else PATHS['output']\n",
    "    os.makedirs(os.path.dirname(out_path) or '.', exist_ok=True)\n",
    "    df_unique.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"完成，结果保存到: {out_path}\")\n",
    "    print(\"结束时间:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(test_mode=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gogogo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zpy10\\Anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\array\\core.py:4830: PerformanceWarning: Increasing number of chunks by factor of 31\n",
      "  result = blockwise(\n",
      "c:\\Users\\zpy10\\Anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\array\\core.py:4830: PerformanceWarning: Increasing number of chunks by factor of 31\n",
      "  result = blockwise(\n",
      "c:\\Users\\zpy10\\Anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\array\\core.py:4830: PerformanceWarning: Increasing number of chunks by factor of 31\n",
      "  result = blockwise(\n",
      "c:\\Users\\zpy10\\Anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\array\\core.py:4830: PerformanceWarning: Increasing number of chunks by factor of 31\n",
      "  result = blockwise(\n",
      "c:\\Users\\zpy10\\Anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\array\\core.py:4830: PerformanceWarning: Increasing number of chunks by factor of 124\n",
      "  result = blockwise(\n",
      "c:\\Users\\zpy10\\Anaconda3\\envs\\glbcropland\\Lib\\site-packages\\xarray\\core\\indexing.py:1598: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      "c:\\Users\\zpy10\\Anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\array\\core.py:4830: PerformanceWarning: Increasing number of chunks by factor of 93\n",
      "  result = blockwise(\n",
      "c:\\Users\\zpy10\\Anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\array\\core.py:4830: PerformanceWarning: Increasing number of chunks by factor of 93\n",
      "  result = blockwise(\n",
      "c:\\Users\\zpy10\\Anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\array\\core.py:4830: PerformanceWarning: Increasing number of chunks by factor of 93\n",
      "  result = blockwise(\n",
      "c:\\Users\\zpy10\\Anaconda3\\envs\\glbcropland\\Lib\\site-packages\\xarray\\core\\indexing.py:1598: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      "c:\\Users\\zpy10\\Anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\array\\core.py:4830: PerformanceWarning: Increasing number of chunks by factor of 14\n",
      "  result = blockwise(\n",
      "c:\\Users\\zpy10\\Anaconda3\\envs\\glbcropland\\Lib\\site-packages\\xarray\\core\\indexing.py:1598: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      "c:\\Users\\zpy10\\Anaconda3\\envs\\glbcropland\\Lib\\site-packages\\xarray\\core\\indexing.py:1598: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      "c:\\Users\\zpy10\\Anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\array\\core.py:4830: PerformanceWarning: Increasing number of chunks by factor of 93\n",
      "  result = blockwise(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from function import *\n",
    "\n",
    "\n",
    "abandon_2d_variable = [\n",
    "    \"current_abandonment\",\n",
    "    \"recultivation\", \n",
    "    \"abandonment_duration\",\n",
    "    \"abandonment_year\"\n",
    "]\n",
    "fea_3d_variable = [\n",
    "    'GDPpc',\n",
    "    'GDPtot',\n",
    "    'GURdist',\n",
    "    'Population',\n",
    "    'gdmp',\n",
    "    'rsds',\n",
    "    'tas',\n",
    "    'wind'\n",
    "]\n",
    "fea_2d_variable = [\n",
    "    'DEM',\n",
    "    'Powerdist',\n",
    "    'PrimaryRoad',\n",
    "    'SecondaryRoad',\n",
    "    'Slope',\n",
    "    'TertiaryRoad'\n",
    "]\n",
    "PATHS = {\n",
    "    'abandonment': \"D:/xarray/abandonment_chunkall/*.nc\",\n",
    "    'feature':     \"D:/xarray/aligned2/Feature_all/*.nc\",\n",
    "    'csv':         \"data/aligned_for_training0519.csv\",\n",
    "}\n",
    "\n",
    "YEARS = [2018, 2020]\n",
    "time=['2018-01-01','2020-01-01']\n",
    "# 2. 读取 PV 站点并切片（测试模式）\n",
    "# 2. 读取 PV 站点并切片（测试模式）\n",
    "pv_df = load_pv_sites(PATHS['csv'], years=YEARS)\n",
    "# Convert lon and lat columns to float32\n",
    "\n",
    "pv_df['lon'] = pv_df['lon'].astype('float32')\n",
    "pv_df['lat'] = pv_df['lat'].astype('float32')\n",
    "# Rename 'year' to 'time' and convert to datexxtime64\n",
    "pv_df = pv_df.rename(columns={'year': 'time'})\n",
    "pv_df['time'] = pd.to_datetime(pv_df['time'], format='%Y')\n",
    "\n",
    "\n",
    "# 1. 打开并 rechunk\n",
    "ds_abandon, ds_feat = load_datasets(\n",
    "    PATHS['abandonment'], PATHS['feature']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ds_merge=xr.merge([ds_abandon, ds_feat])\n",
    "# Convert coordinates to float32 while preserving other variables\n",
    "ds_merge = ds_merge.assign_coords({\n",
    "    'lon': ds_merge.lon.astype('float32'),\n",
    "    'lat': ds_merge.lat.astype('float32')\n",
    "})\n",
    "\n",
    "# For variables without time dimension, expand them to have same value for all times\n",
    "for var in ds_merge.data_vars:\n",
    "    if 'time' not in ds_merge[var].dims:\n",
    "        # Expand the variable to have time dimension with same values\n",
    "        ds_merge[var] = ds_merge[var].expand_dims(time=ds_merge.time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 ZZZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>p_area</th>\n",
       "      <th>capacity_m</th>\n",
       "      <th>country</th>\n",
       "      <th>time</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13592.361840</td>\n",
       "      <td>1.201704</td>\n",
       "      <td>GRC</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>21.612499</td>\n",
       "      <td>38.112499</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45763</td>\n",
       "      <td>3005.453465</td>\n",
       "      <td>0.281647</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>116.545830</td>\n",
       "      <td>33.420834</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45764</td>\n",
       "      <td>2134.049502</td>\n",
       "      <td>0.200225</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>116.520836</td>\n",
       "      <td>32.987499</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45765</td>\n",
       "      <td>3429.546491</td>\n",
       "      <td>0.321102</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>116.504166</td>\n",
       "      <td>34.495834</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45766</td>\n",
       "      <td>2231.352132</td>\n",
       "      <td>0.209629</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>116.562500</td>\n",
       "      <td>32.287498</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90825</th>\n",
       "      <td>11758</td>\n",
       "      <td>40069.483340</td>\n",
       "      <td>2.684475</td>\n",
       "      <td>USA</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>-78.079170</td>\n",
       "      <td>35.312500</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90826</th>\n",
       "      <td>11759</td>\n",
       "      <td>56.397704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>USA</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>-79.037498</td>\n",
       "      <td>36.004166</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90827</th>\n",
       "      <td>11760</td>\n",
       "      <td>22.106298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>USA</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>-79.054169</td>\n",
       "      <td>35.954166</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90828</th>\n",
       "      <td>11754</td>\n",
       "      <td>26182.946620</td>\n",
       "      <td>1.875714</td>\n",
       "      <td>USA</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>-78.262497</td>\n",
       "      <td>36.420834</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90829</th>\n",
       "      <td>35272</td>\n",
       "      <td>2494.764356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>RUS</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>30.945833</td>\n",
       "      <td>61.387501</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90830 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id        p_area  capacity_m country       time         lon  \\\n",
       "0              1  13592.361840    1.201704     GRC 2018-01-01   21.612499   \n",
       "1          45763   3005.453465    0.281647     CHN 2018-01-01  116.545830   \n",
       "2          45764   2134.049502    0.200225     CHN 2018-01-01  116.520836   \n",
       "3          45765   3429.546491    0.321102     CHN 2018-01-01  116.504166   \n",
       "4          45766   2231.352132    0.209629     CHN 2018-01-01  116.562500   \n",
       "...          ...           ...         ...     ...        ...         ...   \n",
       "90825      11758  40069.483340    2.684475     USA 2020-01-01  -78.079170   \n",
       "90826      11759     56.397704    0.000000     USA 2020-01-01  -79.037498   \n",
       "90827      11760     22.106298    0.000000     USA 2020-01-01  -79.054169   \n",
       "90828      11754  26182.946620    1.875714     USA 2020-01-01  -78.262497   \n",
       "90829      35272   2494.764356    0.000000     RUS 2020-01-01   30.945833   \n",
       "\n",
       "             lat  year  \n",
       "0      38.112499  2018  \n",
       "1      33.420834  2018  \n",
       "2      32.987499  2018  \n",
       "3      34.495834  2018  \n",
       "4      32.287498  2018  \n",
       "...          ...   ...  \n",
       "90825  35.312500  2020  \n",
       "90826  36.004166  2020  \n",
       "90827  35.954166  2020  \n",
       "90828  36.420834  2020  \n",
       "90829  61.387501  2020  \n",
       "\n",
       "[90830 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pv_df\n",
    "pv_df['year'] = pd.to_datetime(pv_df['time']).dt.year\n",
    "pv_df_dedup = (\n",
    "    pv_df.sort_values('time')  # 可改为其他排序依据\n",
    "         .drop_duplicates(subset=['year', 'lat', 'lon'], keep='last')  # 你也可以用 keep='first'\n",
    "         .reset_index(drop=True)\n",
    ")\n",
    "pv_df_dedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 SDFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 1 of 19: abandonment_year\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8fd6f6b37b448afa175524cd644dba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 2 of 19: abandonment_duration\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91918885f214fbeaac45895d186a535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 3 of 19: recultivation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379bd73cce8d4ac98efcc2ec48f44738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 4 of 19: current_abandonment\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3d695bb6554f11a4cc7a05e8876054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 5 of 19: landcover\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb8cd4c28504bab9253b65848ab805f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 6 of 19: DEM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa03754f9d645d397910f597b156a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 7 of 19: GDPpc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed729dbe2af344f4b3dbe826f0d56ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 8 of 19: GDPtot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d3e5eed1114bfd837234afa4673bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 9 of 19: GURdist\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bfb1dc793d450cb3d6eb2419ad43d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 10 of 19: Population\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e33b306cf5c4a6daf90fe2c2fed4873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 11 of 19: Powerdist\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f19ca25103b48858f7cb451cec5ebea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 12 of 19: PrimaryRoad\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2650d6acdf7a4afbb7422fb6c57b53c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 13 of 19: SecondaryRoad\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abee160151c8405d8ee6d5003843d4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 14 of 19: Slope\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e29580c9c4492093cf4f8d6dab0b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 15 of 19: TertiaryRoad\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fb8c2220434648b4045f6cd6285241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 16 of 19: gdmp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b78c07f5c94a68b92272fc280c8562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 17 of 19: rsds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a917a85cca4741b43295424eb57aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 18 of 19: tas\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fa7bd0fb9743daac796c7c8c342855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable 19 of 19: wind\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8782c920feef43ac81ccc9d32cdec18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据块:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ee24e3959a48daab5f81f9cd2a4749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "合并数据:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def process_chunk(df, ds_merge, step,stop=0):\n",
    "\n",
    "    # Get coordinates that exist in both df_temp and pv_df\n",
    "    common_lats = np.intersect1d(ds_merge.lat.values, df['lat'].unique())\n",
    "    common_lons = np.intersect1d(ds_merge.lon.values, df['lon'].unique())\n",
    "\n",
    "    # Select data from df_temp using only the common coordinates\n",
    "    ds_merge = ds_merge.sel(\n",
    "        lat=common_lats,\n",
    "        lon=common_lons,\n",
    "    )\n",
    "\n",
    "    #step = 500\n",
    "    total_lat = len(ds_merge.lat)\n",
    "    total_lon = len(ds_merge.lon)\n",
    "    merged_dfs = []\n",
    "    \n",
    "    # Calculate total iterations for progress bar\n",
    "    total_iterations = (total_lat // step + (1 if total_lat % step else 0)) * \\\n",
    "                    (total_lon // step + (1 if total_lon % step else 0))\n",
    "\n",
    "    # Create progress bar with Chinese description\n",
    "    pbar = tqdm(total=total_iterations, desc=\"处理数据块\")\n",
    "\n",
    "    # Iterate through all latitude and longitude points in chunks\n",
    "    for start_lat in range(0, total_lat, step):\n",
    "        end_lat = min(start_lat + step, total_lat)\n",
    "        for start_lon in range(0, total_lon, step):\n",
    "            end_lon = min(start_lon + step, total_lon)\n",
    "            #print(1)\n",
    "            # Extract data from ds_merge for each time point\n",
    "            df = ds_merge.isel(\n",
    "                lat=slice(start_lat, end_lat),\n",
    "                lon=slice(start_lon, end_lon)\n",
    "            ).compute().to_dataframe()\n",
    "            \n",
    "            # Reset index to convert multi-index to columns\n",
    "            df = df.reset_index()\n",
    "            \n",
    "            # Merge with pv_df based on lat/lon coordinates\n",
    "            chunk_merged = pd.merge(df, pv_df_dedup, on=['lat','lon','time'], how='inner')\n",
    "            merged_dfs.append(chunk_merged)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "            if stop==1:\n",
    "                break\n",
    "    # Close progress bar\n",
    "    pbar.close()\n",
    "    # Combine all chunks into final dataframe\n",
    "    merged_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Iterate through each variable\n",
    "# Get all variables from ds_merge\n",
    "all_vars = list(ds_merge.data_vars)\n",
    "merged_dfs = []\n",
    "# Iterate through each variable\n",
    "for i, var in enumerate(all_vars):\n",
    "    print(f\"Processing variable {i+1} of {len(all_vars)}: {var}\")\n",
    "    # Check if variable has time dimension\n",
    "    df_temp = ds_merge[var].sel(time=['2018-01-01','2020-01-01'])\n",
    "\n",
    "    # Process the chunk and merge with pv_df\n",
    "    merged_df = process_chunk(pv_df_dedup, df_temp, step=3000)\n",
    "\n",
    "    merged_dfs.append(merged_df)\n",
    "\n",
    "# Concatenate all merged dataframes with single-column retention\n",
    "\n",
    "final_merged_df = merged_dfs[0]\n",
    "for df in tqdm(merged_dfs[1:], desc=\"合并数据\"):\n",
    "\n",
    "    final_merged_df = pd.merge(\n",
    "        final_merged_df,\n",
    "        df,\n",
    "        on=[\"time\", \"lon\", \"lat\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"\", \"_drop\")\n",
    "    )\n",
    "    # Drop duplicated columns with \"_drop\" suffix\n",
    "    final_merged_df = final_merged_df.loc[:, ~final_merged_df.columns.str.endswith(\"_drop\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging variable 2 of 6\n",
      "Merging variable 3 of 6\n",
      "Merging variable 4 of 6\n",
      "Merging variable 5 of 6\n",
      "Merging variable 6 of 6\n",
      "✅ Final Dask-merged CSVs saved.\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# 将每个 pandas.DataFrame 转为 Dask DataFrame（默认划分成 4 个分区）\n",
    "ddfs = [dd.from_pandas(df, npartitions=4) for df in merged_dfs]\n",
    "\n",
    "# 初始化为第一个\n",
    "merged_ddf = ddfs[0]\n",
    "\n",
    "# 逐个合并后续 DataFrames\n",
    "for i, ddf in enumerate(ddfs[1:], start=1):\n",
    "    print(f\"Merging variable {i+1} of {len(ddfs)}\")\n",
    "    merged_ddf = merged_ddf.merge(ddf, on=[\"time\", \"lon\", \"lat\"], how=\"inner\", suffixes=(\"\", \"_drop\"))\n",
    "    # 删除带 \"_drop\" 后缀的重复列\n",
    "    merged_ddf = merged_ddf.loc[:, [col for col in merged_ddf.columns if not col.endswith(\"_drop\")]]\n",
    "\n",
    "\n",
    "output_path = \"D:/xarray/aligned2/temp/final_merged_*.csv\"\n",
    "merged_ddf.to_csv(output_path, index=False, single_file=False)\n",
    "print(\"✅ Final Dask-merged CSVs saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'duckdb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m         out_df\u001b[38;5;241m.\u001b[39mto_parquet(out_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Saved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mduckdb\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin_all_parquet_to_csv\u001b[39m(parquet_dir, output_csv):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'duckdb'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def write_each_variable_parquet(merged_dfs, save_dir, base_cols=[\"time\", \"lat\", \"lon\"]):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for i, df in enumerate(merged_dfs):\n",
    "        var_cols = [c for c in df.columns if c not in base_cols and c not in ['band', 'spatial_ref']]\n",
    "        if len(var_cols) != 1:\n",
    "            print(f\"❌ Warning: variable {i} has unexpected columns: {var_cols}\")\n",
    "            continue\n",
    "        var_name = var_cols[0]\n",
    "        out_df = df[base_cols + [var_name]]\n",
    "        out_path = os.path.join(save_dir, f\"{var_name}.parquet\")\n",
    "        out_df.to_parquet(out_path, index=False)\n",
    "        print(f\"✅ Saved {var_name}.parquet\")\n",
    "\n",
    "\n",
    "import duckdb\n",
    "import glob\n",
    "\n",
    "def join_all_parquet_to_csv(parquet_dir, output_csv):\n",
    "    files = sorted(glob.glob(f\"{parquet_dir}/*.parquet\"))\n",
    "    \n",
    "    # 构造 DuckDB SQL JOIN 查询\n",
    "    base = f\"read_parquet('{files[0]}')\"\n",
    "    for f in files[1:]:\n",
    "        base = f\"({base}) JOIN read_parquet('{f}') USING (time, lat, lon)\"\n",
    "    \n",
    "    # 执行 SQL，并写出 CSV（或 Parquet）\n",
    "    con = duckdb.connect()\n",
    "    con.execute(f\"COPY ({base}) TO '{output_csv}' (HEADER, DELIMITER ',')\")\n",
    "    con.close()\n",
    "    print(f\"🎯 Final merged training table saved at: {output_csv}\")\n",
    "\n",
    "\n",
    "# 第一步：保存每个变量\n",
    "write_each_variable_parquet(merged_dfs, save_dir=\"D:/xarray/aligned2/vars\")\n",
    "\n",
    "# 第二步：用 DuckDB 整合为训练表\n",
    "join_all_parquet_to_csv(\n",
    "    parquet_dir=\"D:/xarray/aligned2/vars\",\n",
    "    output_csv=\"D:/xarray/aligned2/final_training.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>spatial_ref</th>\n",
       "      <th>band</th>\n",
       "      <th>abandonment_duration</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>p_area</th>\n",
       "      <th>capacity_m</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>-13.837500</td>\n",
       "      <td>-171.804169</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26510</td>\n",
       "      <td>87614.296875</td>\n",
       "      <td>7.850205</td>\n",
       "      <td>WSM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>-13.829166</td>\n",
       "      <td>-171.995834</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26500</td>\n",
       "      <td>10696.658203</td>\n",
       "      <td>0.956408</td>\n",
       "      <td>WSM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>-13.829166</td>\n",
       "      <td>-171.987503</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26501</td>\n",
       "      <td>66982.132812</td>\n",
       "      <td>5.989546</td>\n",
       "      <td>WSM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>-29.245832</td>\n",
       "      <td>-177.929169</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19077</td>\n",
       "      <td>385.150726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NZL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>-22.645834</td>\n",
       "      <td>-152.795837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34745</td>\n",
       "      <td>615.614868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>PYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103912</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>55.712502</td>\n",
       "      <td>37.462502</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32467</td>\n",
       "      <td>75.500473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>RUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103913</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>55.737499</td>\n",
       "      <td>37.579166</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9832</td>\n",
       "      <td>281.253357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>RUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103914</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>55.820835</td>\n",
       "      <td>37.704166</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32288</td>\n",
       "      <td>44.549313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>RUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103915</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>55.979168</td>\n",
       "      <td>37.262501</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17828</td>\n",
       "      <td>538.808228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>RUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103916</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>67.662498</td>\n",
       "      <td>134.654160</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19298</td>\n",
       "      <td>41369.183594</td>\n",
       "      <td>2.757651</td>\n",
       "      <td>RUS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103917 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             time        lat         lon  spatial_ref  band  \\\n",
       "0      2018-01-01 -13.837500 -171.804169            0     1   \n",
       "1      2018-01-01 -13.829166 -171.995834            0     1   \n",
       "2      2018-01-01 -13.829166 -171.987503            0     1   \n",
       "3      2020-01-01 -29.245832 -177.929169            0     1   \n",
       "4      2020-01-01 -22.645834 -152.795837            0     1   \n",
       "...           ...        ...         ...          ...   ...   \n",
       "103912 2020-01-01  55.712502   37.462502            0     1   \n",
       "103913 2020-01-01  55.737499   37.579166            0     1   \n",
       "103914 2020-01-01  55.820835   37.704166            0     1   \n",
       "103915 2020-01-01  55.979168   37.262501            0     1   \n",
       "103916 2020-01-01  67.662498  134.654160            0     1   \n",
       "\n",
       "        abandonment_duration  unique_id        p_area  capacity_m country  \n",
       "0                        NaN      26510  87614.296875    7.850205     WSM  \n",
       "1                        NaN      26500  10696.658203    0.956408     WSM  \n",
       "2                        NaN      26501  66982.132812    5.989546     WSM  \n",
       "3                        NaN      19077    385.150726    0.000000     NZL  \n",
       "4                        NaN      34745    615.614868    0.000000     PYF  \n",
       "...                      ...        ...           ...         ...     ...  \n",
       "103912                   NaN      32467     75.500473    0.000000     RUS  \n",
       "103913                   NaN       9832    281.253357    0.000000     RUS  \n",
       "103914                   NaN      32288     44.549313    0.000000     RUS  \n",
       "103915                   NaN      17828    538.808228    0.000000     RUS  \n",
       "103916                   NaN      19298  41369.183594    2.757651     RUS  \n",
       "\n",
       "[103917 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir=r\"D:\\xarray\\aligned2\\temp_dfs\"\n",
    "all_files = sorted([f for f in os.listdir(save_dir) if f.endswith(\".parquet\") and f.startswith(\"df_\")])\n",
    "unmerge_df=[]\n",
    "\n",
    "\n",
    "\n",
    "for i, file in enumerate(all_files):\n",
    "    df_path = os.path.join(save_dir, file)\n",
    "    df=pd.read_parquet(df_path)\n",
    "    unmerge_df.append(df)\n",
    "\n",
    "    if i==5:\n",
    "        break\n",
    "\n",
    "\n",
    "len(unmerge_df)\n",
    "unmerge_df[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>spatial_ref</th>\n",
       "      <th>band</th>\n",
       "      <th>abandonment_year</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>p_area</th>\n",
       "      <th>capacity_m</th>\n",
       "      <th>country</th>\n",
       "      <th>...</th>\n",
       "      <th>Population</th>\n",
       "      <th>Powerdist</th>\n",
       "      <th>PrimaryRoad</th>\n",
       "      <th>SecondaryRoad</th>\n",
       "      <th>Slope</th>\n",
       "      <th>TertiaryRoad</th>\n",
       "      <th>gdmp</th>\n",
       "      <th>rsds</th>\n",
       "      <th>tas</th>\n",
       "      <th>wind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>-13.837500</td>\n",
       "      <td>-171.804169</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26510</td>\n",
       "      <td>87614.298290</td>\n",
       "      <td>7.850205</td>\n",
       "      <td>WSM</td>\n",
       "      <td>...</td>\n",
       "      <td>646.119934</td>\n",
       "      <td>622589.0000</td>\n",
       "      <td>12.719680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.521324</td>\n",
       "      <td>197.039190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.063463</td>\n",
       "      <td>2995.384528</td>\n",
       "      <td>2.293710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>-13.829166</td>\n",
       "      <td>-171.995834</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26500</td>\n",
       "      <td>10696.657940</td>\n",
       "      <td>0.956408</td>\n",
       "      <td>WSM</td>\n",
       "      <td>...</td>\n",
       "      <td>138.296951</td>\n",
       "      <td>622588.9375</td>\n",
       "      <td>14.276298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715136</td>\n",
       "      <td>100.218651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.820724</td>\n",
       "      <td>2999.923652</td>\n",
       "      <td>2.770978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>-13.829166</td>\n",
       "      <td>-171.987503</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26501</td>\n",
       "      <td>66982.132520</td>\n",
       "      <td>5.989546</td>\n",
       "      <td>WSM</td>\n",
       "      <td>...</td>\n",
       "      <td>138.296951</td>\n",
       "      <td>622588.9375</td>\n",
       "      <td>14.276298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.922556</td>\n",
       "      <td>100.218651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.825073</td>\n",
       "      <td>2999.838979</td>\n",
       "      <td>2.721136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>-4.387500</td>\n",
       "      <td>-79.812500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10893</td>\n",
       "      <td>16680.699130</td>\n",
       "      <td>1.511097</td>\n",
       "      <td>ECU</td>\n",
       "      <td>...</td>\n",
       "      <td>15.645079</td>\n",
       "      <td>622609.3750</td>\n",
       "      <td>18.676649</td>\n",
       "      <td>24.508936</td>\n",
       "      <td>6.674829</td>\n",
       "      <td>43.411486</td>\n",
       "      <td>167.423999</td>\n",
       "      <td>18.745583</td>\n",
       "      <td>2969.209102</td>\n",
       "      <td>0.836408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>-4.254167</td>\n",
       "      <td>-79.470833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10880</td>\n",
       "      <td>34065.381000</td>\n",
       "      <td>3.038932</td>\n",
       "      <td>ECU</td>\n",
       "      <td>...</td>\n",
       "      <td>37.699821</td>\n",
       "      <td>622609.3750</td>\n",
       "      <td>6.512673</td>\n",
       "      <td>18.597032</td>\n",
       "      <td>7.650049</td>\n",
       "      <td>89.491292</td>\n",
       "      <td>128.872533</td>\n",
       "      <td>17.977419</td>\n",
       "      <td>2920.696152</td>\n",
       "      <td>3.776348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90816</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>55.712502</td>\n",
       "      <td>37.462502</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32467</td>\n",
       "      <td>75.500476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>RUS</td>\n",
       "      <td>...</td>\n",
       "      <td>14468.128906</td>\n",
       "      <td>622589.8125</td>\n",
       "      <td>62.451511</td>\n",
       "      <td>112.277082</td>\n",
       "      <td>1.968263</td>\n",
       "      <td>861.205086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.967303</td>\n",
       "      <td>2804.318214</td>\n",
       "      <td>2.684799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90817</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>55.737499</td>\n",
       "      <td>37.579166</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9832</td>\n",
       "      <td>281.253369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>RUS</td>\n",
       "      <td>...</td>\n",
       "      <td>12943.362305</td>\n",
       "      <td>622589.8125</td>\n",
       "      <td>118.298899</td>\n",
       "      <td>140.942360</td>\n",
       "      <td>0.393874</td>\n",
       "      <td>1132.096021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.992375</td>\n",
       "      <td>2804.168017</td>\n",
       "      <td>2.531498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90818</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>55.820835</td>\n",
       "      <td>37.704166</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32288</td>\n",
       "      <td>44.549314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>RUS</td>\n",
       "      <td>...</td>\n",
       "      <td>16157.847656</td>\n",
       "      <td>622589.8125</td>\n",
       "      <td>38.558650</td>\n",
       "      <td>73.424121</td>\n",
       "      <td>0.412605</td>\n",
       "      <td>642.344304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.950527</td>\n",
       "      <td>2803.408293</td>\n",
       "      <td>2.519807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90819</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>55.979168</td>\n",
       "      <td>37.262501</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17828</td>\n",
       "      <td>538.808249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>RUS</td>\n",
       "      <td>...</td>\n",
       "      <td>4346.128418</td>\n",
       "      <td>622589.5000</td>\n",
       "      <td>63.311948</td>\n",
       "      <td>5.627337</td>\n",
       "      <td>0.978244</td>\n",
       "      <td>316.949814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.815846</td>\n",
       "      <td>2800.570781</td>\n",
       "      <td>2.697752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90820</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>67.662498</td>\n",
       "      <td>134.654160</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19298</td>\n",
       "      <td>41369.184550</td>\n",
       "      <td>2.757651</td>\n",
       "      <td>RUS</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095793</td>\n",
       "      <td>622609.5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.918713</td>\n",
       "      <td>25.727123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.621017</td>\n",
       "      <td>2608.666671</td>\n",
       "      <td>2.189081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90821 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            time        lat         lon  spatial_ref  band  abandonment_year  \\\n",
       "0     2018-01-01 -13.837500 -171.804169            0     1               NaN   \n",
       "1     2018-01-01 -13.829166 -171.995834            0     1               NaN   \n",
       "2     2018-01-01 -13.829166 -171.987503            0     1               NaN   \n",
       "3     2018-01-01  -4.387500  -79.812500            0     1               NaN   \n",
       "4     2018-01-01  -4.254167  -79.470833            0     1               NaN   \n",
       "...          ...        ...         ...          ...   ...               ...   \n",
       "90816 2020-01-01  55.712502   37.462502            0     1               NaN   \n",
       "90817 2020-01-01  55.737499   37.579166            0     1               NaN   \n",
       "90818 2020-01-01  55.820835   37.704166            0     1               NaN   \n",
       "90819 2020-01-01  55.979168   37.262501            0     1               NaN   \n",
       "90820 2020-01-01  67.662498  134.654160            0     1               NaN   \n",
       "\n",
       "       unique_id        p_area  capacity_m country  ...    Population  \\\n",
       "0          26510  87614.298290    7.850205     WSM  ...    646.119934   \n",
       "1          26500  10696.657940    0.956408     WSM  ...    138.296951   \n",
       "2          26501  66982.132520    5.989546     WSM  ...    138.296951   \n",
       "3          10893  16680.699130    1.511097     ECU  ...     15.645079   \n",
       "4          10880  34065.381000    3.038932     ECU  ...     37.699821   \n",
       "...          ...           ...         ...     ...  ...           ...   \n",
       "90816      32467     75.500476    0.000000     RUS  ...  14468.128906   \n",
       "90817       9832    281.253369    0.000000     RUS  ...  12943.362305   \n",
       "90818      32288     44.549314    0.000000     RUS  ...  16157.847656   \n",
       "90819      17828    538.808249    0.000000     RUS  ...   4346.128418   \n",
       "90820      19298  41369.184550    2.757651     RUS  ...      0.095793   \n",
       "\n",
       "         Powerdist  PrimaryRoad  SecondaryRoad     Slope  TertiaryRoad  \\\n",
       "0      622589.0000    12.719680       0.000000  2.521324    197.039190   \n",
       "1      622588.9375    14.276298       0.000000  0.715136    100.218651   \n",
       "2      622588.9375    14.276298       0.000000  0.922556    100.218651   \n",
       "3      622609.3750    18.676649      24.508936  6.674829     43.411486   \n",
       "4      622609.3750     6.512673      18.597032  7.650049     89.491292   \n",
       "...            ...          ...            ...       ...           ...   \n",
       "90816  622589.8125    62.451511     112.277082  1.968263    861.205086   \n",
       "90817  622589.8125   118.298899     140.942360  0.393874   1132.096021   \n",
       "90818  622589.8125    38.558650      73.424121  0.412605    642.344304   \n",
       "90819  622589.5000    63.311948       5.627337  0.978244    316.949814   \n",
       "90820  622609.5000     0.000000       0.000000  2.918713     25.727123   \n",
       "\n",
       "             gdmp       rsds          tas      wind  \n",
       "0             NaN  19.063463  2995.384528  2.293710  \n",
       "1             NaN  18.820724  2999.923652  2.770978  \n",
       "2             NaN  18.825073  2999.838979  2.721136  \n",
       "3      167.423999  18.745583  2969.209102  0.836408  \n",
       "4      128.872533  17.977419  2920.696152  3.776348  \n",
       "...           ...        ...          ...       ...  \n",
       "90816         NaN   9.967303  2804.318214  2.684799  \n",
       "90817         NaN   9.992375  2804.168017  2.531498  \n",
       "90818         NaN   9.950527  2803.408293  2.519807  \n",
       "90819         NaN   9.815846  2800.570781  2.697752  \n",
       "90820         NaN   9.621017  2608.666671  2.189081  \n",
       "\n",
       "[90821 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Remove spatial_ref and band columns\n",
    "final_merged_df = final_merged_df.drop(['spatial_ref', 'band'], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "final_merged_df.to_csv('training_embedding.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "备注：这里经过了年内去重操作、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>abandonment_year</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>p_area</th>\n",
       "      <th>capacity_m</th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>abandonment_duration</th>\n",
       "      <th>...</th>\n",
       "      <th>Population</th>\n",
       "      <th>Powerdist</th>\n",
       "      <th>PrimaryRoad</th>\n",
       "      <th>SecondaryRoad</th>\n",
       "      <th>Slope</th>\n",
       "      <th>TertiaryRoad</th>\n",
       "      <th>gdmp</th>\n",
       "      <th>rsds</th>\n",
       "      <th>tas</th>\n",
       "      <th>wind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>-13.837500</td>\n",
       "      <td>-171.804169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26510</td>\n",
       "      <td>87614.298290</td>\n",
       "      <td>7.850205</td>\n",
       "      <td>WSM</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>646.119934</td>\n",
       "      <td>622589.0000</td>\n",
       "      <td>12.719680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.521324</td>\n",
       "      <td>197.039190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.063463</td>\n",
       "      <td>2995.384528</td>\n",
       "      <td>2.293710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>-13.829166</td>\n",
       "      <td>-171.995834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26500</td>\n",
       "      <td>10696.657940</td>\n",
       "      <td>0.956408</td>\n",
       "      <td>WSM</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>138.296951</td>\n",
       "      <td>622588.9375</td>\n",
       "      <td>14.276298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715136</td>\n",
       "      <td>100.218651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.820724</td>\n",
       "      <td>2999.923652</td>\n",
       "      <td>2.770978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>-13.829166</td>\n",
       "      <td>-171.987503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26501</td>\n",
       "      <td>66982.132520</td>\n",
       "      <td>5.989546</td>\n",
       "      <td>WSM</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>138.296951</td>\n",
       "      <td>622588.9375</td>\n",
       "      <td>14.276298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.922556</td>\n",
       "      <td>100.218651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.825073</td>\n",
       "      <td>2999.838979</td>\n",
       "      <td>2.721136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>-4.387500</td>\n",
       "      <td>-79.812500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10893</td>\n",
       "      <td>16680.699130</td>\n",
       "      <td>1.511097</td>\n",
       "      <td>ECU</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15.645079</td>\n",
       "      <td>622609.3750</td>\n",
       "      <td>18.676649</td>\n",
       "      <td>24.508936</td>\n",
       "      <td>6.674829</td>\n",
       "      <td>43.411486</td>\n",
       "      <td>167.423999</td>\n",
       "      <td>18.745583</td>\n",
       "      <td>2969.209102</td>\n",
       "      <td>0.836408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>-4.254167</td>\n",
       "      <td>-79.470833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10880</td>\n",
       "      <td>34065.381000</td>\n",
       "      <td>3.038932</td>\n",
       "      <td>ECU</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>37.699821</td>\n",
       "      <td>622609.3750</td>\n",
       "      <td>6.512673</td>\n",
       "      <td>18.597032</td>\n",
       "      <td>7.650049</td>\n",
       "      <td>89.491292</td>\n",
       "      <td>128.872533</td>\n",
       "      <td>17.977419</td>\n",
       "      <td>2920.696152</td>\n",
       "      <td>3.776348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90816</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>55.712502</td>\n",
       "      <td>37.462502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32467</td>\n",
       "      <td>75.500476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>RUS</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14468.128906</td>\n",
       "      <td>622589.8125</td>\n",
       "      <td>62.451511</td>\n",
       "      <td>112.277082</td>\n",
       "      <td>1.968263</td>\n",
       "      <td>861.205086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.967303</td>\n",
       "      <td>2804.318214</td>\n",
       "      <td>2.684799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90817</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>55.737499</td>\n",
       "      <td>37.579166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9832</td>\n",
       "      <td>281.253369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>RUS</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12943.362305</td>\n",
       "      <td>622589.8125</td>\n",
       "      <td>118.298899</td>\n",
       "      <td>140.942360</td>\n",
       "      <td>0.393874</td>\n",
       "      <td>1132.096021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.992375</td>\n",
       "      <td>2804.168017</td>\n",
       "      <td>2.531498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90818</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>55.820835</td>\n",
       "      <td>37.704166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32288</td>\n",
       "      <td>44.549314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>RUS</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>16157.847656</td>\n",
       "      <td>622589.8125</td>\n",
       "      <td>38.558650</td>\n",
       "      <td>73.424121</td>\n",
       "      <td>0.412605</td>\n",
       "      <td>642.344304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.950527</td>\n",
       "      <td>2803.408293</td>\n",
       "      <td>2.519807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90819</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>55.979168</td>\n",
       "      <td>37.262501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17828</td>\n",
       "      <td>538.808249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>RUS</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4346.128418</td>\n",
       "      <td>622589.5000</td>\n",
       "      <td>63.311948</td>\n",
       "      <td>5.627337</td>\n",
       "      <td>0.978244</td>\n",
       "      <td>316.949814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.815846</td>\n",
       "      <td>2800.570781</td>\n",
       "      <td>2.697752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90820</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>67.662498</td>\n",
       "      <td>134.654160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19298</td>\n",
       "      <td>41369.184550</td>\n",
       "      <td>2.757651</td>\n",
       "      <td>RUS</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095793</td>\n",
       "      <td>622609.5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.918713</td>\n",
       "      <td>25.727123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.621017</td>\n",
       "      <td>2608.666671</td>\n",
       "      <td>2.189081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90821 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            time        lat         lon  abandonment_year  unique_id  \\\n",
       "0     2018-01-01 -13.837500 -171.804169               NaN      26510   \n",
       "1     2018-01-01 -13.829166 -171.995834               NaN      26500   \n",
       "2     2018-01-01 -13.829166 -171.987503               NaN      26501   \n",
       "3     2018-01-01  -4.387500  -79.812500               NaN      10893   \n",
       "4     2018-01-01  -4.254167  -79.470833               NaN      10880   \n",
       "...          ...        ...         ...               ...        ...   \n",
       "90816 2020-01-01  55.712502   37.462502               NaN      32467   \n",
       "90817 2020-01-01  55.737499   37.579166               NaN       9832   \n",
       "90818 2020-01-01  55.820835   37.704166               NaN      32288   \n",
       "90819 2020-01-01  55.979168   37.262501               NaN      17828   \n",
       "90820 2020-01-01  67.662498  134.654160               NaN      19298   \n",
       "\n",
       "             p_area  capacity_m country  year  abandonment_duration  ...  \\\n",
       "0      87614.298290    7.850205     WSM  2018                   NaN  ...   \n",
       "1      10696.657940    0.956408     WSM  2018                   NaN  ...   \n",
       "2      66982.132520    5.989546     WSM  2018                   NaN  ...   \n",
       "3      16680.699130    1.511097     ECU  2018                   NaN  ...   \n",
       "4      34065.381000    3.038932     ECU  2018                   NaN  ...   \n",
       "...             ...         ...     ...   ...                   ...  ...   \n",
       "90816     75.500476    0.000000     RUS  2020                   NaN  ...   \n",
       "90817    281.253369    0.000000     RUS  2020                   NaN  ...   \n",
       "90818     44.549314    0.000000     RUS  2020                   NaN  ...   \n",
       "90819    538.808249    0.000000     RUS  2020                   NaN  ...   \n",
       "90820  41369.184550    2.757651     RUS  2020                   NaN  ...   \n",
       "\n",
       "         Population    Powerdist  PrimaryRoad  SecondaryRoad     Slope  \\\n",
       "0        646.119934  622589.0000    12.719680       0.000000  2.521324   \n",
       "1        138.296951  622588.9375    14.276298       0.000000  0.715136   \n",
       "2        138.296951  622588.9375    14.276298       0.000000  0.922556   \n",
       "3         15.645079  622609.3750    18.676649      24.508936  6.674829   \n",
       "4         37.699821  622609.3750     6.512673      18.597032  7.650049   \n",
       "...             ...          ...          ...            ...       ...   \n",
       "90816  14468.128906  622589.8125    62.451511     112.277082  1.968263   \n",
       "90817  12943.362305  622589.8125   118.298899     140.942360  0.393874   \n",
       "90818  16157.847656  622589.8125    38.558650      73.424121  0.412605   \n",
       "90819   4346.128418  622589.5000    63.311948       5.627337  0.978244   \n",
       "90820      0.095793  622609.5000     0.000000       0.000000  2.918713   \n",
       "\n",
       "       TertiaryRoad        gdmp       rsds          tas      wind  \n",
       "0        197.039190         NaN  19.063463  2995.384528  2.293710  \n",
       "1        100.218651         NaN  18.820724  2999.923652  2.770978  \n",
       "2        100.218651         NaN  18.825073  2999.838979  2.721136  \n",
       "3         43.411486  167.423999  18.745583  2969.209102  0.836408  \n",
       "4         89.491292  128.872533  17.977419  2920.696152  3.776348  \n",
       "...             ...         ...        ...          ...       ...  \n",
       "90816    861.205086         NaN   9.967303  2804.318214  2.684799  \n",
       "90817   1132.096021         NaN   9.992375  2804.168017  2.531498  \n",
       "90818    642.344304         NaN   9.950527  2803.408293  2.519807  \n",
       "90819    316.949814         NaN   9.815846  2800.570781  2.697752  \n",
       "90820     25.727123         NaN   9.621017  2608.666671  2.189081  \n",
       "\n",
       "[90821 rows x 27 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id_x</th>\n",
       "      <th>p_area_x</th>\n",
       "      <th>capacity_m_x</th>\n",
       "      <th>country_x</th>\n",
       "      <th>time</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>year_x</th>\n",
       "      <th>abandonment_year</th>\n",
       "      <th>unique_id_y</th>\n",
       "      <th>...</th>\n",
       "      <th>Powerdist</th>\n",
       "      <th>PrimaryRoad</th>\n",
       "      <th>SecondaryRoad</th>\n",
       "      <th>Slope</th>\n",
       "      <th>TertiaryRoad</th>\n",
       "      <th>gdmp</th>\n",
       "      <th>rsds</th>\n",
       "      <th>tas</th>\n",
       "      <th>wind</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18639</th>\n",
       "      <td>67752</td>\n",
       "      <td>8082.070784</td>\n",
       "      <td>0.731052</td>\n",
       "      <td>GBR</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>53.487499</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18677</th>\n",
       "      <td>67834</td>\n",
       "      <td>2716.929277</td>\n",
       "      <td>0.245711</td>\n",
       "      <td>GBR</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>53.412498</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18883</th>\n",
       "      <td>68009</td>\n",
       "      <td>2457.466166</td>\n",
       "      <td>0.222334</td>\n",
       "      <td>GBR</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>53.437500</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20724</th>\n",
       "      <td>65832</td>\n",
       "      <td>426831.056900</td>\n",
       "      <td>31.896933</td>\n",
       "      <td>GBR</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>52.062500</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26806</th>\n",
       "      <td>56002</td>\n",
       "      <td>20156.432690</td>\n",
       "      <td>1.762364</td>\n",
       "      <td>ESP</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>38.812500</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73947</th>\n",
       "      <td>5807</td>\n",
       "      <td>4218.800539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GBR</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>53.362499</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75772</th>\n",
       "      <td>5171</td>\n",
       "      <td>5906.320755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ESP</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>38.812500</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77505</th>\n",
       "      <td>7497</td>\n",
       "      <td>562.506739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GBR</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>52.062500</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85144</th>\n",
       "      <td>17155</td>\n",
       "      <td>345.945594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GBR</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>52.079166</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id_x       p_area_x  capacity_m_x country_x       time  \\\n",
       "18639        67752    8082.070784      0.731052       GBR 2018-01-01   \n",
       "18677        67834    2716.929277      0.245711       GBR 2018-01-01   \n",
       "18883        68009    2457.466166      0.222334       GBR 2018-01-01   \n",
       "20724        65832  426831.056900     31.896933       GBR 2018-01-01   \n",
       "26806        56002   20156.432690      1.762364       ESP 2018-01-01   \n",
       "73947         5807    4218.800539      0.000000       GBR 2020-01-01   \n",
       "75772         5171    5906.320755      0.000000       ESP 2020-01-01   \n",
       "77505         7497     562.506739      0.000000       GBR 2020-01-01   \n",
       "85144        17155     345.945594      0.000000       GBR 2020-01-01   \n",
       "\n",
       "            lon        lat  year_x  abandonment_year  unique_id_y  ...  \\\n",
       "18639  0.004167  53.487499    2018               NaN          NaN  ...   \n",
       "18677  0.004167  53.412498    2018               NaN          NaN  ...   \n",
       "18883  0.004167  53.437500    2018               NaN          NaN  ...   \n",
       "20724  0.004167  52.062500    2018               NaN          NaN  ...   \n",
       "26806  0.004167  38.812500    2018               NaN          NaN  ...   \n",
       "73947  0.004167  53.362499    2020               NaN          NaN  ...   \n",
       "75772  0.004167  38.812500    2020               NaN          NaN  ...   \n",
       "77505  0.004167  52.062500    2020               NaN          NaN  ...   \n",
       "85144  0.004167  52.079166    2020               NaN          NaN  ...   \n",
       "\n",
       "       Powerdist  PrimaryRoad SecondaryRoad  Slope  TertiaryRoad  gdmp  rsds  \\\n",
       "18639        NaN          NaN           NaN    NaN           NaN   NaN   NaN   \n",
       "18677        NaN          NaN           NaN    NaN           NaN   NaN   NaN   \n",
       "18883        NaN          NaN           NaN    NaN           NaN   NaN   NaN   \n",
       "20724        NaN          NaN           NaN    NaN           NaN   NaN   NaN   \n",
       "26806        NaN          NaN           NaN    NaN           NaN   NaN   NaN   \n",
       "73947        NaN          NaN           NaN    NaN           NaN   NaN   NaN   \n",
       "75772        NaN          NaN           NaN    NaN           NaN   NaN   NaN   \n",
       "77505        NaN          NaN           NaN    NaN           NaN   NaN   NaN   \n",
       "85144        NaN          NaN           NaN    NaN           NaN   NaN   NaN   \n",
       "\n",
       "       tas  wind     _merge  \n",
       "18639  NaN   NaN  left_only  \n",
       "18677  NaN   NaN  left_only  \n",
       "18883  NaN   NaN  left_only  \n",
       "20724  NaN   NaN  left_only  \n",
       "26806  NaN   NaN  left_only  \n",
       "73947  NaN   NaN  left_only  \n",
       "75772  NaN   NaN  left_only  \n",
       "77505  NaN   NaN  left_only  \n",
       "85144  NaN   NaN  left_only  \n",
       "\n",
       "[9 rows x 33 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_keys = pd.merge(pv_df_dedup, final_merged_df, on=['lat', 'lon','time'], how='left', indicator=True)\n",
    "missing = missing_keys[missing_keys['_merge'] == 'left_only']\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float32, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glbcropland",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
