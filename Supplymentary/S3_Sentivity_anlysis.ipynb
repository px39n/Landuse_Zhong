{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b678cf",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf082678",
   "metadata": {},
   "source": [
    "\n",
    "- Ê†∏ÂøÉÁõÆÁöÑ\n",
    "ËøõË°åË∂ÖÂèÇÊï∞ÊïèÊÑüÊÄßÂàÜÊûêÂπ∂ÁîüÊàêÂ§ö‰∏™dataframe„ÄÇËÆ∞ÂΩïÂú®‰∏çÂêåË∂ÖÂèÇÊï∞ÁªÑÂêà‰∏ãÁöÑÔºö\n",
    "- ËøáÊãüÂêàscore\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- ÊúÄÁªàÈ¢ÑÊµãËæìÂá∫ÁöÑmean probability\n",
    "\n",
    "- ‰∏öÂä°ÈÄªËæë\n",
    "1. ËÆæËÆ°Ê≠£‰∫§Ë°®ÊïèÊÑüÊÄßÂàÜÊûêÂÆûÈ™åÔºàL18(3^6)Ê≠£‰∫§Ë°®ÔºåÂ∞Ü729Ê¨°ÂÆûÈ™åÂáèÂ∞ëÂà∞18Ê¨°Ôºâ\n",
    "2. ‰ΩøÁî®Ê®°ÂùóÂåñÁöÑËÆ≠ÁªÉÂáΩÊï∞ÔºàÂ∑≤Â≠òÂÇ®Âú®functionÁõÆÂΩï‰∏ãÔºâ\n",
    "3. ÈíàÂØπ‰∏çÂêåÂèÇÊï∞ÁªÑÂêàÊâßË°åÊïèÊÑüÊÄßÂàÜÊûê\n",
    "4. ËæìÂá∫ÁõÆÂΩïÔºö`Supplymentary/ML_sensitivity`\n",
    "\n",
    "- Ë∂ÖÂèÇÊï∞ÈÖçÁΩÆ\n",
    "- **Âü∫Á°ÄÈÖçÁΩÆ**Ôºölearning_rate=0.001, resnet_layers=[128,128,64], d_model=64, num_heads=4, num_layers=2, dropout_rate=0.3\n",
    "- **ÊïèÊÑüÊÄßÊµãËØïÂèÇÊï∞**ÔºöÊØè‰∏™ÂèÇÊï∞3‰∏™Ê∞¥Âπ≥ÔºåÂÖ±6‰∏™ÂèÇÊï∞\n",
    "- **‰∏çËøõË°åÊïèÊÑüÊÄßÊµãËØï**Ôºöbatch_size, epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20859aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GPUÁéØÂ¢ÉÈÖçÁΩÆÊ£ÄÊü• - WSL Ubuntu + bayes-gpu\n",
      "================================================================================\n",
      "TensorFlow ÁâàÊú¨: 2.19.0\n",
      "CUDA build: None\n",
      "cuDNN build: None\n",
      "\n",
      "‚ö†Ô∏è Êú™Ê£ÄÊµãÂà∞ GPUÔºåÂ∞Ü‰ΩøÁî® CPU\n",
      "\n",
      "================================================================================\n",
      "GPUÊÄßËÉΩÊµãËØï\n",
      "================================================================================\n",
      "‚úÖ ËÆæÂ§á: /CPU:0\n",
      "‚úÖ Áü©Èòµ‰πòÊ≥ï 2000x2000 ËÄóÊó∂: 39.61 ms\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== ÁéØÂ¢ÉÈÖçÁΩÆ ==========\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# 2. Áõ¥Êé•ÈáçÂÆöÂêëstderrÔºà‰∏¥Êó∂Ôºâ\n",
    "_old_stderr = sys.stderr\n",
    "sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "# 3. ÂØºÂÖ•matplotlibÂíåtensorflow\n",
    "try:\n",
    "    import matplotlib\n",
    "    import matplotlib.font_manager as fm\n",
    "    matplotlib.rcParams['font.family'] = 'DejaVu Sans'\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "finally:\n",
    "    # 4. ÊÅ¢Â§çstderr\n",
    "    sys.stderr.close()\n",
    "    sys.stderr = _old_stderr\n",
    "\n",
    "# 5. ËÆæÁΩÆË≠¶ÂëäËøáÊª§\n",
    "import sklearn\n",
    "warnings.filterwarnings('ignore', category=sklearn.base.InconsistentVersionWarning)\n",
    "warnings.filterwarnings('ignore', module='matplotlib')\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GPUÁéØÂ¢ÉÈÖçÁΩÆÊ£ÄÊü• - WSL Ubuntu + bayes-gpu\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ---------------------------\n",
    "# 0. Âü∫Êú¨‰ø°ÊÅØÔºàÂèØÈÄâÔºâ\n",
    "# ---------------------------\n",
    "print(f\"TensorFlow ÁâàÊú¨: {tf.__version__}\")\n",
    "# Â∞ùËØïÊâìÂç∞ÊûÑÂª∫Êó∂ CUDA/cuDNN ÁâàÊú¨Ôºà‰∏çÂêåÁâàÊú¨Â≠óÊÆµÂêçÂèØËÉΩ‰∏çÂêåÔºâ\n",
    "try:\n",
    "    from tensorflow.sysconfig import get_build_info\n",
    "    bi = get_build_info()\n",
    "    print(\"CUDA build:\", bi.get(\"cuda_version\"))\n",
    "    print(\"cuDNN build:\", bi.get(\"cudnn_version\"))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Ê£ÄÊü•Âπ∂ÈÖçÁΩÆ GPU\n",
    "# ---------------------------\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    print(f\"\\n‚úÖ Ê£ÄÊµãÂà∞ {len(physical_gpus)} ‰∏™ GPU:\")\n",
    "    # ÂÖàÈÖçÁΩÆÊòæÂ≠òÁ≠ñÁï•ÔºàÂøÖÈ°ªÂú®‰ªª‰Ωï GPU ÂàùÂßãÂåñ‰πãÂâçÔºâ\n",
    "    try:\n",
    "        for g in physical_gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "        print(\"‚úÖ GPU ÊòæÂ≠òÂä®ÊÄÅÂ¢ûÈïøÂ∑≤ÂêØÁî®\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è ÊòæÂ≠òÂä®ÊÄÅÂ¢ûÈïøËÆæÁΩÆÂ§±Ë¥•ÔºàÂèØËÉΩÂ∑≤ÂàùÂßãÂåñÔºâÔºö{e}\")\n",
    "\n",
    "    # ÊâìÂç∞ËÆæÂ§áÁªÜËäÇ\n",
    "    for i, g in enumerate(physical_gpus):\n",
    "        print(f\"   GPU {i}: {g.name}\")\n",
    "        try:\n",
    "            det = tf.config.experimental.get_device_details(g)\n",
    "            # Â∏∏ËßÅÈîÆÔºö'device_name', 'compute_capability'\n",
    "            print(f\"       ËÆæÂ§áËØ¶ÊÉÖ: {det}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Êú™Ê£ÄÊµãÂà∞ GPUÔºåÂ∞Ü‰ΩøÁî® CPU\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Â∞èÊµãËØïÔºöGPU ËÆ°ÁÆó‰∏éËÆ°Êó∂\n",
    "# ---------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GPUÊÄßËÉΩÊµãËØï\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "use_gpu = len(physical_gpus) > 0\n",
    "device_str = \"/GPU:0\" if use_gpu else \"/CPU:0\"\n",
    "\n",
    "# ÊöñÊú∫ÔºàÈÅøÂÖçÁ¨¨‰∏ÄÊ¨°Ë∞ÉÁî®ÁöÑÂä†ËΩΩÂºÄÈîÄÂΩ±ÂìçËÆ°Êó∂Ôºâ\n",
    "with tf.device(device_str):\n",
    "    _ = tf.matmul(tf.random.normal([256, 256]), tf.random.normal([256, 256]))  # warm-up\n",
    "    _ = tf.matmul(tf.random.normal([256, 256]), tf.random.normal([256, 256]))  # warm-up\n",
    "\n",
    "# ËÆ°Êó∂ÊµãËØïÔºàÁ°Æ‰øùÂêåÊ≠•Ôºâ\n",
    "size = 2000  # Â¶ÇÈúÄÊõ¥ÊòéÊòæÂ∑ÆÂºÇÂèØË∞ÉÂ§ßÂà∞ 4096\n",
    "with tf.device(device_str):\n",
    "    a = tf.random.normal([size, size])\n",
    "    b = tf.random.normal([size, size])\n",
    "    t0 = time.time()\n",
    "    c = tf.matmul(a, b)\n",
    "    _ = c.numpy()  # ÂêåÊ≠•Âà∞‰∏ªÊú∫ÔºåÁ°Æ‰øùËÆ°Êó∂ÁúüÂÆû\n",
    "    elapsed_ms = (time.time() - t0) * 1000\n",
    "\n",
    "print(f\"‚úÖ ËÆæÂ§á: {device_str}\")\n",
    "print(f\"‚úÖ Áü©Èòµ‰πòÊ≥ï {size}x{size} ËÄóÊó∂: {elapsed_ms:.2f} ms\")\n",
    "if use_gpu:\n",
    "    print(\"üöÄ GPU Âä†ÈÄüÊ≠£Â∏∏Ôºà‰∏é CPU Áõ∏ÊØîÂ∫îÊòéÊòæÊõ¥Âø´Ôºâ\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50aa111",
   "metadata": {},
   "source": [
    "## 1„ÄÅLoad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed049a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "È°πÁõÆÊ†πÁõÆÂΩï: C:\\Dev\\Landuse_Zhong_clean\n",
      "Êï∞ÊçÆË∑ØÂæÑ: C:\\Dev\\Landuse_Zhong_clean\\data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ÊúÄÂèØÈù†ÁöÑÊñπÊ≥ïÔºöÊü•ÊâæÂåÖÂê´dataÂíåfunctionÁõÆÂΩïÁöÑÈ°πÁõÆÊ†πÁõÆÂΩï\n",
    "def find_project_root(start_path=None):\n",
    "    \"\"\"Êü•ÊâæÈ°πÁõÆÊ†πÁõÆÂΩïÔºàÂåÖÂê´dataÂíåfunctionÁõÆÂΩïÁöÑÁõÆÂΩïÔºâ\"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = Path.cwd()\n",
    "    \n",
    "    current = Path(start_path).resolve()\n",
    "    \n",
    "    # Âêë‰∏äÊü•ÊâæÔºåÁõ¥Âà∞ÊâæÂà∞ÂåÖÂê´dataÂíåfunctionÁõÆÂΩïÁöÑÁõÆÂΩï\n",
    "    for _ in range(5):  # ÊúÄÂ§öÂêë‰∏äÊü•Êâæ5Â±Ç\n",
    "        if (current / 'data').exists() and (current / 'function').exists():\n",
    "            return current\n",
    "        parent = current.parent\n",
    "        if parent == current:  # Âà∞ËææÊ†πÁõÆÂΩï\n",
    "            break\n",
    "        current = parent\n",
    "    \n",
    "    # Â¶ÇÊûúÊâæ‰∏çÂà∞ÔºåÂÅáËÆæÂΩìÂâçÁõÆÂΩïÁöÑÁà∂ÁõÆÂΩïÊòØÈ°πÁõÆÊ†πÁõÆÂΩï\n",
    "    return Path.cwd().parent\n",
    "\n",
    "project_root = find_project_root()\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "DATA_PATH = project_root / 'data'\n",
    "\n",
    "print(f\"È°πÁõÆÊ†πÁõÆÂΩï: {project_root}\")\n",
    "print(f\"Êï∞ÊçÆË∑ØÂæÑ: {DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a7cadea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gogogo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764019411.733590  147604 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "I0000 00:00:1764019411.878892  147604 cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764019413.225357  147604 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] TensorFlow available\n",
      "[OK] scikeras available\n",
      "[OK] SHAP available\n",
      "us_abandon path: /mnt/c/Dev/Landuse_Zhong_clean/data/us_abandon_clean.csv\n",
      "Âä†ËΩΩÊï∞ÊçÆ...\n",
      "You want to predict the year: 2020.0\n",
      "You want to predict the year: 2020.0\n",
      "È¢ÑÂ§ÑÁêÜÊï∞ÊçÆ...\n",
      "Âàó GDPpc Ê≤°ÊúâÈúÄË¶ÅÂ°´ÂÖÖÁöÑÈùûÊ≠£ÂÄºÊàñNaNÂÄº\n",
      "Âàó GDPpc Ê≤°ÊúâÈúÄË¶ÅÂ°´ÂÖÖÁöÑÈùûÊ≠£ÂÄºÊàñNaNÂÄº\n",
      "‚úÖ Êï∞ÊçÆÂä†ËΩΩÂÆåÊàê\n",
      "  - df_embedding_fill: 10473 Ë°å\n",
      "  - df_abandon_filtered: 70337 Ë°å\n",
      "  - features_no_coords: 15 ‰∏™ÁâπÂæÅ\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from function import (\n",
    "    load_embedding, load_abandon, fill_nonpositive_with_nearest, \n",
    "    filter_duplicates, NUMERIC_FEATURES, CAT_COLS, PATHS\n",
    ")\n",
    "from function.pipeline import run_correct_training_pipeline\n",
    "from function.model_saving import save_complete_model_pipeline\n",
    "from datetime import datetime\n",
    "import platform\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def normalize_path(path):\n",
    "    \"\"\"\n",
    "    Ë∑®Âπ≥Âè∞Ë∑ØÂæÑËßÑËåÉÂåñ\n",
    "    Â∞ÜWindowsË∑ØÂæÑ‰∏≠ÁöÑÂèçÊñúÊù†ËΩ¨Êç¢‰∏∫Ê≠£ÊñúÊù†ÔºàÂú®Linux/WSL‰∏≠Ôºâ\n",
    "    Windows: r'data\\file.shp' -> r'data\\file.shp' (‰øùÊåÅ‰∏çÂèò)\n",
    "    Linux/WSL: r'data\\file.shp' -> 'data/file.shp' (ËΩ¨Êç¢)\n",
    "    \"\"\"\n",
    "    if isinstance(path, str):\n",
    "        # Ê£ÄÊµãÊòØÂê¶Âú®WSLÊàñLinuxÁéØÂ¢É‰∏≠\n",
    "        is_linux = platform.system() in ['Linux', 'Darwin']\n",
    "        \n",
    "        if is_linux:\n",
    "            # Âú®Linux/WSL‰∏≠ÔºåÂ∞ÜÂèçÊñúÊù†ËΩ¨‰∏∫Ê≠£ÊñúÊù†\n",
    "            path = path.replace('\\\\', '/')\n",
    "        \n",
    "        return path\n",
    "    return path\n",
    "\n",
    "def normalize_paths_dict(paths_dict):\n",
    "    \"\"\"\n",
    "    ËßÑËåÉÂåñË∑ØÂæÑÂ≠óÂÖ∏‰∏≠ÁöÑÊâÄÊúâË∑ØÂæÑ\n",
    "    \"\"\"\n",
    "    normalized = {}\n",
    "    for key, value in paths_dict.items():\n",
    "        normalized[key] = normalize_path(value)\n",
    "    return normalized\n",
    "\n",
    "def clip_data_with_us_states(df, us_states_gdf, lon_col='lon', lat_col='lat'):\n",
    "    \"\"\"\n",
    "    ‰ΩøÁî®ÁæéÂõΩÂ∑ûÁïå shapefile Ââ™Ë£ÅÁÇπÊï∞ÊçÆÔºõÂÖºÂÆπ‰∏çÂêåÁâàÊú¨ÁöÑ geopandas ÂèÇÊï∞Âêç\n",
    "    \"\"\"\n",
    "    geometry = [Point(xy) for xy in zip(df[lon_col], df[lat_col])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')\n",
    "    us_states_4326 = us_states_gdf.to_crs('EPSG:4326')\n",
    "\n",
    "    try:\n",
    "        clipped = gpd.sjoin(gdf, us_states_4326, how='inner', predicate='within')\n",
    "    except TypeError:\n",
    "        # ÊóßÁâàÊú¨ geopandas ‰ΩøÁî® op ÂèÇÊï∞\n",
    "        clipped = gpd.sjoin(gdf, us_states_4326, how='inner', op='within')\n",
    "\n",
    "    # Ê∏ÖÁêÜ shapefile ÈôÑÂä†Â≠óÊÆµ\n",
    "    clipped = clipped.drop(columns=['geometry', 'index_right'], errors='ignore')\n",
    "    for col in us_states_gdf.columns:\n",
    "        if col in clipped.columns:\n",
    "            clipped = clipped.drop(columns=[col], errors='ignore')\n",
    "    return clipped\n",
    "\n",
    "PATHS = normalize_paths_dict(PATHS)\n",
    "PATHS['us_pv_embedding'] = normalize_path(str(DATA_PATH / 'training_embedding.csv'))\n",
    "PATHS['us_abandon'] = normalize_path(str(DATA_PATH / 'us_abandon_clean.csv'))\n",
    "print(f\"us_abandon path: {PATHS['us_abandon']}\")\n",
    "\n",
    "\n",
    "usa_bounds_main = dict(lon_min=-125, lon_max=-65, lat_min=25, lat_max=49)\n",
    "# Âä†ËΩΩÊï∞ÊçÆ\n",
    "print(\"Âä†ËΩΩÊï∞ÊçÆ...\")\n",
    "df_embedding = load_embedding(PATHS['us_pv_embedding'])\n",
    "df_abandon = load_abandon(PATHS['us_abandon'])\n",
    "\n",
    "df_abandon = load_abandon(PATHS['us_abandon'])\n",
    "df_embedding = load_embedding(PATHS['us_pv_embedding'])\n",
    "\n",
    "# ÂàùÊ≠•ÁªèÁ∫¨Â∫¶ËåÉÂõ¥ËøáÊª§\n",
    "df_embedding = df_embedding[\n",
    "    (df_embedding['lon'] >= usa_bounds_main['lon_min']) &\n",
    "    (df_embedding['lon'] <= usa_bounds_main['lon_max']) &\n",
    "    (df_embedding['lat'] >= usa_bounds_main['lat_min']) &\n",
    "    (df_embedding['lat'] <= usa_bounds_main['lat_max'])\n",
    "]\n",
    "\n",
    "df_abandon = df_abandon[\n",
    "    (df_abandon['lon'] >= usa_bounds_main['lon_min']) &\n",
    "    (df_abandon['lon'] <= usa_bounds_main['lon_max']) &\n",
    "    (df_abandon['lat'] >= usa_bounds_main['lat_min']) &\n",
    "    (df_abandon['lat'] <= usa_bounds_main['lat_max'])\n",
    "]\n",
    "# Á¨¨‰∫åÊ¨°Áî®Â∑ûÁïåÁü¢ÈáèÊõ¥Á≤æÁ°ÆË£ÅÂâ™\n",
    "us_nation = gpd.read_file(normalize_path(str(DATA_PATH / 'US_data' / 'cb_2018_us_nation_5m.shp')))\n",
    "df_abandon = clip_data_with_us_states(df_abandon, us_nation)\n",
    "df_embedding = clip_data_with_us_states(df_embedding, us_nation)\n",
    "\n",
    "# Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ\n",
    "print(\"È¢ÑÂ§ÑÁêÜÊï∞ÊçÆ...\")\n",
    "df_embedding_fill = fill_nonpositive_with_nearest(df_embedding)\n",
    "df_abandon_fill = fill_nonpositive_with_nearest(df_abandon)\n",
    "df_abandon_filtered = filter_duplicates(df_abandon_fill, df_embedding_fill)\n",
    "\n",
    "# ÂÆö‰πâÁâπÂæÅÂàóË°®ÔºàÊéíÈô§ÂùêÊ†áÔºâ\n",
    "features_no_coords = [f for f in (NUMERIC_FEATURES + CAT_COLS) if f not in ['lat', 'lon']]\n",
    "features_no_coords = [c for c in features_no_coords if c in df_embedding_fill.columns]\n",
    "\n",
    "print(f\"‚úÖ Êï∞ÊçÆÂä†ËΩΩÂÆåÊàê\")\n",
    "print(f\"  - df_embedding_fill: {len(df_embedding_fill)} Ë°å\")\n",
    "print(f\"  - df_abandon_filtered: {len(df_abandon_filtered)} Ë°å\")\n",
    "print(f\"  - features_no_coords: {len(features_no_coords)} ‰∏™ÁâπÂæÅ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476dd674",
   "metadata": {},
   "source": [
    "## 2„ÄÅHyperparameter settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce49666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Âü∫Á°ÄÈÖçÁΩÆÔºàbase model = ÂêÑÂõ†Â≠êÁöÑ‰∏≠Á≠âÊ∞¥Âπ≥Ôºâ\n",
    "base_configs = {\n",
    "    \"learning_rate\": 0.001,           # ‰∏≠Á≠â\n",
    "    \"dropout_rate\": 0.3,              # ‰∏≠Á≠â\n",
    "    \"transformer_config\": {\n",
    "        \"d_model\": 64,                # ‰∏≠Á≠âÂÆΩÂ∫¶\n",
    "        \"num_heads\": 4,\n",
    "        \"num_layers\": 8,              # ‚Üê ‰∏≠Á≠âÊ∑±Â∫¶ÔºàÊµÖ/‰∏≠/Ê∑±Ôºö4 / 8 / 12Ôºâ\n",
    "    },\n",
    "    \"resnet_width\": 128,\n",
    "    \"resnet_depth\": 6,\n",
    "}\n",
    "\n",
    "# ÊïèÊÑüÊÄßÊµãËØïÂèÇÊï∞Ôºà6‰∏™Âõ†Â≠êÔºåÊØè‰∏™3‰∏™Ê∞¥Âπ≥Ôºâ\n",
    "sensitivity_configs = {\n",
    "    # 1. TransformerÂÆΩÂ∫¶Ôºàd_modelÔºâ\n",
    "    \"d_model\": [32, 64, 128],             # Â∞è / ‰∏≠ / Â§ß\n",
    "\n",
    "    # 2. TransformerÊ∑±Â∫¶Ôºànum_layersÔºâ\n",
    "    \"num_layers\": [4, 8, 12],             # ÊµÖ / ‰∏≠ / Ê∑±ÔºàÂ∑ÆÂºÇÊòéÊòæÔºâ\n",
    "\n",
    "    # 3. ResNetÂÆΩÂ∫¶ÔºàÁ¨¨‰∏ÄÂ±ÇÂÆΩÂ∫¶Ôºâ\n",
    "    \"resnet_width\": [64, 128, 256],       # Â∞è / ‰∏≠ / Â§ß\n",
    "\n",
    "    # 4. ResNetÊ∑±Â∫¶ÔºàÂ±ÇÊï∞Ôºâ\n",
    "    \"resnet_depth\": [3, 6, 9],            # ÊµÖ / ‰∏≠ / Ê∑±ÔºàÊòéÊòæÂå∫ÂàÜÔºâ\n",
    "\n",
    "    # 5. Â≠¶‰π†Áéá\n",
    "    \"learning_rate\": [0.0001, 0.001, 0.01],  # Â∞è / ‰∏≠ / Â§ß\n",
    "\n",
    "    # 6. DropoutÁéá\n",
    "    \"dropout_rate\": [0.1, 0.3, 0.5],      # Âº± / ‰∏≠ / Âº∫Ê≠£Âàô\n",
    "}\n",
    "\n",
    "# ËæÖÂä©ÂáΩÊï∞ÔºöÂ∞Üresnet_widthÂíåresnet_depthËΩ¨Êç¢‰∏∫resnet_layersÂàóË°®\n",
    "def generate_resnet_layers(width, depth):\n",
    "    layers = []\n",
    "    current_width = width\n",
    "    for i in range(depth):\n",
    "        layers.append(int(current_width))\n",
    "        if i < depth - 1:  # ÊúÄÂêé‰∏ÄÂ±Ç‰∏çÁªßÁª≠ÂáèÂçä\n",
    "            current_width = current_width / 2\n",
    "    return layers\n",
    "\n",
    "# base_model ÁöÑ resnet_layers Á§∫‰æã\n",
    "base_resnet_layers = generate_resnet_layers(128, 6)\n",
    "# e.g. [128, 64, 32, 16, 8, 4]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3703b3",
   "metadata": {},
   "source": [
    "|   Exp |   d_model |   num_layers |   resnet_width |   resnet_depth |   learning_rate |   dropout_rate |\n",
    "|------:|----------:|-------------:|---------------:|---------------:|----------------:|---------------:|\n",
    "|     1 |        32 |            4 |             64 |              3 |          0.0001 |            0.1 |\n",
    "|     2 |        32 |            8 |            128 |              6 |          0.001  |            0.3 |\n",
    "|     3 |        32 |           12 |            256 |              9 |          0.01   |            0.5 |\n",
    "|     4 |        64 |            4 |             64 |              6 |          0.001  |            0.5 |\n",
    "|     5 |        64 |            8 |            128 |              9 |          0.01   |            0.1 |\n",
    "|     6 |        64 |           12 |            256 |              3 |          0.0001 |            0.3 |\n",
    "|     7 |       128 |            4 |            128 |              3 |          0.01   |            0.3 |\n",
    "|     8 |       128 |            8 |            256 |              6 |          0.0001 |            0.5 |\n",
    "|     9 |       128 |           12 |             64 |              9 |          0.001  |            0.1 |\n",
    "|    10 |        32 |            4 |            256 |              9 |          0.001  |            0.3 |\n",
    "|    11 |        32 |            8 |             64 |              3 |          0.01   |            0.5 |\n",
    "|    12 |        32 |           12 |            128 |              6 |          0.0001 |            0.1 |\n",
    "|    13 |        64 |            4 |            128 |              9 |          0.0001 |            0.5 |\n",
    "|    14 |        64 |            8 |            256 |              3 |          0.001  |            0.1 |\n",
    "|    15 |        64 |           12 |             64 |              6 |          0.01   |            0.3 |\n",
    "|    16 |       128 |            4 |            256 |              6 |          0.001  |            0.1 |\n",
    "|    17 |       128 |            8 |             64 |              9 |          0.01   |            0.3 |\n",
    "|    18 |       128 |           12 |            128 |              3 |          0.0001 |            0.5 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f7929a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ L18(3^6)Ê≠£‰∫§Ë°®ÁîüÊàêÂÆåÊàê\n",
      "  - ÂÆûÈ™åÊ¨°Êï∞: 18 (ÂÖ®Âõ†Â≠êËÆæËÆ°ÈúÄË¶Å 729 Ê¨°)\n",
      "  - ÂáèÂ∞ëÂÆûÈ™åÊ¨°Êï∞: 711 Ê¨° (97.5%)\n",
      "  - Ê≠£‰∫§Ë°®ÂΩ¢Áä∂: (18, 6)\n"
     ]
    }
   ],
   "source": [
    "# 3. Ê≠£‰∫§ÂÆûÈ™åËÆæËÆ°ÔºàL18(3^6)Ôºâ\n",
    "def generate_l18_orthogonal_array():\n",
    "    \"\"\"\n",
    "    ÁîüÊàêL18(3^6)Ê†áÂáÜÊ≠£‰∫§Ë°®\n",
    "    18Ë°åÔºàÂÆûÈ™åÊ¨°Êï∞Ôºâ√ó 6ÂàóÔºàÂõ†Â≠êÊï∞Ôºâ\n",
    "    ÊØèÂàóÂåÖÂê´0,1,2‰∏â‰∏™Ê∞¥Âπ≥ÁöÑÂπ≥Ë°°ÂàÜÂ∏É\n",
    "    \"\"\"\n",
    "    # Ê†áÂáÜL18(3^6)Ê≠£‰∫§Ë°®\n",
    "    orthogonal_array = np.array([\n",
    "        [0, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 1, 1, 1, 1],\n",
    "        [0, 2, 2, 2, 2, 2],\n",
    "        [1, 0, 0, 1, 1, 2],\n",
    "        [1, 1, 1, 2, 2, 0],\n",
    "        [1, 2, 2, 0, 0, 1],\n",
    "        [2, 0, 1, 0, 2, 1],\n",
    "        [2, 1, 2, 1, 0, 2],\n",
    "        [2, 2, 0, 2, 1, 0],\n",
    "        [0, 0, 2, 2, 1, 1],\n",
    "        [0, 1, 0, 0, 2, 2],\n",
    "        [0, 2, 1, 1, 0, 0],\n",
    "        [1, 0, 1, 2, 0, 2],\n",
    "        [1, 1, 2, 0, 1, 0],\n",
    "        [1, 2, 0, 1, 2, 1],\n",
    "        [2, 0, 2, 1, 1, 0],\n",
    "        [2, 1, 0, 2, 2, 1],\n",
    "        [2, 2, 1, 0, 0, 2]\n",
    "    ])\n",
    "    return orthogonal_array\n",
    "\n",
    "# ÁîüÊàêÊ≠£‰∫§Ë°®\n",
    "orthogonal_array = generate_l18_orthogonal_array()\n",
    "print(\"‚úÖ L18(3^6)Ê≠£‰∫§Ë°®ÁîüÊàêÂÆåÊàê\")\n",
    "print(f\"  - ÂÆûÈ™åÊ¨°Êï∞: {len(orthogonal_array)} (ÂÖ®Âõ†Â≠êËÆæËÆ°ÈúÄË¶Å {3**6} Ê¨°)\")\n",
    "print(f\"  - ÂáèÂ∞ëÂÆûÈ™åÊ¨°Êï∞: {3**6 - len(orthogonal_array)} Ê¨° ({100*(1-len(orthogonal_array)/(3**6)):.1f}%)\")\n",
    "print(f\"  - Ê≠£‰∫§Ë°®ÂΩ¢Áä∂: {orthogonal_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa9b3dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÂèÇÊï∞ÁªÑÂêàÊò†Â∞ÑÂÆåÊàê\n",
      "  - ÂÖ± 18 ‰∏™ÂèÇÊï∞ÁªÑÂêà\n",
      "  - Á§∫‰æãÁªÑÂêà 0:\n",
      "      d_model: 32\n",
      "      num_layers: 4\n",
      "      learning_rate: 0.0001\n",
      "      dropout_rate: 0.1\n",
      "      resnet_layers: [64, 32, 16]\n",
      "      _resnet_width: 64\n",
      "      _resnet_depth: 3\n",
      "      num_heads: 2\n"
     ]
    }
   ],
   "source": [
    "# 4. ÂèÇÊï∞Êò†Â∞ÑÂáΩÊï∞ÔºàÊõ¥Êñ∞ÁâàÔºöÂ§ÑÁêÜResNetÂÆΩÂ∫¶ÂíåÊ∑±Â∫¶Ôºâ\n",
    "def map_orthogonal_to_params(orthogonal_array, param_levels):\n",
    "    \"\"\"\n",
    "    Â∞ÜÊ≠£‰∫§Ë°®‰∏≠ÁöÑÊ∞¥Âπ≥Á¥¢ÂºïÊò†Â∞ÑÂà∞ÂÆûÈôÖÂèÇÊï∞ÂÄº\n",
    "    ÁâπÊÆäÂ§ÑÁêÜÔºöÂ∞Üresnet_widthÂíåresnet_depthËΩ¨Êç¢‰∏∫resnet_layersÂàóË°®\n",
    "    \n",
    "    ÂèÇÊï∞:\n",
    "    - orthogonal_array: 18√ó6ÁöÑÊ≠£‰∫§Ë°®Êï∞ÁªÑ\n",
    "    - param_levels: ÂèÇÊï∞Â≠óÂÖ∏ÔºåÊØè‰∏™ÂèÇÊï∞ÂØπÂ∫î3‰∏™Ê∞¥Âπ≥ÂÄºÁöÑÂàóË°®\n",
    "    \n",
    "    ËøîÂõû:\n",
    "    - param_combinations: ÂèÇÊï∞ÁªÑÂêàÂàóË°®ÔºàÂåÖÂê´ËΩ¨Êç¢ÂêéÁöÑresnet_layersÔºâ\n",
    "    \"\"\"\n",
    "    param_names = list(param_levels.keys())\n",
    "    param_combinations = []\n",
    "    \n",
    "    for row in orthogonal_array:\n",
    "        param_dict = {}\n",
    "        for i, param_name in enumerate(param_names):\n",
    "            level_idx = row[i]\n",
    "            param_dict[param_name] = param_levels[param_name][level_idx]\n",
    "        \n",
    "        # ÁâπÊÆäÂ§ÑÁêÜÔºöÂ∞Üresnet_widthÂíåresnet_depthËΩ¨Êç¢‰∏∫resnet_layers\n",
    "        if 'resnet_width' in param_dict and 'resnet_depth' in param_dict:\n",
    "            resnet_layers = generate_resnet_layers(\n",
    "                param_dict['resnet_width'], \n",
    "                param_dict['resnet_depth']\n",
    "            )\n",
    "            param_dict['resnet_layers'] = resnet_layers\n",
    "            # ‰øùÁïôÂéüÂßãÂÄºÁî®‰∫éËÆ∞ÂΩï\n",
    "            param_dict['_resnet_width'] = param_dict.pop('resnet_width')\n",
    "            param_dict['_resnet_depth'] = param_dict.pop('resnet_depth')\n",
    "        \n",
    "        # Á°Æ‰øùnum_heads‰∏éd_modelÂÖºÂÆπÔºàd_modelÂøÖÈ°ªËÉΩË¢´num_headsÊï¥Èô§Ôºâ\n",
    "        if 'd_model' in param_dict:\n",
    "            d_model = param_dict['d_model']\n",
    "            # Ê†πÊçÆd_modelËá™Âä®ÈÄâÊã©ÂêàÈÄÇÁöÑnum_heads\n",
    "            if d_model == 32:\n",
    "                param_dict['num_heads'] = 2  # 32/2=16\n",
    "            elif d_model == 64:\n",
    "                param_dict['num_heads'] = 4  # 64/4=16\n",
    "            elif d_model == 96:\n",
    "                param_dict['num_heads'] = 4  # 96/4=24ÔºàÊàñ8Ôºå‰ΩÜ4Êõ¥Á®≥ÂÆöÔºâ\n",
    "        \n",
    "        param_combinations.append(param_dict)\n",
    "    \n",
    "    return param_combinations\n",
    "\n",
    "# ÁîüÊàêÂèÇÊï∞ÁªÑÂêà\n",
    "param_combinations = map_orthogonal_to_params(orthogonal_array, sensitivity_configs)\n",
    "print(\"‚úÖ ÂèÇÊï∞ÁªÑÂêàÊò†Â∞ÑÂÆåÊàê\")\n",
    "print(f\"  - ÂÖ± {len(param_combinations)} ‰∏™ÂèÇÊï∞ÁªÑÂêà\")\n",
    "print(f\"  - Á§∫‰æãÁªÑÂêà 0:\")\n",
    "for k, v in param_combinations[0].items():\n",
    "    print(f\"      {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8bf92f",
   "metadata": {},
   "source": [
    "## 3„ÄÅSensitivity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d7b2f8",
   "metadata": {},
   "source": [
    "\n",
    "Ê†∏ÂøÉÁõÆÁöÑÔºöÊàëËÆ≠ÁªÉÁîüÊàê‰∫Ü\n",
    "\n",
    "Ê®°ÂûãÂΩìÂâçÁöÑÊñá‰ª∂Â≠òÂÇ®Ë∑ØÂæÑ\n",
    "```\n",
    " Supplymentary/\n",
    " ‚îî‚îÄ‚îÄ Supplymentary/\n",
    "     ‚îî‚îÄ‚îÄ ML_sensitivity/\n",
    "         ‚îî‚îÄ‚îÄ results/\n",
    "             ‚îú‚îÄ‚îÄ E1/\n",
    "             ‚îî‚îÄ‚îÄ ...\n",
    "             ‚îî‚îÄ‚îÄ E18/\n",
    "```\n",
    "\n",
    "\n",
    " ‰ª•‰∏ãÊòØÂ≠òÂÇ®Êñá‰ª∂ÁöÑjsonÁªìÊûÑÔºàÊ†ëÂΩ¢ÁªìÊûÑ„ÄÅ‰ª•E2ÁºñÂè∑‰∏∫‰æãÔºå‰∏ÄÂÖ±Êúâ18‰∏™ÁºñÂè∑Ôºâ\n",
    " \n",
    " Êàë‰ª¨‰∏ªË¶ÅÂÖ≥Ê≥®ÁöÑÊòØd_modelÔºånum_layersÔºålearning_rateÔºådropout_rateÔºåresnet_layersÔºå_resnet_widthÔºå_resnet_depth‰∏ªË¶ÅÂåÖÂê´‰∫ÜËøô6‰∏™ÂèÇÊï∞ÁªÑÊàêÁöÑÊ∞¥Âπ≥Ôºà‰∏ÄÂÖ±Êúâ18‰∏™Ôºâ\n",
    "\n",
    "```\n",
    " ‚îú‚îÄ‚îÄ d_model: 32\n",
    " ‚îú‚îÄ‚îÄ num_layers: 8\n",
    " ‚îú‚îÄ‚îÄ learning_rate: 0.001\n",
    " ‚îú‚îÄ‚îÄ dropout_rate: 0.3\n",
    " ‚îú‚îÄ‚îÄ resnet_layers: \n",
    " ‚îÇ     ‚îî‚îÄ‚îÄ [128, 64, 32, 16, 8, 4]\n",
    " ‚îú‚îÄ‚îÄ _resnet_width: 128\n",
    " ‚îú‚îÄ‚îÄ _resnet_depth: 6\n",
    " ‚îú‚îÄ‚îÄ num_heads: 2\n",
    " ‚îú‚îÄ‚îÄ overfitting_score: 0.01461287859752991\n",
    " ‚îú‚îÄ‚îÄ accuracy: 0.888474913065077\n",
    " ‚îú‚îÄ‚îÄ precision: 0.8754562043795621\n",
    " ‚îú‚îÄ‚îÄ recall: 0.9159904534606206\n",
    " ‚îú‚îÄ‚îÄ f1: 0.8952647539071612\n",
    " ‚îú‚îÄ‚îÄ mean_probability: '0.8077919'\n",
    " ‚îú‚îÄ‚îÄ exp_id: 'E2'\n",
    " ‚îú‚îÄ‚îÄ training_plot_path: 'results/E2/images/E2_training_results.png'\n",
    " ‚îú‚îÄ‚îÄ pipeline_plot_path: 'results/E2/images/E2_pipeline_analysis.png'\n",
    " ‚îú‚îÄ‚îÄ learning_curve_plot_path: 'results/E2/images/E2_learning_curve.png'\n",
    " ‚îú‚îÄ‚îÄ model_path: '/output/models/E2_transformer_generation.pkl'\n",
    " ‚îú‚îÄ‚îÄ uploaded_model_files: \n",
    " ‚îÇ     ‚îú‚îÄ‚îÄ E2_transformer_generation_gmm.pkl\n",
    " ‚îÇ     ‚îú‚îÄ‚îÄ E2_transformer_generation_preprocessor.pkl\n",
    " ‚îÇ     ‚îú‚îÄ‚îÄ E2_transformer_generation_test_data.npz\n",
    " ‚îÇ     ‚îú‚îÄ‚îÄ E2_transformer_generation_config.json\n",
    " ‚îÇ     ‚îú‚îÄ‚îÄ E2_transformer_generation.pkl\n",
    " ‚îÇ     ‚îî‚îÄ‚îÄ E2_transformer_generation_dl.h5\n",
    " ‚îî‚îÄ‚îÄ model_gcs_path: 'gs://pv_cropland/results/E2/models/'\n",
    "```\n",
    "\n",
    "\n",
    "ËæìÂá∫Êï∞ÊçÆÔºöÂåÖÊã¨ÂÆûÈ™åÁöÑË∂ÖÂèÇÊï∞Ë∞É‰ºò„ÄÅ‰ª•ÂèäÂÆûÈ™åÁöÑ‰∏ªË¶ÅÁ≤æÂ∫¶ÁªìÊûú\n",
    "Ôºà1ÔºâÈ¶ñÂÖàÊòØ‰∏ªË¶ÅË∂ÖÂèÇÊï∞ÁªìÊûúdataframe\n",
    "\n",
    " ‰ªøÁÖßÁ≤æÂ∫¶ dataframeÔºå‰∏ªË¶ÅÂÖ≥Ê≥® 6 ‰∏™Ë∂ÖÂèÇÊï∞Ôºöd_model, num_layers, learning_rate, dropout_rate, resnet_layers, resnet_width, _resnet_depth\n",
    " ```python\n",
    " Example:\n",
    " df_params = pd.DataFrame([{\n",
    "     'ÂÆûÈ™åÁºñÂè∑': e2_results.get('exp_id', None),\n",
    "     'd_model': e2_results.get('d_model', None),\n",
    "     'num_layers': e2_results.get('num_layers', None),\n",
    "     'learning_rate': e2_results.get('learning_rate', None),\n",
    "     'dropout_rate': e2_results.get('dropout_rate', None),\n",
    "     'resnet_layers': e2_results.get('resnet_layers', None),\n",
    "     '_resnet_width': e2_results.get('_resnet_width', None),\n",
    "     '_resnet_depth': e2_results.get('_resnet_depth', None)\n",
    " }])\n",
    " ```\n",
    "Ôºà2ÔºâÂÖ∂Ê¨°ÊòØ‰∏ªË¶ÅÁ≤æÂ∫¶ÁªìÊûúdataframe\n",
    "```python\n",
    "df = pd.DataFrame([{\n",
    "    'ÂÆûÈ™åÁºñÂè∑': e2_results.get('exp_id', None),\n",
    "    'overfitting_score': e2_results.get('overfitting_score', None),\n",
    "    'accuracy': e2_results.get('accuracy', None),\n",
    "    'precision': e2_results.get('precision', None),\n",
    "    'recall': e2_results.get('recall', None),\n",
    "    'f1': e2_results.get('f1', None),\n",
    "    'mean_probability': e2_results.get('mean_probability', None)\n",
    "}])\n",
    "```\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7920b234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 32,\n",
       " 'num_layers': 8,\n",
       " 'learning_rate': 0.001,\n",
       " 'dropout_rate': 0.3,\n",
       " 'resnet_layers': [128, 64, 32, 16, 8, 4],\n",
       " '_resnet_width': 128,\n",
       " '_resnet_depth': 6,\n",
       " 'num_heads': 2,\n",
       " 'overfitting_score': 0.01461287859752991,\n",
       " 'accuracy': 0.888474913065077,\n",
       " 'precision': 0.8754562043795621,\n",
       " 'recall': 0.9159904534606206,\n",
       " 'f1': 0.8952647539071612,\n",
       " 'mean_probability': '0.8077919',\n",
       " 'exp_id': 'E2',\n",
       " 'training_plot_path': 'results/E2/images/E2_training_results.png',\n",
       " 'pipeline_plot_path': 'results/E2/images/E2_pipeline_analysis.png',\n",
       " 'learning_curve_plot_path': 'results/E2/images/E2_learning_curve.png',\n",
       " 'model_path': '/output/models/E2_transformer_generation.pkl',\n",
       " 'uploaded_model_files': ['E2_transformer_generation_gmm.pkl',\n",
       "  'E2_transformer_generation_preprocessor.pkl',\n",
       "  'E2_transformer_generation_test_data.npz',\n",
       "  'E2_transformer_generation_config.json',\n",
       "  'E2_transformer_generation.pkl',\n",
       "  'E2_transformer_generation_dl.h5'],\n",
       " 'model_gcs_path': 'gs://pv_cropland/results/E2/models/'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÊåâÁÖßÁõ∏ÂØπÁõÆÂΩïÁªìÊûÑÊâìÂºÄÊñá‰ª∂ E2_results.json\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# ÊûÑÈÄ† E2_results.json ÁöÑÁõ∏ÂØπË∑ØÂæÑ (Supplymentary/ML_sensitivity/results/E2/E2_results.json)\n",
    "results_file = Path('Supplymentary/ML_sensitivity/results/E2/E2_results.json')\n",
    "\n",
    "# ËØªÂèñÊñá‰ª∂ÂÜÖÂÆπ\n",
    "with open(results_file, 'r', encoding='utf-8') as f:\n",
    "    e2_results = json.load(f)\n",
    "\n",
    "e2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d2f1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚úÖ Êï∞ÊçÆÊèêÂèñÂÆåÊàê\n",
      "================================================================================\n",
      "\n",
      "Ë∂ÖÂèÇÊï∞DataFrame (df_params):\n",
      "  - Ë°åÊï∞: 18\n",
      "  - ÂàóÊï∞: 8\n",
      "  - ÂàóÂêç: ['ÂÆûÈ™åÁºñÂè∑', 'd_model', 'num_layers', 'learning_rate', 'dropout_rate', 'resnet_layers', '_resnet_width', '_resnet_depth']\n",
      "\n",
      "Á≤æÂ∫¶ÁªìÊûúDataFrame (df_metrics):\n",
      "  - Ë°åÊï∞: 18\n",
      "  - ÂàóÊï∞: 7\n",
      "  - ÂàóÂêç: ['ÂÆûÈ™åÁºñÂè∑', 'overfitting_score', 'accuracy', 'precision', 'recall', 'f1', 'mean_probability']\n",
      "\n",
      "================================================================================\n",
      "Ë∂ÖÂèÇÊï∞DataFrameÈ¢ÑËßà:\n",
      "================================================================================\n",
      "   ÂÆûÈ™åÁºñÂè∑  d_model  num_layers  learning_rate  dropout_rate  \\\n",
      "0    E1       32           4         0.0001           0.1   \n",
      "1   E10       32           4         0.0010           0.3   \n",
      "2   E11       32           8         0.0100           0.5   \n",
      "3   E12       32          12         0.0001           0.1   \n",
      "4   E13       64           4         0.0001           0.5   \n",
      "5   E14       64           8         0.0010           0.1   \n",
      "6   E15       64          12         0.0100           0.3   \n",
      "7   E16      128           4         0.0010           0.1   \n",
      "8   E17      128           8         0.0100           0.3   \n",
      "9   E18      128          12         0.0001           0.5   \n",
      "10   E2       32           8         0.0010           0.3   \n",
      "11   E3       32          12         0.0100           0.5   \n",
      "12   E4       64           4         0.0010           0.5   \n",
      "13   E5       64           8         0.0100           0.1   \n",
      "14   E6       64          12         0.0001           0.3   \n",
      "15   E7      128           4         0.0100           0.3   \n",
      "16   E8      128           8         0.0001           0.5   \n",
      "17   E9      128          12         0.0010           0.1   \n",
      "\n",
      "                         resnet_layers  _resnet_width  _resnet_depth  \n",
      "0                         [64, 32, 16]             64              3  \n",
      "1   [256, 128, 64, 32, 16, 8, 4, 2, 1]            256              9  \n",
      "2                         [64, 32, 16]             64              3  \n",
      "3              [128, 64, 32, 16, 8, 4]            128              6  \n",
      "4     [128, 64, 32, 16, 8, 4, 2, 1, 0]            128              9  \n",
      "5                       [256, 128, 64]            256              3  \n",
      "6                [64, 32, 16, 8, 4, 2]             64              6  \n",
      "7            [256, 128, 64, 32, 16, 8]            256              6  \n",
      "8       [64, 32, 16, 8, 4, 2, 1, 0, 0]             64              9  \n",
      "9                        [128, 64, 32]            128              3  \n",
      "10             [128, 64, 32, 16, 8, 4]            128              6  \n",
      "11  [256, 128, 64, 32, 16, 8, 4, 2, 1]            256              9  \n",
      "12               [64, 32, 16, 8, 4, 2]             64              6  \n",
      "13    [128, 64, 32, 16, 8, 4, 2, 1, 0]            128              9  \n",
      "14                      [256, 128, 64]            256              3  \n",
      "15                       [128, 64, 32]            128              3  \n",
      "16           [256, 128, 64, 32, 16, 8]            256              6  \n",
      "17      [64, 32, 16, 8, 4, 2, 1, 0, 0]             64              9  \n",
      "\n",
      "================================================================================\n",
      "Á≤æÂ∫¶ÁªìÊûúDataFrameÈ¢ÑËßà:\n",
      "================================================================================\n",
      "   ÂÆûÈ™åÁºñÂè∑  overfitting_score  accuracy  precision    recall        f1  \\\n",
      "0    E1       9.509989e-03  0.844759   0.830783  0.881146  0.855224   \n",
      "1   E10       4.505275e-03  0.887978   0.900585  0.882100  0.891247   \n",
      "2   E11       1.064980e-02  0.895181   0.900048  0.898329  0.899188   \n",
      "3   E12       4.463680e-03  0.846746   0.819913  0.904057  0.859932   \n",
      "4   E13       4.091142e-03  0.707402   0.733096  0.688305  0.709995   \n",
      "5   E14       3.620968e-02  0.893443   0.883165  0.916468  0.899508   \n",
      "6   E15       1.277565e-02  0.870343   0.946115  0.796181  0.864697   \n",
      "7   E16       3.170876e-02  0.896175   0.889095  0.914558  0.901647   \n",
      "8   E17       2.195782e-08  0.520368   0.520368  1.000000  0.684529   \n",
      "9   E18       8.724783e-03  0.812221   0.755827  0.944153  0.839559   \n",
      "10   E2       1.461288e-02  0.888475   0.875456  0.915990  0.895265   \n",
      "11   E3       1.022934e-02  0.876304   0.936577  0.817661  0.873089   \n",
      "12   E4       4.147772e-03  0.900397   0.893222  0.918377  0.905625   \n",
      "13   E5      -5.372834e-05  0.520368   0.520368  1.000000  0.684529   \n",
      "14   E6       8.213654e-03  0.854446   0.824516  0.915036  0.867421   \n",
      "15   E7       1.025066e-02  0.888227   0.876086  0.914558  0.894909   \n",
      "16   E8      -2.784090e-03  0.751366   0.791889  0.708353  0.747795   \n",
      "17   E9      -6.214761e-04  0.520368   0.520368  1.000000  0.684529   \n",
      "\n",
      "   mean_probability  \n",
      "0         0.7433181  \n",
      "1         0.8309969  \n",
      "2        0.78290653  \n",
      "3        0.76893795  \n",
      "4        0.69130975  \n",
      "5          0.864741  \n",
      "6         0.6603419  \n",
      "7         0.8455377  \n",
      "8        0.53321844  \n",
      "9        0.79377943  \n",
      "10        0.8077919  \n",
      "11       0.71059215  \n",
      "12       0.86266416  \n",
      "13       0.53029054  \n",
      "14       0.77087706  \n",
      "15        0.8704279  \n",
      "16        0.6862237  \n",
      "17       0.53161913  \n"
     ]
    }
   ],
   "source": [
    "# ËØªÂèñÊâÄÊúâÂÆûÈ™åÁöÑJSONÊñá‰ª∂Âπ∂ÁîüÊàê‰∏§‰∏™DataFrame\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ÂÆö‰πâÁªìÊûúÁõÆÂΩïË∑ØÂæÑ\n",
    "results_base_dir = Path('Supplymentary/ML_sensitivity/results')\n",
    "\n",
    "# Â≠òÂÇ®ÊâÄÊúâÂÆûÈ™åÁªìÊûúÁöÑÂàóË°®\n",
    "all_params_list = []\n",
    "all_metrics_list = []\n",
    "\n",
    "# ÈÅçÂéÜE1Âà∞E18\n",
    "for exp_num in range(1, 19):\n",
    "    exp_id = f'E{exp_num}'\n",
    "    json_file = results_base_dir / exp_id / f'{exp_id}_results.json'\n",
    "    \n",
    "    if json_file.exists():\n",
    "        try:\n",
    "            # ËØªÂèñJSONÊñá‰ª∂\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                exp_results = json.load(f)\n",
    "            \n",
    "            # ÊèêÂèñË∂ÖÂèÇÊï∞Êï∞ÊçÆ\n",
    "            params_dict = {\n",
    "                'ÂÆûÈ™åÁºñÂè∑': exp_results.get('exp_id', exp_id),\n",
    "                'd_model': exp_results.get('d_model', None),\n",
    "                'num_layers': exp_results.get('num_layers', None),\n",
    "                'learning_rate': exp_results.get('learning_rate', None),\n",
    "                'dropout_rate': exp_results.get('dropout_rate', None),\n",
    "                'resnet_layers': exp_results.get('resnet_layers', None),\n",
    "                '_resnet_width': exp_results.get('_resnet_width', None),\n",
    "                '_resnet_depth': exp_results.get('_resnet_depth', None)\n",
    "            }\n",
    "            all_params_list.append(params_dict)\n",
    "            \n",
    "            # ÊèêÂèñÁ≤æÂ∫¶ÁªìÊûúÊï∞ÊçÆ\n",
    "            metrics_dict = {\n",
    "                'ÂÆûÈ™åÁºñÂè∑': exp_results.get('exp_id', exp_id),\n",
    "                'overfitting_score': exp_results.get('overfitting_score', None),\n",
    "                'accuracy': exp_results.get('accuracy', None),\n",
    "                'precision': exp_results.get('precision', None),\n",
    "                'recall': exp_results.get('recall', None),\n",
    "                'f1': exp_results.get('f1', None),\n",
    "                'mean_probability': exp_results.get('mean_probability', None)\n",
    "            }\n",
    "            all_metrics_list.append(metrics_dict)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è ËØªÂèñ {exp_id} Êó∂Âá∫Èîô: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Êñá‰ª∂‰∏çÂ≠òÂú®: {json_file}\")\n",
    "\n",
    "# ÁîüÊàêË∂ÖÂèÇÊï∞DataFrame\n",
    "df_params = pd.DataFrame(all_params_list)\n",
    "\n",
    "# ÁîüÊàêÁ≤æÂ∫¶ÁªìÊûúDataFrame\n",
    "df_metrics = pd.DataFrame(all_metrics_list)\n",
    "\n",
    "# ÊåâÂÆûÈ™åÁºñÂè∑ÊéíÂ∫èÔºàÁ°Æ‰øùE1Âà∞E18ÁöÑÈ°∫Â∫èÔºâ\n",
    "df_params = df_params.sort_values('ÂÆûÈ™åÁºñÂè∑').reset_index(drop=True)\n",
    "df_metrics = df_metrics.sort_values('ÂÆûÈ™åÁºñÂè∑').reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab197f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÊåâÁÖßÊåáÂÆöE1-E18È°∫Â∫èÊéíÂ∫è\n",
    "exp_id_order = [f\"E{i}\" for i in range(1, 19)]\n",
    "df_params['ÂÆûÈ™åÁºñÂè∑'] = pd.Categorical(df_params['ÂÆûÈ™åÁºñÂè∑'], categories=exp_id_order, ordered=True)\n",
    "df_params = df_params.sort_values('ÂÆûÈ™åÁºñÂè∑').reset_index(drop=True)\n",
    "df_params.to_csv(f\"{results_base_dir}/hyparameters.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f296bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÊåâÁÖßÊåáÂÆöE1-E18È°∫Â∫èÊéíÂ∫è\n",
    "exp_id_order = [f\"E{i}\" for i in range(1, 19)]\n",
    "df_metrics['ÂÆûÈ™åÁºñÂè∑'] = pd.Categorical(df_metrics['ÂÆûÈ™åÁºñÂè∑'], categories=exp_id_order, ordered=True)\n",
    "df_metrics = df_metrics.sort_values('ÂÆûÈ™åÁºñÂè∑').reset_index(drop=True)\n",
    "df_metrics.to_csv(f\"{results_base_dir}/metrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8de90f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ÂÆûÈ™åÁºñÂè∑</th>\n",
       "      <th>overfitting_score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>mean_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E1</td>\n",
       "      <td>9.509989e-03</td>\n",
       "      <td>0.844759</td>\n",
       "      <td>0.830783</td>\n",
       "      <td>0.881146</td>\n",
       "      <td>0.855224</td>\n",
       "      <td>0.7433181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E2</td>\n",
       "      <td>1.461288e-02</td>\n",
       "      <td>0.888475</td>\n",
       "      <td>0.875456</td>\n",
       "      <td>0.915990</td>\n",
       "      <td>0.895265</td>\n",
       "      <td>0.8077919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E3</td>\n",
       "      <td>1.022934e-02</td>\n",
       "      <td>0.876304</td>\n",
       "      <td>0.936577</td>\n",
       "      <td>0.817661</td>\n",
       "      <td>0.873089</td>\n",
       "      <td>0.71059215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E4</td>\n",
       "      <td>4.147772e-03</td>\n",
       "      <td>0.900397</td>\n",
       "      <td>0.893222</td>\n",
       "      <td>0.918377</td>\n",
       "      <td>0.905625</td>\n",
       "      <td>0.86266416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E5</td>\n",
       "      <td>-5.372834e-05</td>\n",
       "      <td>0.520368</td>\n",
       "      <td>0.520368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.684529</td>\n",
       "      <td>0.53029054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E6</td>\n",
       "      <td>8.213654e-03</td>\n",
       "      <td>0.854446</td>\n",
       "      <td>0.824516</td>\n",
       "      <td>0.915036</td>\n",
       "      <td>0.867421</td>\n",
       "      <td>0.77087706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E7</td>\n",
       "      <td>1.025066e-02</td>\n",
       "      <td>0.888227</td>\n",
       "      <td>0.876086</td>\n",
       "      <td>0.914558</td>\n",
       "      <td>0.894909</td>\n",
       "      <td>0.8704279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E8</td>\n",
       "      <td>-2.784090e-03</td>\n",
       "      <td>0.751366</td>\n",
       "      <td>0.791889</td>\n",
       "      <td>0.708353</td>\n",
       "      <td>0.747795</td>\n",
       "      <td>0.6862237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E9</td>\n",
       "      <td>-6.214761e-04</td>\n",
       "      <td>0.520368</td>\n",
       "      <td>0.520368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.684529</td>\n",
       "      <td>0.53161913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>E10</td>\n",
       "      <td>4.505275e-03</td>\n",
       "      <td>0.887978</td>\n",
       "      <td>0.900585</td>\n",
       "      <td>0.882100</td>\n",
       "      <td>0.891247</td>\n",
       "      <td>0.8309969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>E11</td>\n",
       "      <td>1.064980e-02</td>\n",
       "      <td>0.895181</td>\n",
       "      <td>0.900048</td>\n",
       "      <td>0.898329</td>\n",
       "      <td>0.899188</td>\n",
       "      <td>0.78290653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>E12</td>\n",
       "      <td>4.463680e-03</td>\n",
       "      <td>0.846746</td>\n",
       "      <td>0.819913</td>\n",
       "      <td>0.904057</td>\n",
       "      <td>0.859932</td>\n",
       "      <td>0.76893795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>E13</td>\n",
       "      <td>4.091142e-03</td>\n",
       "      <td>0.707402</td>\n",
       "      <td>0.733096</td>\n",
       "      <td>0.688305</td>\n",
       "      <td>0.709995</td>\n",
       "      <td>0.69130975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>E14</td>\n",
       "      <td>3.620968e-02</td>\n",
       "      <td>0.893443</td>\n",
       "      <td>0.883165</td>\n",
       "      <td>0.916468</td>\n",
       "      <td>0.899508</td>\n",
       "      <td>0.864741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>E15</td>\n",
       "      <td>1.277565e-02</td>\n",
       "      <td>0.870343</td>\n",
       "      <td>0.946115</td>\n",
       "      <td>0.796181</td>\n",
       "      <td>0.864697</td>\n",
       "      <td>0.6603419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>E16</td>\n",
       "      <td>3.170876e-02</td>\n",
       "      <td>0.896175</td>\n",
       "      <td>0.889095</td>\n",
       "      <td>0.914558</td>\n",
       "      <td>0.901647</td>\n",
       "      <td>0.8455377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>E17</td>\n",
       "      <td>2.195782e-08</td>\n",
       "      <td>0.520368</td>\n",
       "      <td>0.520368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.684529</td>\n",
       "      <td>0.53321844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>E18</td>\n",
       "      <td>8.724783e-03</td>\n",
       "      <td>0.812221</td>\n",
       "      <td>0.755827</td>\n",
       "      <td>0.944153</td>\n",
       "      <td>0.839559</td>\n",
       "      <td>0.79377943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ÂÆûÈ™åÁºñÂè∑  overfitting_score  accuracy  precision    recall        f1  \\\n",
       "0    E1       9.509989e-03  0.844759   0.830783  0.881146  0.855224   \n",
       "1    E2       1.461288e-02  0.888475   0.875456  0.915990  0.895265   \n",
       "2    E3       1.022934e-02  0.876304   0.936577  0.817661  0.873089   \n",
       "3    E4       4.147772e-03  0.900397   0.893222  0.918377  0.905625   \n",
       "4    E5      -5.372834e-05  0.520368   0.520368  1.000000  0.684529   \n",
       "5    E6       8.213654e-03  0.854446   0.824516  0.915036  0.867421   \n",
       "6    E7       1.025066e-02  0.888227   0.876086  0.914558  0.894909   \n",
       "7    E8      -2.784090e-03  0.751366   0.791889  0.708353  0.747795   \n",
       "8    E9      -6.214761e-04  0.520368   0.520368  1.000000  0.684529   \n",
       "9   E10       4.505275e-03  0.887978   0.900585  0.882100  0.891247   \n",
       "10  E11       1.064980e-02  0.895181   0.900048  0.898329  0.899188   \n",
       "11  E12       4.463680e-03  0.846746   0.819913  0.904057  0.859932   \n",
       "12  E13       4.091142e-03  0.707402   0.733096  0.688305  0.709995   \n",
       "13  E14       3.620968e-02  0.893443   0.883165  0.916468  0.899508   \n",
       "14  E15       1.277565e-02  0.870343   0.946115  0.796181  0.864697   \n",
       "15  E16       3.170876e-02  0.896175   0.889095  0.914558  0.901647   \n",
       "16  E17       2.195782e-08  0.520368   0.520368  1.000000  0.684529   \n",
       "17  E18       8.724783e-03  0.812221   0.755827  0.944153  0.839559   \n",
       "\n",
       "   mean_probability  \n",
       "0         0.7433181  \n",
       "1         0.8077919  \n",
       "2        0.71059215  \n",
       "3        0.86266416  \n",
       "4        0.53029054  \n",
       "5        0.77087706  \n",
       "6         0.8704279  \n",
       "7         0.6862237  \n",
       "8        0.53161913  \n",
       "9         0.8309969  \n",
       "10       0.78290653  \n",
       "11       0.76893795  \n",
       "12       0.69130975  \n",
       "13         0.864741  \n",
       "14        0.6603419  \n",
       "15        0.8455377  \n",
       "16       0.53321844  \n",
       "17       0.79377943  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "884074bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊâæÂà∞ 4 ‰∏™Ê®°ÂûãÈÖçÁΩÆÊñá‰ª∂\n",
      "================================================================================\n",
      "‚úÖ Êï∞ÊçÆÊèêÂèñÂÆåÊàê\n",
      "================================================================================\n",
      "\n",
      "Ë∂ÖÂèÇÊï∞DataFrame (df_params):\n",
      "  - Ë°åÊï∞: 4\n",
      "  - ÂàóÊï∞: 13\n",
      "  - ÂàóÂêç: ['ÂÆûÈ™åÁºñÂè∑', 'model_type', 'd_model', 'num_layers', 'num_heads', 'learning_rate', 'dropout_rate', 'resnet_layers', '_resnet_width', '_resnet_depth', 'epochs', 'batch_size', 'timestamp']\n",
      "\n",
      "Á≤æÂ∫¶ÁªìÊûúDataFrame (df_metrics):\n",
      "  - Ë°åÊï∞: 4\n",
      "  - ÂàóÊï∞: 10\n",
      "  - ÂàóÂêç: ['ÂÆûÈ™åÁºñÂè∑', 'model_type', 'overfitting_score', 'accuracy', 'precision', 'recall', 'f1', 'auc', 'mean_probability', 'timestamp']\n",
      "\n",
      "================================================================================\n",
      "Ë∂ÖÂèÇÊï∞DataFrameÈ¢ÑËßà:\n",
      "================================================================================\n",
      "                 ÂÆûÈ™åÁºñÂè∑   model_type  d_model num_layers num_heads  \\\n",
      "0          mlp_144212          mlp    128.0       None      None   \n",
      "1           rf_212051           rf      NaN       None      None   \n",
      "2  transformer_212041  transformer     64.0       None      None   \n",
      "3  transformer_234807  transformer     64.0       None      None   \n",
      "\n",
      "   learning_rate  dropout_rate resnet_layers _resnet_width _resnet_depth  \\\n",
      "0          0.001           0.3          None          None          None   \n",
      "1          0.001           0.3          None          None          None   \n",
      "2          0.001           0.3          None          None          None   \n",
      "3          0.001           0.3          None          None          None   \n",
      "\n",
      "   epochs  batch_size timestamp  \n",
      "0      80         256    144212  \n",
      "1      50         256    212051  \n",
      "2      80         256    212041  \n",
      "3      80         256    234807  \n",
      "\n",
      "================================================================================\n",
      "Á≤æÂ∫¶ÁªìÊûúDataFrameÈ¢ÑËßà:\n",
      "================================================================================\n",
      "                 ÂÆûÈ™åÁºñÂè∑   model_type  overfitting_score  accuracy  precision  \\\n",
      "0          mlp_144212          mlp           0.010042  0.890710   0.883991   \n",
      "1           rf_212051           rf                NaN  0.986339   0.985252   \n",
      "2  transformer_212041  transformer           0.022779  0.923994   0.920545   \n",
      "3  transformer_234807  transformer           0.023694  0.891704   0.878594   \n",
      "\n",
      "     recall        f1       auc mean_probability timestamp  \n",
      "0  0.909308  0.896471  0.959599             None    144212  \n",
      "1  0.988544  0.986895  0.998856             None    212051  \n",
      "2  0.934606  0.927523  0.974490             None    212041  \n",
      "3  0.918854  0.898273  0.960646             None    234807  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# ÂÆö‰πâÊ®°ÂûãÁõÆÂΩïË∑ØÂæÑ\n",
    "model_base_dir = DATA_PATH / 'US_data/ML_model'\n",
    "\n",
    "# Â≠òÂÇ®ÊâÄÊúâÊ®°ÂûãÁªìÊûúÁöÑÂàóË°®\n",
    "all_params_list = []\n",
    "all_metrics_list = []\n",
    "\n",
    "# Êü•ÊâæÊâÄÊúâ *_config.json Êñá‰ª∂\n",
    "config_files = glob.glob(str(model_base_dir / '*_config.json'))\n",
    "\n",
    "print(f\"ÊâæÂà∞ {len(config_files)} ‰∏™Ê®°ÂûãÈÖçÁΩÆÊñá‰ª∂\")\n",
    "\n",
    "for config_file in sorted(config_files):\n",
    "    try:\n",
    "        # ËØªÂèñÈÖçÁΩÆÊñá‰ª∂\n",
    "        with open(config_file, 'r', encoding='utf-8') as f:\n",
    "            config_data = json.load(f)\n",
    "        \n",
    "        # ‰ªéÊñá‰ª∂ÂêçÊèêÂèñÊ®°Âûã‰ø°ÊÅØ\n",
    "        file_name = Path(config_file).stem\n",
    "        # Ê†ºÂºè: landuse_transformer_generation_single_20251124_234807_config\n",
    "        parts = file_name.replace('_config', '').split('_')\n",
    "        model_type = parts[1] if len(parts) > 1 else 'unknown'\n",
    "        timestamp = parts[-1] if len(parts) > 1 else 'unknown'\n",
    "        exp_id = f\"{model_type}_{timestamp}\"\n",
    "        \n",
    "        # ÊèêÂèñË∂ÖÂèÇÊï∞\n",
    "        model_config = config_data.get('model_config', {})\n",
    "        model_params = model_config.get('model_params', {})\n",
    "        \n",
    "        # ÊèêÂèñ transformer Áõ∏ÂÖ≥ÂèÇÊï∞ÔºàÂ¶ÇÊûúÂ≠òÂú®Ôºâ\n",
    "        transformer_config = model_config.get('transformer_config', {})\n",
    "        resnet_layers = model_config.get('resnet_layers', model_params.get('resnet_layers'))\n",
    "        \n",
    "        # ‰ªé dl_architecture Êé®Êñ≠ÂèÇÊï∞ÔºàÂ¶ÇÊûúÂèØËÉΩÔºâ\n",
    "        dl_arch = config_data.get('dl_architecture', {})\n",
    "        d_model = None\n",
    "        num_layers = None\n",
    "        num_heads = None\n",
    "        \n",
    "        # Â∞ùËØï‰ªé layers ‰∏≠Êé®Êñ≠ transformer ÂèÇÊï∞\n",
    "        if dl_arch and 'layers' in dl_arch:\n",
    "            layers = dl_arch['layers']\n",
    "            # Êü•Êâæ transformer Áõ∏ÂÖ≥Â±Ç\n",
    "            for layer in layers:\n",
    "                if isinstance(layer, dict):\n",
    "                    layer_name = layer.get('name', '')\n",
    "                    if 'dense' in layer_name.lower() and 'units' in layer:\n",
    "                        # Á¨¨‰∏Ä‰∏™ dense Â±ÇÁöÑ units ÂèØËÉΩÊòØ d_model\n",
    "                        if d_model is None and layer.get('units') in [32, 64, 128]:\n",
    "                            d_model = layer.get('units')\n",
    "                    if 'multi_head_attention' in layer_name.lower() or 'attention' in layer_name.lower():\n",
    "                        if 'num_heads' in layer:\n",
    "                            num_heads = layer.get('num_heads')\n",
    "        \n",
    "        # Â¶ÇÊûúÊ≤°ÊúâÊâæÂà∞Ôºå‰ΩøÁî®ÈªòËÆ§ÂÄºÊàñ‰ªé transformer_config Ëé∑Âèñ\n",
    "        if transformer_config:\n",
    "            d_model = transformer_config.get('d_model', d_model)\n",
    "            num_layers = transformer_config.get('num_layers', num_layers)\n",
    "            num_heads = transformer_config.get('num_heads', num_heads)\n",
    "        \n",
    "        # ÊèêÂèñ ResNet ÂèÇÊï∞\n",
    "        if isinstance(resnet_layers, list) and len(resnet_layers) > 0:\n",
    "            _resnet_width = resnet_layers[0] if resnet_layers else None\n",
    "            _resnet_depth = len(resnet_layers)\n",
    "        else:\n",
    "            _resnet_width = None\n",
    "            _resnet_depth = None\n",
    "        \n",
    "        # ÊûÑÂª∫Ë∂ÖÂèÇÊï∞Â≠óÂÖ∏\n",
    "        params_dict = {\n",
    "            'ÂÆûÈ™åÁºñÂè∑': exp_id,\n",
    "            'model_type': model_type,\n",
    "            'd_model': d_model,\n",
    "            'num_layers': num_layers,\n",
    "            'num_heads': num_heads,\n",
    "            'learning_rate': model_params.get('learning_rate'),\n",
    "            'dropout_rate': model_params.get('dropout_rate'),\n",
    "            'resnet_layers': resnet_layers,\n",
    "            '_resnet_width': _resnet_width,\n",
    "            '_resnet_depth': _resnet_depth,\n",
    "            'epochs': model_params.get('epochs'),\n",
    "            'batch_size': model_params.get('batch_size'),\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        all_params_list.append(params_dict)\n",
    "        \n",
    "        # ÊèêÂèñÁ≤æÂ∫¶ÁªìÊûúÊï∞ÊçÆ\n",
    "        training_metrics = config_data.get('training_metrics', {})\n",
    "        test_metrics = training_metrics.get('test', {})\n",
    "        \n",
    "        # ËÆ°ÁÆó overfitting_scoreÔºàÂ¶ÇÊûúÊúâÂ≠¶‰π†Êõ≤Á∫øÂàÜÊûêÔºâ\n",
    "        overfitting_score = None\n",
    "        learning_curve_analysis = config_data.get('learning_curve_analysis', {})\n",
    "        if learning_curve_analysis:\n",
    "            overfitting_analysis = learning_curve_analysis.get('overfitting_analysis', {})\n",
    "            overfitting_score = overfitting_analysis.get('final_gap')\n",
    "        \n",
    "        # ËÆ°ÁÆó mean_probabilityÔºà‰ªé prediction_results Êàñ final_resultsÔºâ\n",
    "        mean_probability = None\n",
    "        # Ê≥®ÊÑèÔºömean_probability ÂèØËÉΩ‰∏çÂú® config.json ‰∏≠ÔºåÈúÄË¶Å‰ªéÂÖ∂‰ªñÊñá‰ª∂ËØªÂèñ\n",
    "        \n",
    "        metrics_dict = {\n",
    "            'ÂÆûÈ™åÁºñÂè∑': exp_id,\n",
    "            'model_type': model_type,\n",
    "            'overfitting_score': overfitting_score,\n",
    "            'accuracy': test_metrics.get('accuracy'),\n",
    "            'precision': test_metrics.get('precision'),\n",
    "            'recall': test_metrics.get('recall'),\n",
    "            'f1': test_metrics.get('f1'),\n",
    "            'auc': test_metrics.get('auc'),\n",
    "            'mean_probability': mean_probability,\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        all_metrics_list.append(metrics_dict)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è ËØªÂèñ {config_file} Êó∂Âá∫Èîô: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ÁîüÊàêË∂ÖÂèÇÊï∞DataFrame\n",
    "df_params = pd.DataFrame(all_params_list)\n",
    "\n",
    "# ÁîüÊàêÁ≤æÂ∫¶ÁªìÊûúDataFrame\n",
    "df_metrics = pd.DataFrame(all_metrics_list)\n",
    "\n",
    "# ÊåâÂÆûÈ™åÁºñÂè∑ÊéíÂ∫è\n",
    "if not df_params.empty:\n",
    "    df_params = df_params.sort_values('ÂÆûÈ™åÁºñÂè∑').reset_index(drop=True)\n",
    "if not df_metrics.empty:\n",
    "    df_metrics = df_metrics.sort_values('ÂÆûÈ™åÁºñÂè∑').reset_index(drop=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ Êï∞ÊçÆÊèêÂèñÂÆåÊàê\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nË∂ÖÂèÇÊï∞DataFrame (df_params):\")\n",
    "print(f\"  - Ë°åÊï∞: {len(df_params)}\")\n",
    "if not df_params.empty:\n",
    "    print(f\"  - ÂàóÊï∞: {len(df_params.columns)}\")\n",
    "    print(f\"  - ÂàóÂêç: {list(df_params.columns)}\")\n",
    "\n",
    "print(f\"\\nÁ≤æÂ∫¶ÁªìÊûúDataFrame (df_metrics):\")\n",
    "print(f\"  - Ë°åÊï∞: {len(df_metrics)}\")\n",
    "if not df_metrics.empty:\n",
    "    print(f\"  - ÂàóÊï∞: {len(df_metrics.columns)}\")\n",
    "    print(f\"  - ÂàóÂêç: {list(df_metrics.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Ë∂ÖÂèÇÊï∞DataFrameÈ¢ÑËßà:\")\n",
    "print(\"=\" * 80)\n",
    "if not df_params.empty:\n",
    "    print(df_params)\n",
    "else:\n",
    "    print(\"Êó†Êï∞ÊçÆ\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Á≤æÂ∫¶ÁªìÊûúDataFrameÈ¢ÑËßà:\")\n",
    "print(\"=\" * 80)\n",
    "if not df_metrics.empty:\n",
    "    print(df_metrics)\n",
    "else:\n",
    "    print(\"Êó†Êï∞ÊçÆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "378c662d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8403101414085618"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df_main_dir = DATA_PATH / 'US_data/ML_model/results_transformer_20251124_234807.csv'\n",
    "df_main = pd.read_csv(df_main_dir)\n",
    "df_main_notnull = df_main[df_main['lat'].notna() & df_main['lon'].notna()]\n",
    "df_main_notnull['predicted_prob'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd318393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
