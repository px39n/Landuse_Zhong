{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b678cf",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf082678",
   "metadata": {},
   "source": [
    "\n",
    "- æ ¸å¿ƒç›®çš„\n",
    "è¿›è¡Œè¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æžå¹¶ç”Ÿæˆå¤šä¸ªdataframeã€‚è®°å½•åœ¨ä¸åŒè¶…å‚æ•°ç»„åˆä¸‹çš„ï¼š\n",
    "- è¿‡æ‹Ÿåˆscore\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- æœ€ç»ˆé¢„æµ‹è¾“å‡ºçš„mean probability\n",
    "\n",
    "- ä¸šåŠ¡é€»è¾‘\n",
    "1. è®¾è®¡æ­£äº¤è¡¨æ•æ„Ÿæ€§åˆ†æžå®žéªŒï¼ˆL18(3^6)æ­£äº¤è¡¨ï¼Œå°†729æ¬¡å®žéªŒå‡å°‘åˆ°18æ¬¡ï¼‰\n",
    "2. ä½¿ç”¨æ¨¡å—åŒ–çš„è®­ç»ƒå‡½æ•°ï¼ˆå·²å­˜å‚¨åœ¨functionç›®å½•ä¸‹ï¼‰\n",
    "3. é’ˆå¯¹ä¸åŒå‚æ•°ç»„åˆæ‰§è¡Œæ•æ„Ÿæ€§åˆ†æž\n",
    "4. è¾“å‡ºç›®å½•ï¼š`Supplymentary/ML_sensitivity`\n",
    "\n",
    "- è¶…å‚æ•°é…ç½®\n",
    "- **åŸºç¡€é…ç½®**ï¼šlearning_rate=0.001, resnet_layers=[128,128,64], d_model=64, num_heads=4, num_layers=2, dropout_rate=0.3\n",
    "- **æ•æ„Ÿæ€§æµ‹è¯•å‚æ•°**ï¼šæ¯ä¸ªå‚æ•°3ä¸ªæ°´å¹³ï¼Œå…±6ä¸ªå‚æ•°\n",
    "- **ä¸è¿›è¡Œæ•æ„Ÿæ€§æµ‹è¯•**ï¼šbatch_size, epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20859aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GPUçŽ¯å¢ƒé…ç½®æ£€æŸ¥ - WSL Ubuntu + bayes-gpu\n",
      "================================================================================\n",
      "TensorFlow ç‰ˆæœ¬: 2.19.0\n",
      "CUDA build: None\n",
      "cuDNN build: None\n",
      "\n",
      "âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPU\n",
      "\n",
      "================================================================================\n",
      "GPUæ€§èƒ½æµ‹è¯•\n",
      "================================================================================\n",
      "âœ… è®¾å¤‡: /CPU:0\n",
      "âœ… çŸ©é˜µä¹˜æ³• 2000x2000 è€—æ—¶: 39.61 ms\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== çŽ¯å¢ƒé…ç½® ==========\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. è®¾ç½®çŽ¯å¢ƒå˜é‡\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# 2. ç›´æŽ¥é‡å®šå‘stderrï¼ˆä¸´æ—¶ï¼‰\n",
    "_old_stderr = sys.stderr\n",
    "sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "# 3. å¯¼å…¥matplotlibå’Œtensorflow\n",
    "try:\n",
    "    import matplotlib\n",
    "    import matplotlib.font_manager as fm\n",
    "    matplotlib.rcParams['font.family'] = 'DejaVu Sans'\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "finally:\n",
    "    # 4. æ¢å¤stderr\n",
    "    sys.stderr.close()\n",
    "    sys.stderr = _old_stderr\n",
    "\n",
    "# 5. è®¾ç½®è­¦å‘Šè¿‡æ»¤\n",
    "import sklearn\n",
    "warnings.filterwarnings('ignore', category=sklearn.base.InconsistentVersionWarning)\n",
    "warnings.filterwarnings('ignore', module='matplotlib')\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GPUçŽ¯å¢ƒé…ç½®æ£€æŸ¥ - WSL Ubuntu + bayes-gpu\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ---------------------------\n",
    "# 0. åŸºæœ¬ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰\n",
    "# ---------------------------\n",
    "print(f\"TensorFlow ç‰ˆæœ¬: {tf.__version__}\")\n",
    "# å°è¯•æ‰“å°æž„å»ºæ—¶ CUDA/cuDNN ç‰ˆæœ¬ï¼ˆä¸åŒç‰ˆæœ¬å­—æ®µåå¯èƒ½ä¸åŒï¼‰\n",
    "try:\n",
    "    from tensorflow.sysconfig import get_build_info\n",
    "    bi = get_build_info()\n",
    "    print(\"CUDA build:\", bi.get(\"cuda_version\"))\n",
    "    print(\"cuDNN build:\", bi.get(\"cudnn_version\"))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------------------------\n",
    "# 1. æ£€æŸ¥å¹¶é…ç½® GPU\n",
    "# ---------------------------\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    print(f\"\\nâœ… æ£€æµ‹åˆ° {len(physical_gpus)} ä¸ª GPU:\")\n",
    "    # å…ˆé…ç½®æ˜¾å­˜ç­–ç•¥ï¼ˆå¿…é¡»åœ¨ä»»ä½• GPU åˆå§‹åŒ–ä¹‹å‰ï¼‰\n",
    "    try:\n",
    "        for g in physical_gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "        print(\"âœ… GPU æ˜¾å­˜åŠ¨æ€å¢žé•¿å·²å¯ç”¨\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ æ˜¾å­˜åŠ¨æ€å¢žé•¿è®¾ç½®å¤±è´¥ï¼ˆå¯èƒ½å·²åˆå§‹åŒ–ï¼‰ï¼š{e}\")\n",
    "\n",
    "    # æ‰“å°è®¾å¤‡ç»†èŠ‚\n",
    "    for i, g in enumerate(physical_gpus):\n",
    "        print(f\"   GPU {i}: {g.name}\")\n",
    "        try:\n",
    "            det = tf.config.experimental.get_device_details(g)\n",
    "            # å¸¸è§é”®ï¼š'device_name', 'compute_capability'\n",
    "            print(f\"       è®¾å¤‡è¯¦æƒ…: {det}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "else:\n",
    "    print(\"\\nâš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPU\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. å°æµ‹è¯•ï¼šGPU è®¡ç®—ä¸Žè®¡æ—¶\n",
    "# ---------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GPUæ€§èƒ½æµ‹è¯•\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "use_gpu = len(physical_gpus) > 0\n",
    "device_str = \"/GPU:0\" if use_gpu else \"/CPU:0\"\n",
    "\n",
    "# æš–æœºï¼ˆé¿å…ç¬¬ä¸€æ¬¡è°ƒç”¨çš„åŠ è½½å¼€é”€å½±å“è®¡æ—¶ï¼‰\n",
    "with tf.device(device_str):\n",
    "    _ = tf.matmul(tf.random.normal([256, 256]), tf.random.normal([256, 256]))  # warm-up\n",
    "    _ = tf.matmul(tf.random.normal([256, 256]), tf.random.normal([256, 256]))  # warm-up\n",
    "\n",
    "# è®¡æ—¶æµ‹è¯•ï¼ˆç¡®ä¿åŒæ­¥ï¼‰\n",
    "size = 2000  # å¦‚éœ€æ›´æ˜Žæ˜¾å·®å¼‚å¯è°ƒå¤§åˆ° 4096\n",
    "with tf.device(device_str):\n",
    "    a = tf.random.normal([size, size])\n",
    "    b = tf.random.normal([size, size])\n",
    "    t0 = time.time()\n",
    "    c = tf.matmul(a, b)\n",
    "    _ = c.numpy()  # åŒæ­¥åˆ°ä¸»æœºï¼Œç¡®ä¿è®¡æ—¶çœŸå®ž\n",
    "    elapsed_ms = (time.time() - t0) * 1000\n",
    "\n",
    "print(f\"âœ… è®¾å¤‡: {device_str}\")\n",
    "print(f\"âœ… çŸ©é˜µä¹˜æ³• {size}x{size} è€—æ—¶: {elapsed_ms:.2f} ms\")\n",
    "if use_gpu:\n",
    "    print(\"ðŸš€ GPU åŠ é€Ÿæ­£å¸¸ï¼ˆä¸Ž CPU ç›¸æ¯”åº”æ˜Žæ˜¾æ›´å¿«ï¼‰\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50aa111",
   "metadata": {},
   "source": [
    "## 1ã€Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed049a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¡¹ç›®æ ¹ç›®å½•: /mnt/c/Dev/Landuse_Zhong_clean\n",
      "æ•°æ®è·¯å¾„: /mnt/c/Dev/Landuse_Zhong_clean/data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# æœ€å¯é çš„æ–¹æ³•ï¼šæŸ¥æ‰¾åŒ…å«dataå’Œfunctionç›®å½•çš„é¡¹ç›®æ ¹ç›®å½•\n",
    "def find_project_root(start_path=None):\n",
    "    \"\"\"æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•ï¼ˆåŒ…å«dataå’Œfunctionç›®å½•çš„ç›®å½•ï¼‰\"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = Path.cwd()\n",
    "    \n",
    "    current = Path(start_path).resolve()\n",
    "    \n",
    "    # å‘ä¸ŠæŸ¥æ‰¾ï¼Œç›´åˆ°æ‰¾åˆ°åŒ…å«dataå’Œfunctionç›®å½•çš„ç›®å½•\n",
    "    for _ in range(5):  # æœ€å¤šå‘ä¸ŠæŸ¥æ‰¾5å±‚\n",
    "        if (current / 'data').exists() and (current / 'function').exists():\n",
    "            return current\n",
    "        parent = current.parent\n",
    "        if parent == current:  # åˆ°è¾¾æ ¹ç›®å½•\n",
    "            break\n",
    "        current = parent\n",
    "    \n",
    "    # å¦‚æžœæ‰¾ä¸åˆ°ï¼Œå‡è®¾å½“å‰ç›®å½•çš„çˆ¶ç›®å½•æ˜¯é¡¹ç›®æ ¹ç›®å½•\n",
    "    return Path.cwd().parent\n",
    "\n",
    "project_root = find_project_root()\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "DATA_PATH = project_root / 'data'\n",
    "\n",
    "print(f\"é¡¹ç›®æ ¹ç›®å½•: {project_root}\")\n",
    "print(f\"æ•°æ®è·¯å¾„: {DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a7cadea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gogogo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764019411.733590  147604 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "I0000 00:00:1764019411.878892  147604 cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764019413.225357  147604 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] TensorFlow available\n",
      "[OK] scikeras available\n",
      "[OK] SHAP available\n",
      "us_abandon path: /mnt/c/Dev/Landuse_Zhong_clean/data/us_abandon_clean.csv\n",
      "åŠ è½½æ•°æ®...\n",
      "You want to predict the year: 2020.0\n",
      "You want to predict the year: 2020.0\n",
      "é¢„å¤„ç†æ•°æ®...\n",
      "åˆ— GDPpc æ²¡æœ‰éœ€è¦å¡«å……çš„éžæ­£å€¼æˆ–NaNå€¼\n",
      "åˆ— GDPpc æ²¡æœ‰éœ€è¦å¡«å……çš„éžæ­£å€¼æˆ–NaNå€¼\n",
      "âœ… æ•°æ®åŠ è½½å®Œæˆ\n",
      "  - df_embedding_fill: 10473 è¡Œ\n",
      "  - df_abandon_filtered: 70337 è¡Œ\n",
      "  - features_no_coords: 15 ä¸ªç‰¹å¾\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from function import (\n",
    "    load_embedding, load_abandon, fill_nonpositive_with_nearest, \n",
    "    filter_duplicates, NUMERIC_FEATURES, CAT_COLS, PATHS\n",
    ")\n",
    "from function.pipeline import run_correct_training_pipeline\n",
    "from function.model_saving import save_complete_model_pipeline\n",
    "from datetime import datetime\n",
    "import platform\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def normalize_path(path):\n",
    "    \"\"\"\n",
    "    è·¨å¹³å°è·¯å¾„è§„èŒƒåŒ–\n",
    "    å°†Windowsè·¯å¾„ä¸­çš„åæ–œæ è½¬æ¢ä¸ºæ­£æ–œæ ï¼ˆåœ¨Linux/WSLä¸­ï¼‰\n",
    "    Windows: r'data\\file.shp' -> r'data\\file.shp' (ä¿æŒä¸å˜)\n",
    "    Linux/WSL: r'data\\file.shp' -> 'data/file.shp' (è½¬æ¢)\n",
    "    \"\"\"\n",
    "    if isinstance(path, str):\n",
    "        # æ£€æµ‹æ˜¯å¦åœ¨WSLæˆ–LinuxçŽ¯å¢ƒä¸­\n",
    "        is_linux = platform.system() in ['Linux', 'Darwin']\n",
    "        \n",
    "        if is_linux:\n",
    "            # åœ¨Linux/WSLä¸­ï¼Œå°†åæ–œæ è½¬ä¸ºæ­£æ–œæ \n",
    "            path = path.replace('\\\\', '/')\n",
    "        \n",
    "        return path\n",
    "    return path\n",
    "\n",
    "def normalize_paths_dict(paths_dict):\n",
    "    \"\"\"\n",
    "    è§„èŒƒåŒ–è·¯å¾„å­—å…¸ä¸­çš„æ‰€æœ‰è·¯å¾„\n",
    "    \"\"\"\n",
    "    normalized = {}\n",
    "    for key, value in paths_dict.items():\n",
    "        normalized[key] = normalize_path(value)\n",
    "    return normalized\n",
    "\n",
    "def clip_data_with_us_states(df, us_states_gdf, lon_col='lon', lat_col='lat'):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ç¾Žå›½å·žç•Œ shapefile å‰ªè£ç‚¹æ•°æ®ï¼›å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ geopandas å‚æ•°å\n",
    "    \"\"\"\n",
    "    geometry = [Point(xy) for xy in zip(df[lon_col], df[lat_col])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')\n",
    "    us_states_4326 = us_states_gdf.to_crs('EPSG:4326')\n",
    "\n",
    "    try:\n",
    "        clipped = gpd.sjoin(gdf, us_states_4326, how='inner', predicate='within')\n",
    "    except TypeError:\n",
    "        # æ—§ç‰ˆæœ¬ geopandas ä½¿ç”¨ op å‚æ•°\n",
    "        clipped = gpd.sjoin(gdf, us_states_4326, how='inner', op='within')\n",
    "\n",
    "    # æ¸…ç† shapefile é™„åŠ å­—æ®µ\n",
    "    clipped = clipped.drop(columns=['geometry', 'index_right'], errors='ignore')\n",
    "    for col in us_states_gdf.columns:\n",
    "        if col in clipped.columns:\n",
    "            clipped = clipped.drop(columns=[col], errors='ignore')\n",
    "    return clipped\n",
    "\n",
    "PATHS = normalize_paths_dict(PATHS)\n",
    "PATHS['us_pv_embedding'] = normalize_path(str(DATA_PATH / 'training_embedding.csv'))\n",
    "PATHS['us_abandon'] = normalize_path(str(DATA_PATH / 'us_abandon_clean.csv'))\n",
    "print(f\"us_abandon path: {PATHS['us_abandon']}\")\n",
    "\n",
    "\n",
    "usa_bounds_main = dict(lon_min=-125, lon_max=-65, lat_min=25, lat_max=49)\n",
    "# åŠ è½½æ•°æ®\n",
    "print(\"åŠ è½½æ•°æ®...\")\n",
    "df_embedding = load_embedding(PATHS['us_pv_embedding'])\n",
    "df_abandon = load_abandon(PATHS['us_abandon'])\n",
    "\n",
    "df_abandon = load_abandon(PATHS['us_abandon'])\n",
    "df_embedding = load_embedding(PATHS['us_pv_embedding'])\n",
    "\n",
    "# åˆæ­¥ç»çº¬åº¦èŒƒå›´è¿‡æ»¤\n",
    "df_embedding = df_embedding[\n",
    "    (df_embedding['lon'] >= usa_bounds_main['lon_min']) &\n",
    "    (df_embedding['lon'] <= usa_bounds_main['lon_max']) &\n",
    "    (df_embedding['lat'] >= usa_bounds_main['lat_min']) &\n",
    "    (df_embedding['lat'] <= usa_bounds_main['lat_max'])\n",
    "]\n",
    "\n",
    "df_abandon = df_abandon[\n",
    "    (df_abandon['lon'] >= usa_bounds_main['lon_min']) &\n",
    "    (df_abandon['lon'] <= usa_bounds_main['lon_max']) &\n",
    "    (df_abandon['lat'] >= usa_bounds_main['lat_min']) &\n",
    "    (df_abandon['lat'] <= usa_bounds_main['lat_max'])\n",
    "]\n",
    "# ç¬¬äºŒæ¬¡ç”¨å·žç•ŒçŸ¢é‡æ›´ç²¾ç¡®è£å‰ª\n",
    "us_nation = gpd.read_file(normalize_path(str(DATA_PATH / 'US_data' / 'cb_2018_us_nation_5m.shp')))\n",
    "df_abandon = clip_data_with_us_states(df_abandon, us_nation)\n",
    "df_embedding = clip_data_with_us_states(df_embedding, us_nation)\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†\n",
    "print(\"é¢„å¤„ç†æ•°æ®...\")\n",
    "df_embedding_fill = fill_nonpositive_with_nearest(df_embedding)\n",
    "df_abandon_fill = fill_nonpositive_with_nearest(df_abandon)\n",
    "df_abandon_filtered = filter_duplicates(df_abandon_fill, df_embedding_fill)\n",
    "\n",
    "# å®šä¹‰ç‰¹å¾åˆ—è¡¨ï¼ˆæŽ’é™¤åæ ‡ï¼‰\n",
    "features_no_coords = [f for f in (NUMERIC_FEATURES + CAT_COLS) if f not in ['lat', 'lon']]\n",
    "features_no_coords = [c for c in features_no_coords if c in df_embedding_fill.columns]\n",
    "\n",
    "print(f\"âœ… æ•°æ®åŠ è½½å®Œæˆ\")\n",
    "print(f\"  - df_embedding_fill: {len(df_embedding_fill)} è¡Œ\")\n",
    "print(f\"  - df_abandon_filtered: {len(df_abandon_filtered)} è¡Œ\")\n",
    "print(f\"  - features_no_coords: {len(features_no_coords)} ä¸ªç‰¹å¾\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476dd674",
   "metadata": {},
   "source": [
    "## 2ã€Hyperparameter settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce49666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¡€é…ç½®ï¼ˆbase model = å„å› å­çš„ä¸­ç­‰æ°´å¹³ï¼‰\n",
    "base_configs = {\n",
    "    \"learning_rate\": 0.001,           # ä¸­ç­‰\n",
    "    \"dropout_rate\": 0.3,              # ä¸­ç­‰\n",
    "    \"transformer_config\": {\n",
    "        \"d_model\": 64,                # ä¸­ç­‰å®½åº¦\n",
    "        \"num_heads\": 4,\n",
    "        \"num_layers\": 8,              # â† ä¸­ç­‰æ·±åº¦ï¼ˆæµ…/ä¸­/æ·±ï¼š4 / 8 / 12ï¼‰\n",
    "    },\n",
    "    \"resnet_width\": 128,\n",
    "    \"resnet_depth\": 6,\n",
    "}\n",
    "\n",
    "# æ•æ„Ÿæ€§æµ‹è¯•å‚æ•°ï¼ˆ6ä¸ªå› å­ï¼Œæ¯ä¸ª3ä¸ªæ°´å¹³ï¼‰\n",
    "sensitivity_configs = {\n",
    "    # 1. Transformerå®½åº¦ï¼ˆd_modelï¼‰\n",
    "    \"d_model\": [32, 64, 128],             # å° / ä¸­ / å¤§\n",
    "\n",
    "    # 2. Transformeræ·±åº¦ï¼ˆnum_layersï¼‰\n",
    "    \"num_layers\": [4, 8, 12],             # æµ… / ä¸­ / æ·±ï¼ˆå·®å¼‚æ˜Žæ˜¾ï¼‰\n",
    "\n",
    "    # 3. ResNetå®½åº¦ï¼ˆç¬¬ä¸€å±‚å®½åº¦ï¼‰\n",
    "    \"resnet_width\": [64, 128, 256],       # å° / ä¸­ / å¤§\n",
    "\n",
    "    # 4. ResNetæ·±åº¦ï¼ˆå±‚æ•°ï¼‰\n",
    "    \"resnet_depth\": [3, 6, 9],            # æµ… / ä¸­ / æ·±ï¼ˆæ˜Žæ˜¾åŒºåˆ†ï¼‰\n",
    "\n",
    "    # 5. å­¦ä¹ çŽ‡\n",
    "    \"learning_rate\": [0.0001, 0.001, 0.01],  # å° / ä¸­ / å¤§\n",
    "\n",
    "    # 6. DropoutçŽ‡\n",
    "    \"dropout_rate\": [0.1, 0.3, 0.5],      # å¼± / ä¸­ / å¼ºæ­£åˆ™\n",
    "}\n",
    "\n",
    "# è¾…åŠ©å‡½æ•°ï¼šå°†resnet_widthå’Œresnet_depthè½¬æ¢ä¸ºresnet_layersåˆ—è¡¨\n",
    "def generate_resnet_layers(width, depth):\n",
    "    layers = []\n",
    "    current_width = width\n",
    "    for i in range(depth):\n",
    "        layers.append(int(current_width))\n",
    "        if i < depth - 1:  # æœ€åŽä¸€å±‚ä¸ç»§ç»­å‡åŠ\n",
    "            current_width = current_width / 2\n",
    "    return layers\n",
    "\n",
    "# base_model çš„ resnet_layers ç¤ºä¾‹\n",
    "base_resnet_layers = generate_resnet_layers(128, 6)\n",
    "# e.g. [128, 64, 32, 16, 8, 4]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3703b3",
   "metadata": {},
   "source": [
    "|   Exp |   d_model |   num_layers |   resnet_width |   resnet_depth |   learning_rate |   dropout_rate |\n",
    "|------:|----------:|-------------:|---------------:|---------------:|----------------:|---------------:|\n",
    "|     1 |        32 |            4 |             64 |              3 |          0.0001 |            0.1 |\n",
    "|     2 |        32 |            8 |            128 |              6 |          0.001  |            0.3 |\n",
    "|     3 |        32 |           12 |            256 |              9 |          0.01   |            0.5 |\n",
    "|     4 |        64 |            4 |             64 |              6 |          0.001  |            0.5 |\n",
    "|     5 |        64 |            8 |            128 |              9 |          0.01   |            0.1 |\n",
    "|     6 |        64 |           12 |            256 |              3 |          0.0001 |            0.3 |\n",
    "|     7 |       128 |            4 |            128 |              3 |          0.01   |            0.3 |\n",
    "|     8 |       128 |            8 |            256 |              6 |          0.0001 |            0.5 |\n",
    "|     9 |       128 |           12 |             64 |              9 |          0.001  |            0.1 |\n",
    "|    10 |        32 |            4 |            256 |              9 |          0.001  |            0.3 |\n",
    "|    11 |        32 |            8 |             64 |              3 |          0.01   |            0.5 |\n",
    "|    12 |        32 |           12 |            128 |              6 |          0.0001 |            0.1 |\n",
    "|    13 |        64 |            4 |            128 |              9 |          0.0001 |            0.5 |\n",
    "|    14 |        64 |            8 |            256 |              3 |          0.001  |            0.1 |\n",
    "|    15 |        64 |           12 |             64 |              6 |          0.01   |            0.3 |\n",
    "|    16 |       128 |            4 |            256 |              6 |          0.001  |            0.1 |\n",
    "|    17 |       128 |            8 |             64 |              9 |          0.01   |            0.3 |\n",
    "|    18 |       128 |           12 |            128 |              3 |          0.0001 |            0.5 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f7929a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… L18(3^6)æ­£äº¤è¡¨ç”Ÿæˆå®Œæˆ\n",
      "  - å®žéªŒæ¬¡æ•°: 18 (å…¨å› å­è®¾è®¡éœ€è¦ 729 æ¬¡)\n",
      "  - å‡å°‘å®žéªŒæ¬¡æ•°: 711 æ¬¡ (97.5%)\n",
      "  - æ­£äº¤è¡¨å½¢çŠ¶: (18, 6)\n"
     ]
    }
   ],
   "source": [
    "# 3. æ­£äº¤å®žéªŒè®¾è®¡ï¼ˆL18(3^6)ï¼‰\n",
    "def generate_l18_orthogonal_array():\n",
    "    \"\"\"\n",
    "    ç”ŸæˆL18(3^6)æ ‡å‡†æ­£äº¤è¡¨\n",
    "    18è¡Œï¼ˆå®žéªŒæ¬¡æ•°ï¼‰Ã— 6åˆ—ï¼ˆå› å­æ•°ï¼‰\n",
    "    æ¯åˆ—åŒ…å«0,1,2ä¸‰ä¸ªæ°´å¹³çš„å¹³è¡¡åˆ†å¸ƒ\n",
    "    \"\"\"\n",
    "    # æ ‡å‡†L18(3^6)æ­£äº¤è¡¨\n",
    "    orthogonal_array = np.array([\n",
    "        [0, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 1, 1, 1, 1],\n",
    "        [0, 2, 2, 2, 2, 2],\n",
    "        [1, 0, 0, 1, 1, 2],\n",
    "        [1, 1, 1, 2, 2, 0],\n",
    "        [1, 2, 2, 0, 0, 1],\n",
    "        [2, 0, 1, 0, 2, 1],\n",
    "        [2, 1, 2, 1, 0, 2],\n",
    "        [2, 2, 0, 2, 1, 0],\n",
    "        [0, 0, 2, 2, 1, 1],\n",
    "        [0, 1, 0, 0, 2, 2],\n",
    "        [0, 2, 1, 1, 0, 0],\n",
    "        [1, 0, 1, 2, 0, 2],\n",
    "        [1, 1, 2, 0, 1, 0],\n",
    "        [1, 2, 0, 1, 2, 1],\n",
    "        [2, 0, 2, 1, 1, 0],\n",
    "        [2, 1, 0, 2, 2, 1],\n",
    "        [2, 2, 1, 0, 0, 2]\n",
    "    ])\n",
    "    return orthogonal_array\n",
    "\n",
    "# ç”Ÿæˆæ­£äº¤è¡¨\n",
    "orthogonal_array = generate_l18_orthogonal_array()\n",
    "print(\"âœ… L18(3^6)æ­£äº¤è¡¨ç”Ÿæˆå®Œæˆ\")\n",
    "print(f\"  - å®žéªŒæ¬¡æ•°: {len(orthogonal_array)} (å…¨å› å­è®¾è®¡éœ€è¦ {3**6} æ¬¡)\")\n",
    "print(f\"  - å‡å°‘å®žéªŒæ¬¡æ•°: {3**6 - len(orthogonal_array)} æ¬¡ ({100*(1-len(orthogonal_array)/(3**6)):.1f}%)\")\n",
    "print(f\"  - æ­£äº¤è¡¨å½¢çŠ¶: {orthogonal_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa9b3dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å‚æ•°ç»„åˆæ˜ å°„å®Œæˆ\n",
      "  - å…± 18 ä¸ªå‚æ•°ç»„åˆ\n",
      "  - ç¤ºä¾‹ç»„åˆ 0:\n",
      "      d_model: 32\n",
      "      num_layers: 4\n",
      "      learning_rate: 0.0001\n",
      "      dropout_rate: 0.1\n",
      "      resnet_layers: [64, 32, 16]\n",
      "      _resnet_width: 64\n",
      "      _resnet_depth: 3\n",
      "      num_heads: 2\n"
     ]
    }
   ],
   "source": [
    "# 4. å‚æ•°æ˜ å°„å‡½æ•°ï¼ˆæ›´æ–°ç‰ˆï¼šå¤„ç†ResNetå®½åº¦å’Œæ·±åº¦ï¼‰\n",
    "def map_orthogonal_to_params(orthogonal_array, param_levels):\n",
    "    \"\"\"\n",
    "    å°†æ­£äº¤è¡¨ä¸­çš„æ°´å¹³ç´¢å¼•æ˜ å°„åˆ°å®žé™…å‚æ•°å€¼\n",
    "    ç‰¹æ®Šå¤„ç†ï¼šå°†resnet_widthå’Œresnet_depthè½¬æ¢ä¸ºresnet_layersåˆ—è¡¨\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - orthogonal_array: 18Ã—6çš„æ­£äº¤è¡¨æ•°ç»„\n",
    "    - param_levels: å‚æ•°å­—å…¸ï¼Œæ¯ä¸ªå‚æ•°å¯¹åº”3ä¸ªæ°´å¹³å€¼çš„åˆ—è¡¨\n",
    "    \n",
    "    è¿”å›ž:\n",
    "    - param_combinations: å‚æ•°ç»„åˆåˆ—è¡¨ï¼ˆåŒ…å«è½¬æ¢åŽçš„resnet_layersï¼‰\n",
    "    \"\"\"\n",
    "    param_names = list(param_levels.keys())\n",
    "    param_combinations = []\n",
    "    \n",
    "    for row in orthogonal_array:\n",
    "        param_dict = {}\n",
    "        for i, param_name in enumerate(param_names):\n",
    "            level_idx = row[i]\n",
    "            param_dict[param_name] = param_levels[param_name][level_idx]\n",
    "        \n",
    "        # ç‰¹æ®Šå¤„ç†ï¼šå°†resnet_widthå’Œresnet_depthè½¬æ¢ä¸ºresnet_layers\n",
    "        if 'resnet_width' in param_dict and 'resnet_depth' in param_dict:\n",
    "            resnet_layers = generate_resnet_layers(\n",
    "                param_dict['resnet_width'], \n",
    "                param_dict['resnet_depth']\n",
    "            )\n",
    "            param_dict['resnet_layers'] = resnet_layers\n",
    "            # ä¿ç•™åŽŸå§‹å€¼ç”¨äºŽè®°å½•\n",
    "            param_dict['_resnet_width'] = param_dict.pop('resnet_width')\n",
    "            param_dict['_resnet_depth'] = param_dict.pop('resnet_depth')\n",
    "        \n",
    "        # ç¡®ä¿num_headsä¸Žd_modelå…¼å®¹ï¼ˆd_modelå¿…é¡»èƒ½è¢«num_headsæ•´é™¤ï¼‰\n",
    "        if 'd_model' in param_dict:\n",
    "            d_model = param_dict['d_model']\n",
    "            # æ ¹æ®d_modelè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„num_heads\n",
    "            if d_model == 32:\n",
    "                param_dict['num_heads'] = 2  # 32/2=16\n",
    "            elif d_model == 64:\n",
    "                param_dict['num_heads'] = 4  # 64/4=16\n",
    "            elif d_model == 96:\n",
    "                param_dict['num_heads'] = 4  # 96/4=24ï¼ˆæˆ–8ï¼Œä½†4æ›´ç¨³å®šï¼‰\n",
    "        \n",
    "        param_combinations.append(param_dict)\n",
    "    \n",
    "    return param_combinations\n",
    "\n",
    "# ç”Ÿæˆå‚æ•°ç»„åˆ\n",
    "param_combinations = map_orthogonal_to_params(orthogonal_array, sensitivity_configs)\n",
    "print(\"âœ… å‚æ•°ç»„åˆæ˜ å°„å®Œæˆ\")\n",
    "print(f\"  - å…± {len(param_combinations)} ä¸ªå‚æ•°ç»„åˆ\")\n",
    "print(f\"  - ç¤ºä¾‹ç»„åˆ 0:\")\n",
    "for k, v in param_combinations[0].items():\n",
    "    print(f\"      {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12a045",
   "metadata": {},
   "source": [
    "\n",
    "æ•æ„Ÿæ€§åˆ†æžå®ŒæˆåŽï¼Œå°†ç”Ÿæˆä»¥ä¸‹6ä¸ªDataFrameï¼Œæ¯ä¸ªDataFrameåŒ…å«18ä¸ªå®žéªŒçš„å‚æ•°ç»„åˆå’Œå¯¹åº”çš„æŒ‡æ ‡å€¼ï¼š\n",
    "\n",
    "| DataFrameåç§° | æŒ‡æ ‡åˆ— | è¯´æ˜Ž | ä¿å­˜æ–‡ä»¶å |\n",
    "|--------------|--------|------|-----------|\n",
    "| `df_overfitting` | `overfitting_score` | è¿‡æ‹Ÿåˆåˆ†æ•°ï¼ˆè®­ç»ƒé›†ä¸ŽéªŒè¯é›†çš„æœ€ç»ˆå·®å¼‚ï¼‰ | `df_overfitting.csv` |\n",
    "| `df_accuracy` | `accuracy` | æµ‹è¯•é›†å‡†ç¡®çŽ‡ | `df_accuracy.csv` |\n",
    "| `df_precision` | `precision` | æµ‹è¯•é›†ç²¾ç¡®çŽ‡ | `df_precision.csv` |\n",
    "| `df_recall` | `recall` | æµ‹è¯•é›†å¬å›žçŽ‡ | `df_recall.csv` |\n",
    "| `df_f1` | `f1` | æµ‹è¯•é›†F1åˆ†æ•° | `df_f1.csv` |\n",
    "| `df_mean_prob` | `mean_probability` | é¢„æµ‹ç»“æžœçš„å¹³å‡æ¦‚çŽ‡ | `df_mean_prob.csv` |\n",
    "\n",
    "\n",
    "\n",
    "æ¯ä¸ªDataFrameåŒ…å«ä»¥ä¸‹åˆ—ï¼š\n",
    "\n",
    "**å‚æ•°åˆ—ï¼ˆ6åˆ—ï¼‰**ï¼š\n",
    "- `learning_rate`: å­¦ä¹ çŽ‡\n",
    "- `resnet_layers_str`: ResNetå±‚ç»“æž„ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼Œå¦‚ \"[128, 128, 64]\"ï¼‰\n",
    "- `d_model`: Transformeræ¨¡åž‹ç»´åº¦\n",
    "- `num_heads`: Transformeræ³¨æ„åŠ›å¤´æ•°\n",
    "- `num_layers`: Transformerå±‚æ•°\n",
    "- `dropout_rate`: Dropoutæ¯”çŽ‡\n",
    "\n",
    "**æŒ‡æ ‡åˆ—ï¼ˆ1åˆ—ï¼‰**ï¼š\n",
    "- å¯¹åº”çš„æŒ‡æ ‡å€¼ï¼ˆæ ¹æ®DataFrameç±»åž‹ä¸åŒï¼‰\n",
    "\n",
    "\n",
    "æ‰€æœ‰ç»“æžœå°†ä¿å­˜åˆ° `Supplymentary/ML_sensitivity/` ç›®å½•ï¼š\n",
    "- 6ä¸ªCSVæ–‡ä»¶ï¼ˆæ¯ä¸ªDataFrameä¸€ä¸ªï¼‰\n",
    "- `orthogonal_config.json`ï¼šæ­£äº¤è¡¨é…ç½®ä¿¡æ¯\n",
    "- `sensitivity_summary.json`ï¼šç»“æžœæ‘˜è¦ï¼ˆåŒ…æ‹¬æœ€ä½³å‚æ•°ç»„åˆï¼‰\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8bf92f",
   "metadata": {},
   "source": [
    "## 3ã€Sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "064576ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ•æ„Ÿæ€§åˆ†æžå‡½æ•°å®šä¹‰å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# 5. æ•æ„Ÿæ€§åˆ†æžä¸»å‡½æ•°\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def extract_metrics(result, params):\n",
    "    \"\"\"\n",
    "    ä»Žè®­ç»ƒç»“æžœä¸­æå–æŒ‡æ ‡\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - result: run_correct_training_pipelineçš„è¿”å›žç»“æžœ\n",
    "    - params: å‚æ•°å­—å…¸\n",
    "    \n",
    "    è¿”å›ž:\n",
    "    - metrics_dict: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    metrics_dict = params.copy()\n",
    "    \n",
    "    try:\n",
    "        # è¿‡æ‹Ÿåˆscore\n",
    "        if result.get('learning_curve_analysis') and result['learning_curve_analysis'].get('overfitting_analysis'):\n",
    "            metrics_dict['overfitting_score'] = result['learning_curve_analysis']['overfitting_analysis'].get('final_gap', np.nan)\n",
    "        else:\n",
    "            metrics_dict['overfitting_score'] = np.nan\n",
    "        \n",
    "        # æµ‹è¯•é›†æŒ‡æ ‡\n",
    "        if result.get('training_results') and result['training_results'].get('metrics'):\n",
    "            test_metrics = result['training_results']['metrics'].get('test', {})\n",
    "            metrics_dict['accuracy'] = test_metrics.get('accuracy', np.nan)\n",
    "            metrics_dict['precision'] = test_metrics.get('precision', np.nan)\n",
    "            metrics_dict['recall'] = test_metrics.get('recall', np.nan)\n",
    "            metrics_dict['f1'] = test_metrics.get('f1', np.nan)\n",
    "        else:\n",
    "            metrics_dict['accuracy'] = np.nan\n",
    "            metrics_dict['precision'] = np.nan\n",
    "            metrics_dict['recall'] = np.nan\n",
    "            metrics_dict['f1'] = np.nan\n",
    "        \n",
    "        # mean probability\n",
    "        if result.get('prediction_results') is not None:\n",
    "            pred_df = result['prediction_results']\n",
    "            if 'predicted_prob' in pred_df.columns:\n",
    "                metrics_dict['mean_probability'] = pred_df['predicted_prob'].mean()\n",
    "            else:\n",
    "                metrics_dict['mean_probability'] = np.nan\n",
    "        else:\n",
    "            metrics_dict['mean_probability'] = np.nan\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ æå–æŒ‡æ ‡æ—¶å‡ºé”™: {e}\")\n",
    "        metrics_dict['overfitting_score'] = np.nan\n",
    "        metrics_dict['accuracy'] = np.nan\n",
    "        metrics_dict['precision'] = np.nan\n",
    "        metrics_dict['recall'] = np.nan\n",
    "        metrics_dict['f1'] = np.nan\n",
    "        metrics_dict['mean_probability'] = np.nan\n",
    "    \n",
    "    return metrics_dict\n",
    "\n",
    "def run_sensitivity_analysis(\n",
    "    df_positive, df_prediction_pool, features_no_coords,\n",
    "    param_combinations, base_configs,\n",
    "    epochs=80, batch_size=256, random_state=42,\n",
    "    test_size=0.2, val_size=0.2,\n",
    "    negative_strategy='generation', negative_ratio=1,\n",
    "    plot_learning_curve=True, learning_curve_epochs=20,\n",
    "    output_dir='Supplymentary/ML_sensitivity',\n",
    "    save_models=True\n",
    "):\n",
    "    \"\"\"\n",
    "    è¿è¡Œæ•æ„Ÿæ€§åˆ†æž\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - df_positive: æ­£æ ·æœ¬æ•°æ®\n",
    "    - df_prediction_pool: é¢„æµ‹æ± æ•°æ®\n",
    "    - features_no_coords: ç‰¹å¾åˆ—è¡¨\n",
    "    - param_combinations: å‚æ•°ç»„åˆåˆ—è¡¨\n",
    "    - base_configs: åŸºç¡€é…ç½®\n",
    "    - å…¶ä»–å‚æ•°: è®­ç»ƒç›¸å…³å‚æ•°\n",
    "    \n",
    "    è¿”å›ž:\n",
    "    - results: æ‰€æœ‰å®žéªŒçš„ç»“æžœåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    saved_models = []  # è®°å½•ä¿å­˜çš„æ¨¡åž‹è·¯å¾„\n",
    "    \n",
    "    # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    models_dir = os.path.join(output_dir, 'models')\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"å¼€å§‹æ•æ„Ÿæ€§åˆ†æž: {len(param_combinations)} ä¸ªå®žéªŒ\")\n",
    "    print(f\"è´Ÿæ ·æœ¬ç­–ç•¥: {negative_strategy}\")\n",
    "    print(f\"æ¨¡åž‹ä¿å­˜ç›®å½•: {models_dir}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # ä½¿ç”¨tqdmæ˜¾ç¤ºæ€»ä½“è¿›åº¦ï¼Œå¹¶é…ç½®é€‚åˆWSL/Jupyterçš„æ˜¾ç¤ºæ–¹å¼\n",
    "    pbar = tqdm(enumerate(param_combinations), \n",
    "                total=len(param_combinations),\n",
    "                desc=\"æ•æ„Ÿæ€§åˆ†æž\",\n",
    "                unit=\"å®žéªŒ\",\n",
    "                ncols=100,  \n",
    "                mininterval=1.0,  \n",
    "                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')\n",
    "    \n",
    "    for i, params in pbar:\n",
    "        exp_id = f\"E{i+1}\"\n",
    "        # æ›´æ–°è¿›åº¦æ¡æè¿°ï¼Œæ˜¾ç¤ºå½“å‰å®žéªŒä¿¡æ¯\n",
    "        pbar.set_description(f\"å®žéªŒ {exp_id}\")\n",
    "        pbar.set_postfix({\n",
    "            'LR': f\"{params['learning_rate']:.4f}\",\n",
    "            'd_m': params['d_model']\n",
    "        })\n",
    "        \n",
    "        exp_id = f\"E{i+1}\"\n",
    "        print(f\"\\nå®žéªŒ {exp_id} ({i+1}/{len(param_combinations)})\")\n",
    "        print(f\"å‚æ•°ç»„åˆ: {params}\")\n",
    "        \n",
    "        try:\n",
    "            # æž„å»ºtransformer_configï¼ˆä½¿ç”¨æ›´æ–°åŽçš„å‚æ•°ç»“æž„ï¼‰\n",
    "            transformer_config = {\n",
    "                'd_model': params['d_model'],\n",
    "                'num_heads': params.get('num_heads', 4),  # ä»Žæ˜ å°„å‡½æ•°ä¸­å·²è®¾ç½®\n",
    "                'num_layers': params['num_layers']\n",
    "            }\n",
    "            \n",
    "            # èŽ·å–ResNetå±‚é…ç½®ï¼ˆå·²ä»Žwidthå’Œdepthè½¬æ¢ï¼‰\n",
    "            resnet_layers = params['resnet_layers']\n",
    "            \n",
    "            # è°ƒç”¨è®­ç»ƒç®¡é“\n",
    "            result = run_correct_training_pipeline(\n",
    "                df_positive=df_positive,\n",
    "                df_prediction_pool=df_prediction_pool,\n",
    "                features_no_coords=features_no_coords,\n",
    "                negative_strategy=negative_strategy,\n",
    "                negative_ratio=negative_ratio,\n",
    "                augmentation_ratio=1,  \n",
    "                test_size=test_size,\n",
    "                val_size=val_size,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                random_state=random_state,\n",
    "                learning_rate=params['learning_rate'],\n",
    "                dropout_rate=params['dropout_rate'],\n",
    "                resnet_layers=resnet_layers,\n",
    "                transformer_config=transformer_config,\n",
    "                model_type='transformer',\n",
    "                train_mode='single',\n",
    "                plot_learning_curve=plot_learning_curve,\n",
    "                learning_curve_epochs=learning_curve_epochs,\n",
    "                run_shap=False\n",
    "            )\n",
    "            \n",
    "            # æå–æŒ‡æ ‡\n",
    "            metrics = extract_metrics(result, params)\n",
    "            results.append(metrics)\n",
    "            \n",
    "            if save_models and result is not None:\n",
    "                try:\n",
    "                    model_name = f\"{exp_id}_transformer_generation\"\n",
    "                    saved_path = save_complete_model_pipeline(\n",
    "                        gmm_pipeline=result.get('gmm_pipeline'),\n",
    "                        dl_model=result.get('model'),\n",
    "                        retrained_preprocessor=result['training_results'].get('preprocessor'),\n",
    "                        training_results=result.get('training_results'),\n",
    "                        final_results=result.get('final_results'),\n",
    "                        negative_results=result.get('negative_samples'),\n",
    "                        prediction_results=result.get('prediction_results'),\n",
    "                        features=features_no_coords,\n",
    "                        config=result.get('config', {}),\n",
    "                        save_dir=models_dir,\n",
    "                        model_name=model_name,\n",
    "                        model_type='transformer',\n",
    "                        negative_strategy=negative_strategy,\n",
    "                        train_mode='single',\n",
    "                        pu_evaluation=result.get('pu_evaluation')\n",
    "                    )\n",
    "                    saved_models.append({\n",
    "                        'exp_id': exp_id,\n",
    "                        'exp_num': i+1,\n",
    "                        'model_path': saved_path,\n",
    "                        'params': params,\n",
    "                        'metrics': {k: v for k, v in metrics.items() \n",
    "                                   if k in ['accuracy', 'f1', 'overfitting_score', 'precision', 'recall', 'mean_probability']}\n",
    "                    })\n",
    "                    print(f\"  ðŸ’¾ æ¨¡åž‹å·²ä¿å­˜: {saved_path}\")\n",
    "                except Exception as save_error:\n",
    "                    print(f\"  âš ï¸ æ¨¡åž‹ä¿å­˜å¤±è´¥: {save_error}\")\n",
    "            \n",
    "            # æ›´æ–°è¿›åº¦æ¡åŽç½®ä¿¡æ¯\n",
    "            pbar.set_postfix({\n",
    "                'Acc': f\"{metrics.get('accuracy', np.nan):.3f}\",\n",
    "                'F1': f\"{metrics.get('f1', np.nan):.3f}\"\n",
    "            })\n",
    "            \n",
    "            print(f\"âœ… å®žéªŒ {exp_id} å®Œæˆ\")\n",
    "            print(f\"  - Accuracy: {metrics.get('accuracy', 'N/A'):.4f}\")\n",
    "            print(f\"  - F1: {metrics.get('f1', 'N/A'):.4f}\")\n",
    "            print(f\"  - è¿‡æ‹Ÿåˆscore: {metrics.get('overfitting_score', 'N/A'):.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å®žéªŒ {exp_id} å¤±è´¥: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # è®°å½•å¤±è´¥å®žéªŒçš„å‚æ•°\n",
    "            failed_metrics = params.copy()\n",
    "            failed_metrics['overfitting_score'] = np.nan\n",
    "            failed_metrics['accuracy'] = np.nan\n",
    "            failed_metrics['precision'] = np.nan\n",
    "            failed_metrics['recall'] = np.nan\n",
    "            failed_metrics['f1'] = np.nan\n",
    "            failed_metrics['mean_probability'] = np.nan\n",
    "            results.append(failed_metrics)\n",
    "    \n",
    "    \n",
    "    # å…³é—­è¿›åº¦æ¡\n",
    "    pbar.close()\n",
    "    \n",
    "    # ä¿å­˜æ¨¡åž‹ç´¢å¼•æ–‡ä»¶\n",
    "    if save_models and saved_models:\n",
    "        models_index_file = os.path.join(output_dir, 'models_index.json')\n",
    "        with open(models_index_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(saved_models, f, indent=2, ensure_ascii=False, default=str)\n",
    "        print(f\"\\nâœ… æ¨¡åž‹ç´¢å¼•å·²ä¿å­˜: {models_index_file}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"æ•æ„Ÿæ€§åˆ†æžå®Œæˆ: {len(results)}/{len(param_combinations)} ä¸ªå®žéªŒæˆåŠŸ\")\n",
    "    if save_models:\n",
    "        print(f\"å·²ä¿å­˜æ¨¡åž‹: {len(saved_models)}/{len(param_combinations)} ä¸ª\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… æ•æ„Ÿæ€§åˆ†æžå‡½æ•°å®šä¹‰å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df881ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "å¼€å§‹æ•æ„Ÿæ€§åˆ†æž: 18 ä¸ªå®žéªŒ\n",
      "è´Ÿæ ·æœ¬ç­–ç•¥: generation\n",
      "æ¨¡åž‹ä¿å­˜ç›®å½•: Supplymentary/ML_sensitivity/models\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c522ff2c6f454a8c881e17e0678169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "æ•æ„Ÿæ€§åˆ†æž:   0%|                                                          | 0/18 [00:00<?, ?å®žéªŒ/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å®žéªŒ E1 (1/18)\n",
      "å‚æ•°ç»„åˆ: {'d_model': 32, 'num_layers': 4, 'learning_rate': 0.0001, 'dropout_rate': 0.1, 'resnet_layers': [64, 32, 16], '_resnet_width': 64, '_resnet_depth': 3, 'num_heads': 2}\n",
      "================================================================================\n",
      "æ­£ç¡®çš„è®­ç»ƒç®¡é“ï¼šåˆ†å±‚è´Ÿæ ·æœ¬é‡‡æ ·çš„å®Œæ•´æµç¨‹\n",
      "================================================================================\n",
      "\n",
      "æ­¥éª¤1: åŠ è½½æˆ–è®­ç»ƒGMMæ¨¡åž‹ç”¨äºŽçŽ¯å¢ƒç›¸ä¼¼åº¦è¯„ä¼°\n",
      "[GMM] å½“å‰é¡¹ç›®æ ¹ç›®å½•æŽ¨æ–­ä¸º: /mnt/c/Dev/Landuse_Zhong_clean\n",
      "ðŸ” å‘çŽ°å·²ä¿å­˜çš„GMMæ¨¡åž‹æ–‡ä»¶: /mnt/c/Dev/Landuse_Zhong_clean/gmm_model_23c_fixed.pkl\n",
      "ðŸ“‚ å°è¯•åŠ è½½æ¨¡åž‹...\n",
      "âœ… æˆåŠŸåŠ è½½GMMæ¨¡åž‹: gmm_model_23c_fixed.pkl\n",
      "   æ¨¡åž‹ç»„ä»¶æ•°: 23\n",
      "   åæ–¹å·®ç±»åž‹: diag\n",
      "\n",
      "æ­¥éª¤2: è´Ÿæ ·æœ¬ç”Ÿæˆ - ç­–ç•¥: generation\n",
      "\n",
      "================================================================================\n",
      "ç»Ÿä¸€è´Ÿæ ·æœ¬ç”ŸæˆæŽ¥å£ - ç­–ç•¥: generation\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "ç”Ÿæˆå¼è´Ÿæ ·æœ¬ç­–ç•¥ï¼ˆçº¯ç²¹GMMé‡‡æ ·ï¼‰\n",
      "============================================================\n",
      "\n",
      "ç”Ÿæˆé…ç½®:\n",
      "  æ­£æ ·æœ¬æ•°: 10473\n",
      "  ç›®æ ‡è´Ÿæ ·æœ¬æ•°: 10473\n",
      "\n",
      "ç­›é€‰æ ‡å‡†:\n",
      "  æ­£æ ·æœ¬å¹³å‡logæ¦‚çŽ‡: 49.978\n",
      "  é˜ˆå€¼ (å‡å€¼ - 1std): 27.508\n",
      "\n",
      "å°è¯• 1/8: é‡‡æ · 20946 ä¸ª...\n",
      "  å®žé™…é‡‡æ ·: 20946 ä¸ª\n",
      "  ç¬¦åˆé˜ˆå€¼: 217 ä¸ª (1.0%)\n",
      "  ç´¯è®¡æœ‰æ•ˆæ ·æœ¬: 217 ä¸ª\n",
      "  â³ ç»§ç»­å¢žåŠ é‡‡æ ·é‡ (ä¸‹æ¬¡: 31419 ä¸ª)...\n",
      "\n",
      "å°è¯• 2/8: é‡‡æ · 31419 ä¸ª...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zpy10/bayes-gpu/lib/python3.10/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator SimpleImputer from version 1.7.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/zpy10/bayes-gpu/lib/python3.10/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/zpy10/bayes-gpu/lib/python3.10/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.7.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/zpy10/bayes-gpu/lib/python3.10/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator OneHotEncoder from version 1.7.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/zpy10/bayes-gpu/lib/python3.10/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator GaussianMixture from version 1.7.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  å®žé™…é‡‡æ ·: 31419 ä¸ª\n",
      "  ç¬¦åˆé˜ˆå€¼: 359 ä¸ª (1.1%)\n",
      "  ç´¯è®¡æœ‰æ•ˆæ ·æœ¬: 576 ä¸ª\n",
      "  â³ ç»§ç»­å¢žåŠ é‡‡æ ·é‡ (ä¸‹æ¬¡: 41892 ä¸ª)...\n",
      "\n",
      "å°è¯• 3/8: é‡‡æ · 41892 ä¸ª...\n",
      "  å®žé™…é‡‡æ ·: 41892 ä¸ª\n",
      "  ç¬¦åˆé˜ˆå€¼: 476 ä¸ª (1.1%)\n",
      "  ç´¯è®¡æœ‰æ•ˆæ ·æœ¬: 1052 ä¸ª\n",
      "  â³ ç»§ç»­å¢žåŠ é‡‡æ ·é‡ (ä¸‹æ¬¡: 62838 ä¸ª)...\n",
      "\n",
      "å°è¯• 4/8: é‡‡æ · 62838 ä¸ª...\n",
      "  å®žé™…é‡‡æ ·: 62838 ä¸ª\n",
      "  ç¬¦åˆé˜ˆå€¼: 710 ä¸ª (1.1%)\n",
      "  ç´¯è®¡æœ‰æ•ˆæ ·æœ¬: 1762 ä¸ª\n",
      "  â³ ç»§ç»­å¢žåŠ é‡‡æ ·é‡ (ä¸‹æ¬¡: 94257 ä¸ª)...\n",
      "\n",
      "å°è¯• 5/8: é‡‡æ · 94257 ä¸ª...\n",
      "  å®žé™…é‡‡æ ·: 94257 ä¸ª\n",
      "  ç¬¦åˆé˜ˆå€¼: 1070 ä¸ª (1.1%)\n",
      "  ç´¯è®¡æœ‰æ•ˆæ ·æœ¬: 2832 ä¸ª\n",
      "  â³ ç»§ç»­å¢žåŠ é‡‡æ ·é‡ (ä¸‹æ¬¡: 136149 ä¸ª)...\n",
      "\n",
      "å°è¯• 6/8: é‡‡æ · 136149 ä¸ª...\n",
      "  å®žé™…é‡‡æ ·: 136149 ä¸ª\n",
      "  ç¬¦åˆé˜ˆå€¼: 1497 ä¸ª (1.1%)\n",
      "  ç´¯è®¡æœ‰æ•ˆæ ·æœ¬: 4329 ä¸ª\n",
      "  â³ ç»§ç»­å¢žåŠ é‡‡æ ·é‡ (ä¸‹æ¬¡: 198987 ä¸ª)...\n",
      "\n",
      "å°è¯• 7/8: é‡‡æ · 198987 ä¸ª...\n",
      "  å®žé™…é‡‡æ ·: 198987 ä¸ª\n",
      "  ç¬¦åˆé˜ˆå€¼: 2174 ä¸ª (1.1%)\n",
      "  ç´¯è®¡æœ‰æ•ˆæ ·æœ¬: 6503 ä¸ª\n",
      "  â³ ç»§ç»­å¢žåŠ é‡‡æ ·é‡ (ä¸‹æ¬¡: 293244 ä¸ª)...\n",
      "\n",
      "å°è¯• 8/8: é‡‡æ · 293244 ä¸ª...\n",
      "  å®žé™…é‡‡æ ·: 293244 ä¸ª\n",
      "  ç¬¦åˆé˜ˆå€¼: 3151 ä¸ª (1.1%)\n",
      "  ç´¯è®¡æœ‰æ•ˆæ ·æœ¬: 9654 ä¸ª\n",
      "  âš ï¸ å·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨æ ·æœ¬ (9654 ä¸ª)\n",
      "\n",
      "é‡‡æ ·ç»Ÿè®¡:\n",
      "  æ€»é‡‡æ ·æ¬¡æ•°: 879732 ä¸ª\n",
      "  æœ‰æ•ˆæ ·æœ¬çŽ‡: 1.10%\n",
      "  æœ€ç»ˆè´Ÿæ ·æœ¬æ•°: 9654 ä¸ª\n",
      "\n",
      "é€†å˜æ¢æ˜ å°„åˆ°åŽŸå§‹ç‰¹å¾ç©ºé—´ï¼ˆæ‰¹é‡å¤„ç†ï¼‰...\n",
      "é€†å˜æ¢å®Œæˆ: 9654 ä¸ªè´Ÿæ ·æœ¬\n",
      "å¹³å‡log-density: -52.238\n",
      "ç”Ÿæˆå®Œæˆ: 9654 ä¸ªè´Ÿæ ·æœ¬\n",
      "å¹³å‡log-density: -52.238\n",
      "ä¿ç•™æ•´ä¸ªé¢„æµ‹æ± ç”¨äºŽé¢„æµ‹: 70337 ä¸ªæ ·æœ¬\n",
      "\n",
      "æ­¥éª¤3: è®­ç»ƒæ·±åº¦å­¦ä¹ åˆ†ç±»æ¨¡åž‹\n",
      "âœ… TensorFlowç¡®å®šæ€§æ¨¡å¼å·²å¯ç”¨\n",
      "å‡†å¤‡åŽŸå§‹æ•°æ®...\n",
      "åŽŸå§‹ç‰¹å¾: (20127, 15)\n",
      "æ ‡ç­¾åˆ†å¸ƒ: æ­£æ ·æœ¬=10473, è´Ÿæ ·æœ¬=9654, æ­£æ ·æœ¬æ¯”ä¾‹=0.520\n",
      "å…ˆåˆ’åˆ†åŽŸå§‹æ•°æ®...\n",
      "åŽŸå§‹æ•°æ®åˆ’åˆ†å®Œæˆ:\n",
      "  è®­ç»ƒé›†: (12075, 15) (æ­£æ ·æœ¬æ¯”ä¾‹: 0.520)\n",
      "  éªŒè¯é›†: (4026, 15) (æ­£æ ·æœ¬æ¯”ä¾‹: 0.520)\n",
      "  æµ‹è¯•é›†: (4026, 15) (æ­£æ ·æœ¬æ¯”ä¾‹: 0.520)\n",
      "åœ¨è®­ç»ƒé›†ä¸Šé‡æ–°æ‹Ÿåˆé¢„å¤„ç†å™¨...\n",
      "âœ… é¢„å¤„ç†å™¨å·²åœ¨è®­ç»ƒé›†ä¸Šé‡æ–°æ‹Ÿåˆï¼ˆé¿å…æ­£æ ·æœ¬åå·®ï¼‰\n",
      "\n",
      "æ‰§è¡Œå­¦ä¹ æ›²çº¿åˆ†æžï¼ˆä»…è®­ç»ƒé›†ï¼Œæ— æ³„éœ²ï¼‰...\n",
      "âœ… ä½¿ç”¨Transformeræ¨¡åž‹è¿›è¡Œå­¦ä¹ æ›²çº¿åˆ†æž\n",
      "============================================================\n",
      "è®¡ç®—å­¦ä¹ æ›²çº¿æ•°æ®ï¼ˆæ‰‹å†™CVå¾ªçŽ¯ç‰ˆæœ¬ï¼‰\n",
      "============================================================\n",
      "æŽ¨æ–­é¢„å¤„ç†åŽçš„ç‰¹å¾ç»´åº¦...\n",
      "âœ… æŽ¨æ–­çš„è¾“å…¥ç»´åº¦: 23\n",
      "âœ… é¢„æœŸç‰¹å¾æž„æˆ: 14ä¸ªæ•°å€¼ç‰¹å¾ + 9ä¸ªlandcover One-Hot = 23ä¸ªæ€»ç‰¹å¾\n",
      "å¼€å§‹æ‰‹å·¥å­¦ä¹ æ›²çº¿è®¡ç®—...\n",
      "  æ•°æ®é›†å¤§å°: 12075\n",
      "  CVæŠ˜æ•°: 5\n",
      "  åŽŸå§‹ train_sizes: [0.2 0.4 0.6 0.8 1. ]\n",
      "  è¯„åˆ†æŒ‡æ ‡: f1\n",
      "  å®žé™…ä½¿ç”¨çš„è®­ç»ƒé›†å¤§å°: [ 2415  4830  7245  9660 12074]\n",
      "\n",
      "ðŸ”¹ è®­ç»ƒé›†å¤§å° 2415 / 12075\n",
      "Building Transformer+ResNet Hybrid Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1764019435.614351  147604 gpu_device.cc:2456] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0a. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "W0000 00:00:1764019435.616647  147604 gpu_device.cc:2456] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0a. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "I0000 00:00:1764019435.792684  147604 gpu_device.cc:2040] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13159 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5080 Laptop GPU, pci bus id: 0000:02:00.0, compute capability: 12.0a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input dim: 23 | Transformer layers: 4 | Params: 61,837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1764019436.885136  147604 node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n",
      "E0000 00:00:1764019477.534265  147604 node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 1/5 | train_score=0.801, val_score=0.784\n",
      "Building Transformer+ResNet Hybrid Model...\n",
      "  Input dim: 23 | Transformer layers: 4 | Params: 61,837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1764019518.395539  147604 node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 2/5 | train_score=0.810, val_score=0.789\n",
      "Building Transformer+ResNet Hybrid Model...\n",
      "  Input dim: 23 | Transformer layers: 4 | Params: 61,837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1764019563.085634  147604 node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 3/5 | train_score=0.810, val_score=0.778\n",
      "Building Transformer+ResNet Hybrid Model...\n",
      "  Input dim: 23 | Transformer layers: 4 | Params: 61,837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1764019601.661980  147604 node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 4/5 | train_score=0.805, val_score=0.792\n",
      "Building Transformer+ResNet Hybrid Model...\n",
      "  Input dim: 23 | Transformer layers: 4 | Params: 61,837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1764019647.441846  147604 node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 5/5 | train_score=0.804, val_score=0.764\n",
      "\n",
      "ðŸ”¹ è®­ç»ƒé›†å¤§å° 4830 / 12075\n",
      "Building Transformer+ResNet Hybrid Model...\n",
      "  Input dim: 23 | Transformer layers: 4 | Params: 61,837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1764019736.375674  147604 node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 1/5 | train_score=0.829, val_score=0.810\n",
      "Building Transformer+ResNet Hybrid Model...\n",
      "  Input dim: 23 | Transformer layers: 4 | Params: 61,837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1764019848.166257  147604 node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 2/5 | train_score=0.837, val_score=0.820\n",
      "Building Transformer+ResNet Hybrid Model...\n",
      "  Input dim: 23 | Transformer layers: 4 | Params: 61,837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1764019964.835992  147604 node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 3/5 | train_score=0.822, val_score=0.806\n",
      "Building Transformer+ResNet Hybrid Model...\n",
      "  Input dim: 23 | Transformer layers: 4 | Params: 61,837\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_sensitivity_analysis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_positive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_embedding_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_prediction_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_abandon_filtered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures_no_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_no_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam_combinations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_combinations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgeneration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_learning_curve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_curve_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSupplymentary/ML_sensitivity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 131\u001b[0m, in \u001b[0;36mrun_sensitivity_analysis\u001b[0;34m(df_positive, df_prediction_pool, features_no_coords, param_combinations, base_configs, epochs, batch_size, random_state, test_size, val_size, negative_strategy, negative_ratio, plot_learning_curve, learning_curve_epochs, output_dir, save_models)\u001b[0m\n\u001b[1;32m    128\u001b[0m resnet_layers \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet_layers\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# è°ƒç”¨è®­ç»ƒç®¡é“\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_correct_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_positive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_positive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_prediction_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_prediction_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures_no_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_no_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugmentation_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropout_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresnet_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransformer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msingle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_learning_curve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_learning_curve\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_curve_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_curve_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_shap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# æå–æŒ‡æ ‡\u001b[39;00m\n\u001b[1;32m    155\u001b[0m metrics \u001b[38;5;241m=\u001b[39m extract_metrics(result, params)\n",
      "File \u001b[0;32m/mnt/c/Dev/Landuse_Zhong_clean/function/pipeline.py:204\u001b[0m, in \u001b[0;36mrun_correct_training_pipeline\u001b[0;34m(df_positive, df_prediction_pool, features_no_coords, negative_strategy, negative_ratio, sampling_strategy, difficulty_levels, augmentation_ratio, selection_weight, test_size, val_size, epochs, batch_size, random_state, hidden_layers, dropout_rate, learning_rate, plot_learning_curve, learning_curve_epochs, model_type, train_mode, models_to_train, transformer_config, rf_config, resnet_layers, run_shap)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mä½¿ç”¨æœ€ä½³æ¨¡åž‹ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m è¿›è¡Œé¢„æµ‹\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     training_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_combined_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_no_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgmm_preprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplot_learning_curve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_learning_curve\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_curve_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_curve_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransformer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrf_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrf_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresnet_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet_layers\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m training_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mæ¨¡åž‹è®­ç»ƒå¤±è´¥\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/Dev/Landuse_Zhong_clean/function/training.py:178\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(df_combined_training, features_no_coords, gmm_preprocessor, test_size, val_size, epochs, batch_size, random_state, hidden_layers, dropout_rate, learning_rate, plot_learning_curve, learning_curve_epochs, model_type, transformer_config, rf_config, resnet_layers)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… ä½¿ç”¨MLPæ¨¡åž‹è¿›è¡Œå­¦ä¹ æ›²çº¿åˆ†æž\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuild_model_fn\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[0;32m--> 178\u001b[0m     lc_analysis \u001b[38;5;241m=\u001b[39m \u001b[43mplot_learning_curve_nn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuild_model_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_model_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures_no_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_no_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocessor_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgmm_preprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_curve_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcv_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransformer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresnet_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lc_analysis:\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… å­¦ä¹ æ›²çº¿åˆ†æžå®Œæˆ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/Dev/Landuse_Zhong_clean/function/learning_curve.py:740\u001b[0m, in \u001b[0;36mplot_learning_curve_nn\u001b[0;34m(build_model_fn, X_raw, y, features_no_coords, preprocessor_class, preprocessor_instance, train_sizes, cv_splits, epochs, batch_size, learning_rate, hidden_layers, dropout_rate, scoring, random_state, transformer_config, resnet_layers, save_path, model_name)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;124;03mç”Ÿæˆç¥žç»ç½‘ç»œå­¦ä¹ æ›²çº¿ï¼ˆæ•´åˆç‰ˆæœ¬ï¼šè°ƒç”¨ä¸‰ä¸ªå­æ¨¡å—ï¼‰\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;124;03mè¿™ä¸ªå‡½æ•°æ•´åˆäº†ï¼š\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03mä½¿ç”¨æ‰‹å†™CVå¾ªçŽ¯ï¼Œé¿å…sklearnçš„learning_curveå‡½æ•°å¯¼è‡´çš„ç±»åž‹æ£€æµ‹é—®é¢˜ã€‚\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# 1. è®¡ç®—å­¦ä¹ æ›²çº¿æ•°æ®\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m lc_data \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_learning_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuild_model_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_model_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures_no_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_no_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocessor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocessor_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessor_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresnet_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet_layers\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lc_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Dev/Landuse_Zhong_clean/function/learning_curve.py:278\u001b[0m, in \u001b[0;36mcompute_learning_curve\u001b[0;34m(build_model_fn, X_raw, y, features_no_coords, preprocessor_class, preprocessor_instance, train_sizes, cv_splits, epochs, batch_size, learning_rate, hidden_layers, dropout_rate, scoring, random_state, transformer_config, resnet_layers)\u001b[0m\n\u001b[1;32m    275\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m make_complete_pipeline()\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# âœ… è®­ç»ƒ pipeline\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# âœ… ä½¿ç”¨è‡ªå®šä¹‰ scorer è®¡ç®—åˆ†æ•°\u001b[39;00m\n\u001b[1;32m    281\u001b[0m train_scores[i, fold] \u001b[38;5;241m=\u001b[39m scorer_fn(pipeline, X_train, y_train)\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/sklearn/pipeline.py:663\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    658\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    659\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    660\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    661\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    662\u001b[0m         )\n\u001b[0;32m--> 663\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/scikeras/wrappers.py:1501\u001b[0m, in \u001b[0;36mKerasClassifier.fit\u001b[0;34m(self, X, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight\n\u001b[1;32m   1500\u001b[0m     sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight, y\u001b[38;5;241m=\u001b[39my)\n\u001b[0;32m-> 1501\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/scikeras/wrappers.py:770\u001b[0m, in \u001b[0;36mBaseWrapper.fit\u001b[0;34m(self, X, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit__epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs)\n\u001b[1;32m    767\u001b[0m )\n\u001b[1;32m    768\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 770\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarm_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarm_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/scikeras/wrappers.py:938\u001b[0m, in \u001b[0;36mBaseWrapper._fit\u001b[0;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_encoder_\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_model_compatibility(y)\n\u001b[0;32m--> 938\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_keras_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarm_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/scikeras/wrappers.py:533\u001b[0m, in \u001b[0;36mBaseWrapper._fit_keras_model\u001b[0;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tensorflow_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_state):\n\u001b[0;32m--> 533\u001b[0m         hist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    535\u001b[0m     hist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_args)\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    398\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[0;32m--> 399\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    239\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    240\u001b[0m     ):\n\u001b[0;32m--> 241\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1703\u001b[0m   )\n",
      "File \u001b[0;32m~/bayes-gpu/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = run_sensitivity_analysis(\n",
    "    df_positive=df_embedding_fill,  \n",
    "    df_prediction_pool=df_abandon_filtered,  \n",
    "    features_no_coords=features_no_coords,\n",
    "    param_combinations=param_combinations,\n",
    "    base_configs=base_configs,\n",
    "    epochs=80,\n",
    "    batch_size=256,  \n",
    "    random_state=42,\n",
    "    test_size=0.2,\n",
    "    val_size=0.2,\n",
    "    negative_strategy='generation',  \n",
    "    negative_ratio=1,  \n",
    "    plot_learning_curve=True,\n",
    "    learning_curve_epochs=20,\n",
    "    output_dir='Supplymentary/ML_sensitivity', \n",
    "    save_models=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ee0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. ç»“æžœæ•´ç†ä¸ºDataFrame\n",
    "def create_dataframes(results):\n",
    "    \"\"\"\n",
    "    å°†ç»“æžœåˆ—è¡¨è½¬æ¢ä¸ºå¤šä¸ªDataFrame\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - results: ç»“æžœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«å‚æ•°å’ŒæŒ‡æ ‡çš„å­—å…¸\n",
    "    \n",
    "    è¿”å›ž:\n",
    "    - dataframes: åŒ…å«6ä¸ªDataFrameçš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨æ›´æ–°åŽçš„å‚æ•°ç»“æž„ï¼‰\n",
    "    data = []\n",
    "    for r in results:\n",
    "        row = {\n",
    "            'd_model': r.get('d_model', np.nan),  # Transformerå®½åº¦\n",
    "            'num_layers': r.get('num_layers', np.nan),  # Transformeræ·±åº¦\n",
    "            'resnet_width': r.get('_resnet_width', np.nan),  # ResNetå®½åº¦ï¼ˆç¬¬ä¸€å±‚ï¼‰\n",
    "            'resnet_depth': r.get('_resnet_depth', np.nan),  # ResNetæ·±åº¦ï¼ˆå±‚æ•°ï¼‰\n",
    "            'resnet_layers_str': str(r.get('resnet_layers', [])),  # å®Œæ•´ResNeté…ç½®ï¼ˆå­—ç¬¦ä¸²ï¼‰\n",
    "            'learning_rate': r.get('learning_rate', np.nan),\n",
    "            'dropout_rate': r.get('dropout_rate', np.nan),\n",
    "            'num_heads': r.get('num_heads', np.nan),  # è‡ªåŠ¨è®¡ç®—çš„æ³¨æ„åŠ›å¤´æ•°\n",
    "            'overfitting_score': r.get('overfitting_score', np.nan),\n",
    "            'accuracy': r.get('accuracy', np.nan),\n",
    "            'precision': r.get('precision', np.nan),\n",
    "            'recall': r.get('recall', np.nan),\n",
    "            'f1': r.get('f1', np.nan),\n",
    "            'mean_probability': r.get('mean_probability', np.nan)\n",
    "        }\n",
    "        data.append(row)\n",
    "    \n",
    "    df_all = pd.DataFrame(data)\n",
    "    \n",
    "    # åˆ›å»ºå„ä¸ªæŒ‡æ ‡çš„DataFrame\n",
    "    # å‚æ•°åˆ—ï¼šå®½åº¦ã€æ·±åº¦ã€å­¦ä¹ çŽ‡ã€dropout\n",
    "    param_cols = [\n",
    "        'd_model',           # Transformerå®½åº¦\n",
    "        'num_layers',        # Transformeræ·±åº¦\n",
    "        'resnet_width',      # ResNetå®½åº¦\n",
    "        'resnet_depth',      # ResNetæ·±åº¦\n",
    "        'learning_rate',     # å­¦ä¹ çŽ‡\n",
    "        'dropout_rate'       # DropoutçŽ‡\n",
    "    ]\n",
    "    \n",
    "    dataframes = {\n",
    "        'df_overfitting': df_all[param_cols + ['overfitting_score']].copy(),\n",
    "        'df_accuracy': df_all[param_cols + ['accuracy']].copy(),\n",
    "        'df_precision': df_all[param_cols + ['precision']].copy(),\n",
    "        'df_recall': df_all[param_cols + ['recall']].copy(),\n",
    "        'df_f1': df_all[param_cols + ['f1']].copy(),\n",
    "        'df_mean_prob': df_all[param_cols + ['mean_probability']].copy()\n",
    "    }\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "# åˆ›å»ºDataFrame\n",
    "dataframes = create_dataframes(results)\n",
    "\n",
    "print(\"âœ… DataFrameåˆ›å»ºå®Œæˆ\")\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"  - {name}: {df.shape}\")\n",
    "    print(f\"    ç¼ºå¤±å€¼: {df.isna().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097197b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. ä¿å­˜ç»“æžœ\n",
    "output_dir = 'Supplymentary/ML_sensitivity'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ä¿å­˜å„ä¸ªDataFrame\n",
    "for name, df in dataframes.items():\n",
    "    filepath = os.path.join(output_dir, f\"{name}.csv\")\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"âœ… å·²ä¿å­˜: {filepath}\")\n",
    "\n",
    "# ä¿å­˜æ­£äº¤è¡¨é…ç½®ä¿¡æ¯\n",
    "config_info = {\n",
    "    'orthogonal_array': orthogonal_array.tolist(),\n",
    "    'sensitivity_configs': {k: [str(v) if isinstance(v, list) else v for v in vals] \n",
    "                            for k, vals in sensitivity_configs.items()},\n",
    "    'base_configs': {k: (str(v) if isinstance(v, list) else v) \n",
    "                     for k, v in base_configs.items()},\n",
    "    'num_experiments': len(param_combinations),\n",
    "    'full_factorial_experiments': 3**6\n",
    "}\n",
    "\n",
    "config_filepath = os.path.join(output_dir, 'orthogonal_config.json')\n",
    "with open(config_filepath, 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_info, f, indent=2, ensure_ascii=False)\n",
    "print(f\"âœ… å·²ä¿å­˜é…ç½®ä¿¡æ¯: {config_filepath}\")\n",
    "\n",
    "# ä¿å­˜å®Œæ•´ç»“æžœæ‘˜è¦\n",
    "summary = {\n",
    "    'total_experiments': len(results),\n",
    "    'successful_experiments': sum(1 for r in results if not np.isnan(r.get('accuracy', np.nan))),\n",
    "    'failed_experiments': sum(1 for r in results if np.isnan(r.get('accuracy', np.nan))),\n",
    "    'best_accuracy': max([r.get('accuracy', -np.inf) for r in results], default=np.nan),\n",
    "    'best_f1': max([r.get('f1', -np.inf) for r in results], default=np.nan),\n",
    "    'best_params_accuracy': None,\n",
    "    'best_params_f1': None\n",
    "}\n",
    "\n",
    "# æ‰¾åˆ°æœ€ä½³å‚æ•°ç»„åˆ\n",
    "if not np.isnan(summary['best_accuracy']):\n",
    "    best_acc_idx = np.argmax([r.get('accuracy', -np.inf) for r in results])\n",
    "    summary['best_params_accuracy'] = {k: v for k, v in results[best_acc_idx].items() \n",
    "                                       if k not in ['overfitting_score', 'accuracy', 'precision', 'recall', 'f1', 'mean_probability']}\n",
    "\n",
    "if not np.isnan(summary['best_f1']):\n",
    "    best_f1_idx = np.argmax([r.get('f1', -np.inf) for r in results])\n",
    "    summary['best_params_f1'] = {k: v for k, v in results[best_f1_idx].items() \n",
    "                                 if k not in ['overfitting_score', 'accuracy', 'precision', 'recall', 'f1', 'mean_probability']}\n",
    "\n",
    "summary_filepath = os.path.join(output_dir, 'sensitivity_summary.json')\n",
    "with open(summary_filepath, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False, default=str)\n",
    "print(f\"âœ… å·²ä¿å­˜ç»“æžœæ‘˜è¦: {summary_filepath}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"æ‰€æœ‰ç»“æžœå·²ä¿å­˜åˆ°: {output_dir}\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc9f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bayes-gpu GPU)",
   "language": "python",
   "name": "bayes-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
