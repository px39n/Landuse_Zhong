{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3108c3",
   "metadata": {},
   "source": [
    "# 1、Supplementary Figures of training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f69153",
   "metadata": {},
   "source": [
    "\n",
    "我的核心目的是：根据保存的模型内容（尤其是学习曲线与训练精度等）重新绘制Complete Pipeline Analysis函数与plot_learning_curve_nn的训练曲线；\n",
    "\n",
    "第一，请结合3.0 pre-training.ipynb文件，写一个保存结果的检测函数，若检测后发现有关键数据没有保存，请完善当前的save_complete_model_pipeline，并且我需要重新启动训练\n",
    "\n",
    "\n",
    "模块一：学习曲线模块plot_learning_curve_nn的数据流（核心是检测训练集、验证集之间的均值与方差关系，以此判断过拟合的模式）；目前有部分数据没有返回进result（也就是lc_analysis中）\n",
    "（1）由plot_learning_curve_nn通过StratifiedKFold创建的cv、输入learning_curve所生成的train_sizes_abs, train_scores, val_scores；\n",
    "（2）有了以上数据后，可以跨cv折求均值/方差，主要包括train_mean，train_std，val_mean，val_std，以及有关analyze_overfitting产生的overfitting_analysis字典。\n",
    "\n",
    "\n",
    "模块二：训练残差模块plot_training_results基础指标的数据流（核心是关注随着训练次数的增加），这部分数据应该主要集中在training_results中\n",
    "\n",
    "（1）仅在调用深度学习模型的时候生成的history数据（根据model对象所具有的fit方法所产生的history对象）；主要包含了history.history['loss'],history.history['val_loss'], history.history['accuracy'],history.history['val_accuracy']\n",
    "以上数据可以用于绘制随着epochs增长的loss下降、Traning Accuracy，必要时可以保存各个CV的完整训练数据\n",
    "\n",
    "\n",
    "（2）ROC曲线、测试集test的概率预测分布、混淆矩阵所需要的以下数据\n",
    "            y_test_pred = model.predict(X_test, verbose=0).ravel()\n",
    "            y_test_bin = (y_test_pred > 0.5).astype(int)\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_test_pred)\n",
    "            test_auc = auc(fpr, tpr)\n",
    "            test_metrics = {\n",
    "                'accuracy': accuracy_score(y_test, y_test_bin),\n",
    "                'precision': precision_score(y_test, y_test_bin),\n",
    "                'recall': recall_score(y_test, y_test_bin),\n",
    "                'f1': f1_score(y_test, y_test_bin),\n",
    "                'auc': test_auc\n",
    "            }\n",
    "以上数据可以用于绘制ROC曲线、混淆矩阵cm、测试集的概率分布，这里以ROC曲线为例：\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        axes[1].plot(fpr, tpr, label=f'{model_name} (AUC={metrics[\"auc\"]:.3f})', linewidth=2)\n",
    "\n",
    "模块三：完整管线评估plot_complete_pipeline_results的性能指标数据流\n",
    "主要数据来源于train_and_evaluate_model函数生成的training_results\n",
    "    metrics_names = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    train_values = [training_results['metrics']['train'][m] for m in metrics_names]\n",
    "    val_values = [training_results['metrics']['val'][m] for m in metrics_names]\n",
    "    test_values = [training_results['metrics']['test'][m] for m in metrics_names]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7eae5a",
   "metadata": {},
   "source": [
    "```\n",
    "data/US_data/ML_model/\n",
    "├── {model_name}.pkl                    # 主模型文件（包含所有路径引用）\n",
    "├── {model_name}_config.json            # 完整配置文件（包含训练历史、学习曲线等）\n",
    "├── {model_name}_gmm.pkl                # GMM模型\n",
    "├── {model_name}_dl.h5                  # 深度学习模型\n",
    "├── {model_name}_preprocessor.pkl       # 预处理器\n",
    "├── {model_name}_test_data.npz          # 测试数据（包含ROC数据）\n",
    "├── results_{model_type}_{timestamp}.csv # 预测结果\n",
    "└── summary_{model_type}_{timestamp}.json # 轻量级摘要（可选）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65fd1a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "项目根目录: C:\\Dev\\Landuse_Zhong_clean\n",
      "数据路径: C:\\Dev\\Landuse_Zhong_clean\\data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 最可靠的方法：查找包含data和function目录的项目根目录\n",
    "def find_project_root(start_path=None):\n",
    "    \"\"\"查找项目根目录（包含data和function目录的目录）\"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = Path.cwd()\n",
    "    \n",
    "    current = Path(start_path).resolve()\n",
    "    \n",
    "    # 向上查找，直到找到包含data和function目录的目录\n",
    "    for _ in range(5):  # 最多向上查找5层\n",
    "        if (current / 'data').exists() and (current / 'function').exists():\n",
    "            return current\n",
    "        parent = current.parent\n",
    "        if parent == current:  # 到达根目录\n",
    "            break\n",
    "        current = parent\n",
    "    \n",
    "    # 如果找不到，假设当前目录的父目录是项目根目录\n",
    "    return Path.cwd().parent\n",
    "\n",
    "project_root = find_project_root()\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "DATA_PATH = project_root / 'data'\n",
    "\n",
    "print(f\"项目根目录: {project_root}\")\n",
    "print(f\"数据路径: {DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56940280",
   "metadata": {},
   "source": [
    "## 1.1 Check data for plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d24ff9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def check_saved_model_data_completeness(main_model_file):\n",
    "    \"\"\"\n",
    "    检测保存的模型数据是否完整，用于后续绘图\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        保存的模型主文件路径（.pkl文件），支持相对路径和绝对路径\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    report : dict\n",
    "        检测报告，包含：\n",
    "        - 'status': 'complete' | 'incomplete' | 'error'\n",
    "        - 'missing_data': list of missing data keys\n",
    "        - 'details': dict with detailed check results\n",
    "        - 'file_paths': dict with resolved file paths\n",
    "        - 'data_summary': dict with data shape/size information\n",
    "    \"\"\"\n",
    "    import joblib\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    report = {\n",
    "        'status': 'unknown',\n",
    "        'missing_data': [],\n",
    "        'details': {},\n",
    "        'recommendations': [],\n",
    "        'file_paths': {},\n",
    "        'data_summary': {}\n",
    "    }\n",
    "    try:\n",
    "        main_model_file = os.path.abspath(os.path.normpath(main_model_file))\n",
    "        report['file_paths']['main_model'] = main_model_file\n",
    "        if not os.path.exists(main_model_file):\n",
    "            report['status'] = 'error'\n",
    "            report['error'] = f\"模型文件不存在: {main_model_file}\"\n",
    "            return report\n",
    "        try:\n",
    "            main_model = joblib.load(main_model_file)\n",
    "        except Exception as e:\n",
    "            report['status'] = 'error'\n",
    "            report['error'] = f\"无法加载模型文件: {str(e)}\"\n",
    "            return report\n",
    "        config_path = main_model.get('config_path')\n",
    "        test_data_path = main_model.get('test_data_path')\n",
    "        def resolve_path(path, base_dir):\n",
    "            if not path:\n",
    "                return None\n",
    "            if os.path.isabs(path):\n",
    "                return os.path.normpath(path)\n",
    "            full_path = os.path.normpath(os.path.join(base_dir, path))\n",
    "            if os.path.exists(full_path):\n",
    "                return full_path\n",
    "            filename = os.path.basename(path)\n",
    "            fallback_path = os.path.normpath(os.path.join(base_dir, filename))\n",
    "            return fallback_path\n",
    "        model_dir = os.path.dirname(main_model_file)\n",
    "        if not config_path:\n",
    "            report['status'] = 'error'\n",
    "            report['error'] = \"配置路径未在主模型文件中指定\"\n",
    "            return report\n",
    "        config_path = resolve_path(config_path, model_dir)\n",
    "        report['file_paths']['config'] = config_path\n",
    "        if test_data_path:\n",
    "            test_data_path = resolve_path(test_data_path, model_dir)\n",
    "            report['file_paths']['test_data'] = test_data_path\n",
    "        if not os.path.exists(config_path):\n",
    "            report['status'] = 'error'\n",
    "            report['error'] = f\"配置文件不存在: {config_path}\"\n",
    "            report['recommendations'].append(f\"主模型文件目录: {model_dir}\")\n",
    "            report['recommendations'].append(f\"原始配置路径: {main_model.get('config_path')}\")\n",
    "            return report\n",
    "        try:\n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                config = json.load(f)\n",
    "        except Exception as e:\n",
    "            report['status'] = 'error'\n",
    "            report['error'] = f\"无法加载配置文件: {str(e)}\"\n",
    "            return report\n",
    "        test_data = None\n",
    "        if test_data_path and os.path.exists(test_data_path):\n",
    "            try:\n",
    "                test_data = np.load(test_data_path, allow_pickle=True)\n",
    "            except Exception as e:\n",
    "                report['warnings'] = report.get('warnings', [])\n",
    "                report['warnings'].append(f\"无法加载测试数据: {str(e)}\")\n",
    "        elif test_data_path:\n",
    "            report['warnings'] = report.get('warnings', [])\n",
    "            report['warnings'].append(f\"测试数据文件不存在: {test_data_path}\")\n",
    "        # 学习曲线\n",
    "        lc_check = {\n",
    "            'has_learning_curve': False,\n",
    "            'missing_keys': [],\n",
    "            'empty_keys': [],\n",
    "            'required_keys': [\n",
    "                'train_sizes',\n",
    "                'train_scores_mean',\n",
    "                'train_scores_std',\n",
    "                'val_scores_mean',\n",
    "                'val_scores_std',\n",
    "                'overfitting_analysis'\n",
    "            ]\n",
    "        }\n",
    "        if 'learning_curve_analysis' in config:\n",
    "            lc_data = config['learning_curve_analysis']\n",
    "            lc_check['has_learning_curve'] = True\n",
    "            for key in lc_check['required_keys']:\n",
    "                if key not in lc_data:\n",
    "                    lc_check['missing_keys'].append(key)\n",
    "                    report['missing_data'].append(f'learning_curve.{key}')\n",
    "                else:\n",
    "                    value = lc_data[key]\n",
    "                    if value is None:\n",
    "                        lc_check['empty_keys'].append(key)\n",
    "                        report['missing_data'].append(f'learning_curve.{key} (empty)')\n",
    "                    elif isinstance(value, (list, np.ndarray)):\n",
    "                        try:\n",
    "                            if len(value) == 0:\n",
    "                                lc_check['empty_keys'].append(key)\n",
    "                                report['missing_data'].append(f'learning_curve.{key} (empty)')\n",
    "                        except (TypeError, ValueError):\n",
    "                            pass\n",
    "            if 'train_sizes' in lc_data and lc_data['train_sizes']:\n",
    "                report['data_summary']['learning_curve'] = {\n",
    "                    'n_train_sizes': len(lc_data['train_sizes']),\n",
    "                    'train_sizes_range': [min(lc_data['train_sizes']), max(lc_data['train_sizes'])]\n",
    "                }\n",
    "        else:\n",
    "            lc_check['missing_keys'] = lc_check['required_keys']\n",
    "            report['missing_data'].extend([f'learning_curve.{k}' for k in lc_check['required_keys']])\n",
    "        report['details']['learning_curve'] = lc_check\n",
    "        # 训练历史\n",
    "        history_check = {\n",
    "            'has_history': False,\n",
    "            'missing_keys': [],\n",
    "            'empty_keys': [],\n",
    "            'required_keys': ['loss', 'val_loss', 'accuracy', 'val_accuracy']\n",
    "        }\n",
    "        if 'training_history' in config:\n",
    "            history_data = config['training_history']\n",
    "            history_check['has_history'] = True\n",
    "            for key in history_check['required_keys']:\n",
    "                if key not in history_data:\n",
    "                    history_check['missing_keys'].append(key)\n",
    "                    report['missing_data'].append(f'training_history.{key}')\n",
    "                else:\n",
    "                    value = history_data[key]\n",
    "                    if value is None:\n",
    "                        history_check['empty_keys'].append(key)\n",
    "                        report['missing_data'].append(f'training_history.{key} (empty)')\n",
    "                    elif isinstance(value, list):\n",
    "                        try:\n",
    "                            if len(value) == 0:\n",
    "                                history_check['empty_keys'].append(key)\n",
    "                                report['missing_data'].append(f'training_history.{key} (empty)')\n",
    "                        except (TypeError, ValueError):\n",
    "                            pass\n",
    "            if 'loss' in history_data and history_data['loss']:\n",
    "                report['data_summary']['training_history'] = {\n",
    "                    'n_epochs': len(history_data['loss']),\n",
    "                    'final_loss': history_data['loss'][-1] if history_data['loss'] else None,\n",
    "                    'final_val_loss': history_data.get('val_loss', [None])[-1] if history_data.get('val_loss') else None\n",
    "                }\n",
    "        else:\n",
    "            history_check['missing_keys'] = history_check['required_keys']\n",
    "            report['missing_data'].extend([f'training_history.{k}' for k in history_check['required_keys']])\n",
    "        report['details']['training_history'] = history_check\n",
    "        # ROC曲线\n",
    "        roc_check = {\n",
    "            'has_roc_data': False,\n",
    "            'missing_keys': [],\n",
    "            'empty_keys': [],\n",
    "            'required_keys': ['fpr', 'tpr', 'test_auc', 'y_test_pred']\n",
    "        }\n",
    "        if test_data is not None:\n",
    "            for key in roc_check['required_keys']:\n",
    "                if key not in test_data:\n",
    "                    roc_check['missing_keys'].append(key)\n",
    "                    report['missing_data'].append(f'roc_data.{key}')\n",
    "                else:\n",
    "                    data_item = test_data[key]\n",
    "                    if data_item is None:\n",
    "                        roc_check['empty_keys'].append(key)\n",
    "                        report['missing_data'].append(f'roc_data.{key} (empty)')\n",
    "                    elif hasattr(data_item, '__len__') and not isinstance(data_item, (str, int, float, bool)):\n",
    "                        try:\n",
    "                            if len(data_item) == 0:\n",
    "                                roc_check['empty_keys'].append(key)\n",
    "                                report['missing_data'].append(f'roc_data.{key} (empty)')\n",
    "                        except (TypeError, ValueError):\n",
    "                            pass\n",
    "            if len(roc_check['missing_keys']) == 0 and len(roc_check['empty_keys']) == 0:\n",
    "                roc_check['has_roc_data'] = True\n",
    "                if 'test_auc' in test_data:\n",
    "                    report['data_summary']['roc_data'] = {\n",
    "                        'test_auc': float(test_data['test_auc']) if test_data['test_auc'] is not None else None,\n",
    "                        'fpr_length': len(test_data['fpr']) if 'fpr' in test_data else None,\n",
    "                        'tpr_length': len(test_data['tpr']) if 'tpr' in test_data else None\n",
    "                    }\n",
    "        else:\n",
    "            roc_check['missing_keys'] = roc_check['required_keys']\n",
    "            report['missing_data'].extend([f'roc_data.{k}' for k in roc_check['required_keys']])\n",
    "            if not test_data_path:\n",
    "                report['warnings'] = report.get('warnings', [])\n",
    "                report['warnings'].append(\"测试数据路径未指定\")\n",
    "            elif not os.path.exists(test_data_path):\n",
    "                report['warnings'] = report.get('warnings', [])\n",
    "                report['warnings'].append(f\"测试数据文件不存在: {test_data_path}\")\n",
    "        report['details']['roc_data'] = roc_check\n",
    "        # 训练集/测试集数据\n",
    "        metrics_check = {\n",
    "            'has_metrics': False,\n",
    "            'missing_keys': [],\n",
    "            'required_keys': ['train', 'val', 'test']\n",
    "        }\n",
    "        if 'training_metrics' in config:\n",
    "            metrics_data = config['training_metrics']\n",
    "            metrics_check['has_metrics'] = True\n",
    "            for split in metrics_check['required_keys']:\n",
    "                if split not in metrics_data:\n",
    "                    metrics_check['missing_keys'].append(split)\n",
    "                    report['missing_data'].append(f'metrics.{split}')\n",
    "                else:\n",
    "                    required_metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "                    for metric in required_metrics:\n",
    "                        if metric not in metrics_data[split]:\n",
    "                            report['missing_data'].append(f'metrics.{split}.{metric}')\n",
    "        else:\n",
    "            metrics_check['missing_keys'] = metrics_check['required_keys']\n",
    "            report['missing_data'].extend([f'metrics.{k}' for k in metrics_check['required_keys']])\n",
    "        report['details']['metrics'] = metrics_check\n",
    "        test_data_check = {\n",
    "            'has_test_data': False,\n",
    "            'missing_keys': [],\n",
    "            'empty_keys': [],\n",
    "            'required_keys': ['X_test', 'y_test', 'X_train', 'y_train', 'X_val', 'y_val']\n",
    "        }\n",
    "        if test_data is not None:\n",
    "            for key in test_data_check['required_keys']:\n",
    "                if key not in test_data:\n",
    "                    test_data_check['missing_keys'].append(key)\n",
    "                    report['missing_data'].append(f'test_data.{key}')\n",
    "                else:\n",
    "                    data_item = test_data[key]\n",
    "                    if data_item is None:\n",
    "                        test_data_check['empty_keys'].append(key)\n",
    "                        report['missing_data'].append(f'test_data.{key} (empty)')\n",
    "                    elif hasattr(data_item, '__len__') and not isinstance(data_item, (str, int, float, bool)):\n",
    "                        try:\n",
    "                            if len(data_item) == 0:\n",
    "                                test_data_check['empty_keys'].append(key)\n",
    "                                report['missing_data'].append(f'test_data.{key} (empty)')\n",
    "                        except (TypeError, ValueError):\n",
    "                            pass\n",
    "            if len(test_data_check['missing_keys']) == 0 and len(test_data_check['empty_keys']) == 0:\n",
    "                test_data_check['has_test_data'] = True\n",
    "                if 'X_test' in test_data and test_data['X_test'] is not None:\n",
    "                    X_test = test_data['X_test']\n",
    "                    report['data_summary']['test_data'] = {\n",
    "                        'X_test_shape': X_test.shape if hasattr(X_test, 'shape') else f\"len={len(X_test)}\",\n",
    "                        'y_test_shape': test_data['y_test'].shape if 'y_test' in test_data and hasattr(test_data['y_test'], 'shape') else None\n",
    "                    }\n",
    "        else:\n",
    "            test_data_check['missing_keys'] = test_data_check['required_keys']\n",
    "            report['missing_data'].extend([f'test_data.{k}' for k in test_data_check['required_keys']])\n",
    "        report['details']['test_data'] = test_data_check\n",
    "        if len(report['missing_data']) == 0:\n",
    "            report['status'] = 'complete'\n",
    "            report['recommendations'].append(\"✅ 所有必需数据都已保存，可以正常绘图\")\n",
    "        else:\n",
    "            report['status'] = 'incomplete'\n",
    "            if not history_check['has_history']:\n",
    "                report['recommendations'].append(\n",
    "                    \"⚠️ 缺少训练历史数据。需要在 save_complete_model_pipeline 中保存 history.history\"\n",
    "                )\n",
    "            elif history_check.get('empty_keys'):\n",
    "                report['recommendations'].append(\n",
    "                    f\"⚠️ 训练历史数据存在但为空: {', '.join(history_check['empty_keys'])}\"\n",
    "                )\n",
    "            if not lc_check['has_learning_curve']:\n",
    "                report['recommendations'].append(\n",
    "                    \"⚠️ 缺少学习曲线数据。需要确保 plot_learning_curve_nn 返回完整结果并保存\"\n",
    "                )\n",
    "            elif lc_check.get('empty_keys'):\n",
    "                report['recommendations'].append(\n",
    "                    f\"⚠️ 学习曲线数据存在但为空: {', '.join(lc_check['empty_keys'])}\"\n",
    "                )\n",
    "            if not roc_check['has_roc_data']:\n",
    "                report['recommendations'].append(\n",
    "                    \"⚠️ 缺少ROC曲线数据。需要在保存测试数据时添加 fpr, tpr, test_auc, y_test_pred\"\n",
    "                )\n",
    "            elif roc_check.get('empty_keys'):\n",
    "                report['recommendations'].append(\n",
    "                    f\"⚠️ ROC曲线数据存在但为空: {', '.join(roc_check['empty_keys'])}\"\n",
    "                )\n",
    "        return report\n",
    "    except Exception as e:\n",
    "        report['status'] = 'error'\n",
    "        report['error'] = str(e)\n",
    "        import traceback\n",
    "        report['traceback'] = traceback.format_exc()\n",
    "        return report\n",
    "\n",
    "# ============================================================\n",
    "# 查找并检测模型，无打印信息\n",
    "# ============================================================\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def find_latest_model(pattern='landuse_transformer_generation*.pkl', search_dir='../data/US_data/ML_model'):\n",
    "    abs_search_dir = os.path.abspath(os.path.normpath(search_dir))\n",
    "    abs_pattern = os.path.join(abs_search_dir, pattern)\n",
    "    model_files = glob.glob(abs_pattern)\n",
    "    if model_files:\n",
    "        model_files.sort(key=os.path.getmtime, reverse=True)\n",
    "        return model_files[0], len(model_files)\n",
    "    return None, 0\n",
    "\n",
    "model_file, n_files = find_latest_model()\n",
    "\n",
    "if model_file:\n",
    "    report = check_saved_model_data_completeness(model_file)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e99cae4",
   "metadata": {},
   "source": [
    "## 1.2 Plot for training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21085f78",
   "metadata": {},
   "source": [
    "### 1.2.1 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70347dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c:\\\\Dev\\\\Landuse_Zhong_clean\\\\data\\\\US_data\\\\ML_model\\\\landuse_transformer_generation_single_20251124_234807.pkl',\n",
       "  6),\n",
       " ('c:\\\\Dev\\\\Landuse_Zhong_clean\\\\data\\\\US_data\\\\ML_model\\\\landuse_mlp_generation_single_20251210_172430.pkl',\n",
       "  6)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def resolve_path(path, base_dir):\n",
    "    \"\"\"解析路径：绝对路径直接使用，相对路径基于base_dir拼接\"\"\"\n",
    "    if not path:\n",
    "        return None\n",
    "    if os.path.isabs(path):\n",
    "        return os.path.normpath(path)\n",
    "    # 相对路径：先尝试直接拼接\n",
    "    full_path = os.path.normpath(os.path.join(base_dir, path))\n",
    "    if os.path.exists(full_path):\n",
    "        return full_path\n",
    "    filename = os.path.basename(path)\n",
    "    return os.path.normpath(os.path.join(base_dir, filename))\n",
    "\n",
    "def load_model_data(main_model_file):\n",
    "    \"\"\"\n",
    "    加载保存的模型数据\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        主模型文件路径（.pkl文件）\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    data : dict\n",
    "        包含所有模型数据的字典\n",
    "    \"\"\"\n",
    "    import joblib\n",
    "    import json\n",
    "    import numpy as np\n",
    "    \n",
    "    main_model_file = os.path.abspath(os.path.normpath(main_model_file))\n",
    "    model_dir = os.path.dirname(main_model_file)\n",
    "    \n",
    "    main_model = joblib.load(main_model_file)\n",
    "    config_path = resolve_path(main_model.get('config_path'), model_dir)\n",
    "    test_data_path = resolve_path(main_model.get('test_data_path'), model_dir)\n",
    "    \n",
    "    # 加载配置文件\n",
    "    if not config_path or not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"配置文件不存在: {config_path}\")\n",
    "    \n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # 加载测试数据\n",
    "    test_data = None\n",
    "    if test_data_path and os.path.exists(test_data_path):\n",
    "        test_data = np.load(test_data_path, allow_pickle=True)\n",
    "    \n",
    "    return {\n",
    "        'config': config,\n",
    "        'test_data': test_data,\n",
    "        'main_model': main_model\n",
    "    }\n",
    "\n",
    "import glob\n",
    "\n",
    "try:\n",
    "    _ = find_latest_model\n",
    "except NameError:\n",
    "    def find_latest_model(pattern, search_dir='../data/US_data/ML_model'):\n",
    "        \"\"\"查找指定pattern的最新模型文件\"\"\"\n",
    "        abs_search_dir = os.path.abspath(os.path.normpath(search_dir))\n",
    "        abs_pattern = os.path.join(abs_search_dir, pattern)\n",
    "        model_files = glob.glob(abs_pattern)\n",
    "        if model_files:\n",
    "            model_files.sort(key=os.path.getmtime, reverse=True)\n",
    "            return model_files[0]\n",
    "        return None\n",
    "\n",
    "# 查找两类模型的最新文件\n",
    "model_patterns = [\n",
    "    'landuse_transformer_generation*.pkl',\n",
    "    'landuse_mlp_generation_single*.pkl'\n",
    "]\n",
    "\n",
    "latest_model_files = []\n",
    "for pattern in model_patterns:\n",
    "    latest_file = find_latest_model(pattern)\n",
    "    latest_model_files.append(latest_file)\n",
    "\n",
    "latest_model_files  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698735c2",
   "metadata": {},
   "source": [
    "### 1.2.2 Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "663c1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "figsize_mm = 60\n",
    "figsize_inches = figsize_mm / 25.4\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 5,\n",
    "    'axes.titlesize': 5,\n",
    "    'axes.labelsize': 5,\n",
    "    'xtick.labelsize': 5,\n",
    "    'ytick.labelsize': 5,\n",
    "    'legend.fontsize': 5,\n",
    "    'font.family': 'Arial'\n",
    "})\n",
    "\n",
    "\n",
    "def plot_learning_curve(main_model_file, save_dir='Supplymentary_figure/Transformer_performance', panel_label=None):\n",
    "    \"\"\"\n",
    "    绘制学习曲线图\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        主模型文件路径\n",
    "    save_dir : str\n",
    "        保存目录\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    config = data['config']\n",
    "\n",
    "    lc_data = config['learning_curve_analysis']\n",
    "    train_sizes = np.array(lc_data.get('train_sizes', []))\n",
    "    train_scores_mean = np.array(lc_data.get('train_scores_mean', []))\n",
    "    train_scores_std = np.array(lc_data.get('train_scores_std', []))\n",
    "    val_scores_mean = np.array(lc_data.get('val_scores_mean', []))\n",
    "    val_scores_std = np.array(lc_data.get('val_scores_std', []))\n",
    "\n",
    "\n",
    "    valid_indices = []\n",
    "    for arr in [train_sizes, train_scores_mean, train_scores_std, val_scores_mean, val_scores_std]:\n",
    "        if len(arr) > 0:\n",
    "            # 找到最后一个非NaN的索引\n",
    "            valid_mask = ~np.isnan(arr)\n",
    "            if valid_mask.any():\n",
    "                valid_indices.append(np.where(valid_mask)[0][-1])\n",
    "    \n",
    "    # 取所有数组中最小的有效索引（确保所有数组都有数据）\n",
    "    if valid_indices:\n",
    "        max_valid_idx = min(valid_indices) + 1  # +1 因为切片是 [0:max_valid_idx+1]\n",
    "    else:\n",
    "        max_valid_idx = len(train_sizes)\n",
    "    \n",
    "    # 只保留有效数据\n",
    "    train_sizes = train_sizes[:max_valid_idx]\n",
    "    train_scores_mean = train_scores_mean[:max_valid_idx]\n",
    "    train_scores_std = train_scores_std[:max_valid_idx]\n",
    "    val_scores_mean = val_scores_mean[:max_valid_idx]\n",
    "    val_scores_std = val_scores_std[:max_valid_idx]\n",
    "\n",
    "    # 创建图形\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "\n",
    "    # 绘制训练集曲线（带误差条），不带marker\n",
    "    ax.plot(train_sizes, train_scores_mean, '-', color='#1F78B4', \n",
    "            label='Training Score', linewidth=1.5)\n",
    "    ax.fill_between(train_sizes, \n",
    "                    train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std,\n",
    "                    alpha=0.2, color='#1F78B4')\n",
    "\n",
    "    # 绘制验证集曲线（带误差条），不带marker\n",
    "    ax.plot(train_sizes, val_scores_mean, '-', color='#E31A1C',\n",
    "            label='Validation Score', linewidth=1.5)\n",
    "    ax.fill_between(train_sizes,\n",
    "                    val_scores_mean - val_scores_std,\n",
    "                    val_scores_mean + val_scores_std,\n",
    "                    alpha=0.2, color='#E31A1C')\n",
    "\n",
    "    ax.set_xlabel('Training Set Size', fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontweight='bold')\n",
    "    ax.set_title('Learning Curve', fontweight='bold')\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # 设置y轴范围，并确保用于arrow_y的y坐标和ylim一致\n",
    "    ylim_bottom = val_scores_mean.min() * 0.88\n",
    "    ylim_top = train_scores_mean.max() * 1.03\n",
    "    ax.set_ylim([ylim_bottom, ylim_top])\n",
    "\n",
    "    # 添加坐标轴箭头（与 Training Loss 保持一致）\n",
    "    arrow_x = FancyArrowPatch(\n",
    "        posA=(ax.get_xlim()[1] * 1.0, 0),\n",
    "        posB=(ax.get_xlim()[1] * 1.03, 0),\n",
    "        transform=ax.get_xaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_x.set_clip_on(False)\n",
    "    ax.add_patch(arrow_x)\n",
    "\n",
    "    arrow_y = FancyArrowPatch(\n",
    "        posA=(0, ylim_top * 1.0),\n",
    "        posB=(0, ylim_top * 1.01),\n",
    "        transform=ax.get_yaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_y.set_clip_on(False)\n",
    "    ax.add_patch(arrow_y)\n",
    "    ax.tick_params(axis='x', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    ax.tick_params(axis='y', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    \n",
    "    # 添加面板标签\n",
    "    if panel_label:\n",
    "        fig.text(\n",
    "            0.01, 0.99, panel_label, ha='left', va='top', fontsize=7, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', edgecolor='none', alpha=0.7, pad=0.2, lw=0), zorder=100\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_learning_curve.png')\n",
    "    fig.savefig(save_path, dpi=300,  format='png')\n",
    "    print(f\"✅ 学习曲线图已保存: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_overfitting_analysis(main_model_file, save_dir='Supplymentary_figure/Transformer_performance'):\n",
    "    \"\"\"\n",
    "    绘制过拟合分析图\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        主模型文件路径\n",
    "    save_dir : str\n",
    "        保存目录\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    config = data['config']\n",
    "\n",
    "    lc_data = config['learning_curve_analysis']\n",
    "    of_data = lc_data['overfitting_analysis']\n",
    "    train_sizes = np.array(lc_data.get('train_sizes', []))\n",
    "    train_scores_mean = np.array(lc_data.get('train_scores_mean', []))\n",
    "    val_scores_mean = np.array(lc_data.get('val_scores_mean', []))\n",
    "\n",
    "    # 计算训练集和验证集之间的差距\n",
    "    gap = train_scores_mean - val_scores_mean\n",
    "\n",
    "    # 创建图形\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "\n",
    "    # 绘制差距曲线，不带marker\n",
    "    ax.plot(train_sizes, gap, '-', color='#FF7F00', \n",
    "            linewidth=1.5, label='Train-Val Gap')\n",
    "\n",
    "    # 添加过拟合阈值线（如果有）\n",
    "    if 'overfitting_level' in of_data:\n",
    "        of_level = of_data['overfitting_level']\n",
    "        if of_level == 'high':\n",
    "            threshold = 0.1  # 示例阈值\n",
    "            ax.axhline(y=threshold, color='r', linestyle='--', \n",
    "                      linewidth=1, alpha=0.7, label='Overfitting Threshold')\n",
    "\n",
    "    ax.set_xlabel('Training Set Size', fontweight='bold')\n",
    "    ax.set_ylabel('Train-Val Score Gap', fontweight='bold')\n",
    "    ax.set_title('Overfitting Analysis', fontweight='bold')\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # 添加坐标轴箭头\n",
    "    arrow_x = FancyArrowPatch(\n",
    "        posA=(ax.get_xlim()[1] * 1.0, 0),\n",
    "        posB=(ax.get_xlim()[1] * 1.03, 0),\n",
    "        transform=ax.get_xaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_x.set_clip_on(False)\n",
    "    ax.add_patch(arrow_x)\n",
    "\n",
    "    y_lim = ax.get_ylim()\n",
    "    arrow_y = FancyArrowPatch(\n",
    "        posA=(0, y_lim[1] * 1.0),\n",
    "        posB=(0, y_lim[1] * 1.03),\n",
    "        transform=ax.get_yaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_y.set_clip_on(False)\n",
    "    ax.add_patch(arrow_y)\n",
    "    ax.tick_params(axis='x', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    ax.tick_params(axis='y', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_overfitting_analysis.png')\n",
    "    fig.savefig(save_path, dpi=300,  format='png')\n",
    "    print(f\"✅ 过拟合分析图已保存: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_variance_analysis(main_model_file, save_dir='Supplymentary_figure/Transformer_performance', panel_label=None):\n",
    "    \"\"\"\n",
    "    绘制方差分析图\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        主模型文件路径\n",
    "    save_dir : str\n",
    "        保存目录\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    config = data['config']\n",
    "\n",
    "    # 取消数据检查，直接提取数据（假设这些字段必定存在）\n",
    "    lc_data = config['learning_curve_analysis']\n",
    "    train_sizes = np.array(lc_data['train_sizes'])\n",
    "    train_scores_std = np.array(lc_data['train_scores_std'])\n",
    "    val_scores_std = np.array(lc_data['val_scores_std'])\n",
    "\n",
    "    # 创建图形\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "\n",
    "    # 绘制标准差曲线，不带marker\n",
    "    ax.plot(train_sizes, train_scores_std, '-', color='#1F78B4',\n",
    "            linewidth=1.5, label='Training Std')\n",
    "    ax.plot(train_sizes, val_scores_std, '-', color='#E31A1C',\n",
    "            linewidth=1.5, label='Validation Std')\n",
    "\n",
    "    ax.set_xlabel('Training Set Size', fontweight='bold')\n",
    "    ax.set_ylabel('Score Std Dev', fontweight='bold')\n",
    "    ax.set_title('Variance Analysis', fontweight='bold')\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # 添加坐标轴箭头\n",
    "    arrow_x = FancyArrowPatch(\n",
    "        posA=(ax.get_xlim()[1] * 1.0, 0),\n",
    "        posB=(ax.get_xlim()[1] * 1.03, 0),\n",
    "        transform=ax.get_xaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_x.set_clip_on(False)\n",
    "    ax.add_patch(arrow_x)\n",
    "\n",
    "    y_lim = ax.get_ylim()\n",
    "    arrow_y = FancyArrowPatch(\n",
    "        posA=(0, y_lim[1] * 1.0),\n",
    "        posB=(0, y_lim[1] * 1.03),\n",
    "        transform=ax.get_yaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_y.set_clip_on(False)\n",
    "    ax.add_patch(arrow_y)\n",
    "    ax.tick_params(axis='x', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    ax.tick_params(axis='y', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    \n",
    "    # 添加面板标签\n",
    "    if panel_label:\n",
    "        fig.text(\n",
    "            0.01, 0.99, panel_label, ha='left', va='top', fontsize=7, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', edgecolor='none', alpha=0.7, pad=0.2, lw=0), zorder=100\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_variance_analysis.png')\n",
    "    fig.savefig(save_path, dpi=300,  format='png')\n",
    "    print(f\"✅ 方差分析图已保存: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_performance_improvement(main_model_file, save_dir='Supplymentary_figure/Transformer_performance', panel_label=None):\n",
    "    \"\"\"\n",
    "    绘制性能改进图（训练历史）\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        主模型文件路径\n",
    "    save_dir : str\n",
    "        保存目录\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    config = data['config']\n",
    "\n",
    "    # 取消数据检查，直接提取数据（假设这些字段必定存在）\n",
    "    history = config['training_history']\n",
    "\n",
    "    # 默认优先accuracy，没有则用loss\n",
    "    if 'accuracy' in history and 'val_accuracy' in history:\n",
    "        train_metric = np.array(history['accuracy'])\n",
    "        val_metric = np.array(history['val_accuracy'])\n",
    "        metric_name = 'Accuracy'\n",
    "        ylabel = 'Accuracy'\n",
    "    else:\n",
    "        train_metric = np.array(history['loss'])\n",
    "        val_metric = np.array(history['val_loss'])\n",
    "        metric_name = 'Loss'\n",
    "        ylabel = 'Loss'\n",
    "\n",
    "    epochs = range(1, len(train_metric) + 1)\n",
    "\n",
    "    # 创建图形\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "\n",
    "    # 绘制训练和验证曲线，不带marker\n",
    "    ax.plot(epochs, train_metric, '-', color='#1F78B4',\n",
    "            linewidth=1.5, label=f'Training {metric_name}')\n",
    "    ax.plot(epochs, val_metric, '-', color='#E31A1C',\n",
    "            linewidth=1.5, label=f'Validation {metric_name}')\n",
    "\n",
    "    ax.set_xlabel('Epochs', fontweight='bold')\n",
    "    ax.set_ylabel(ylabel, fontweight='bold')\n",
    "    ax.set_title('Performance Improvement', fontweight='bold')\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_ylim([val_metric.min()*0.97, train_metric.max()*1.03])\n",
    "\n",
    "    # 添加坐标轴箭头\n",
    "    arrow_x = FancyArrowPatch(\n",
    "        posA=(ax.get_xlim()[1] * 1.0, 0),\n",
    "        posB=(ax.get_xlim()[1] * 1.03, 0),\n",
    "        transform=ax.get_xaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_x.set_clip_on(False)\n",
    "    ax.add_patch(arrow_x)\n",
    "\n",
    "    y_lim = ax.get_ylim()\n",
    "    arrow_y = FancyArrowPatch(\n",
    "        posA=(0, y_lim[1] * 1.0),\n",
    "        posB=(0, y_lim[1] * 1.01),\n",
    "        transform=ax.get_yaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_y.set_clip_on(False)\n",
    "    ax.add_patch(arrow_y)\n",
    "    ax.tick_params(axis='x', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    ax.tick_params(axis='y', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    \n",
    "    # 添加面板标签\n",
    "    if panel_label:\n",
    "        fig.text(\n",
    "            0.01, 0.99, panel_label, ha='left', va='top', fontsize=7, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', edgecolor='none', alpha=0.7, pad=0.2, lw=0), zorder=100\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    # 保存\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_performance_improvement.png')\n",
    "    fig.savefig(save_path, dpi=300, format='png')\n",
    "    print(f\"✅ 性能改进图已保存: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd7d6869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 找到Transformer模型文件: landuse_transformer_generation_single_20251124_234807.pkl\n",
      "\n",
      "✅ 学习曲线图已保存: Supplymentary_figure/Transformer_performance\\Figure_learning_curve.png\n",
      "✅ 过拟合分析图已保存: Supplymentary_figure/Transformer_performance\\Figure_overfitting_analysis.png\n",
      "✅ 方差分析图已保存: Supplymentary_figure/Transformer_performance\\Figure_variance_analysis.png\n",
      "✅ 性能改进图已保存: Supplymentary_figure/Transformer_performance\\Figure_performance_improvement.png\n",
      "✅ 找到MLP模型文件: landuse_mlp_generation_single_20251210_172430.pkl\n",
      "\n",
      "✅ 学习曲线图已保存: Supplymentary_figure/MLP_performance\\Figure_learning_curve.png\n",
      "✅ 过拟合分析图已保存: Supplymentary_figure/MLP_performance\\Figure_overfitting_analysis.png\n",
      "✅ 方差分析图已保存: Supplymentary_figure/MLP_performance\\Figure_variance_analysis.png\n",
      "✅ 性能改进图已保存: Supplymentary_figure/MLP_performance\\Figure_performance_improvement.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, model_file in enumerate(latest_model_files):\n",
    "    if model_file:\n",
    "        if isinstance(model_file, tuple):\n",
    "            model_path = model_file[0]\n",
    "        else:\n",
    "            model_path = model_file\n",
    "        model_type = \"Transformer\" if i == 0 else \"MLP\"\n",
    "        save_dir = f'Supplymentary_figure/{model_type}_performance'\n",
    "        print(f\"✅ 找到{model_type}模型文件: {os.path.basename(model_path)}\\n\")\n",
    "        \n",
    "        # 根据模型类型设置面板标签\n",
    "        if model_type == \"MLP\":\n",
    "            panel_labels = ['a', 'b', 'c']\n",
    "        else:  # Transformer\n",
    "            panel_labels = ['d', 'e', 'f']\n",
    "        \n",
    "        plot_learning_curve(model_path, save_dir=save_dir, panel_label=panel_labels[0])\n",
    "        plot_overfitting_analysis(model_path, save_dir=save_dir)\n",
    "        plot_variance_analysis(model_path, save_dir=save_dir, panel_label=panel_labels[1])\n",
    "        plot_performance_improvement(model_path, save_dir=save_dir, panel_label=panel_labels[2])\n",
    "    else:\n",
    "        model_type = \"Transformer\" if i == 0 else \"MLP\"\n",
    "        print(f\"❌ 未找到{model_type}模型文件\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45510b92",
   "metadata": {},
   "source": [
    "### 1.2.3 Training loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a13e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_loss(main_model_file, save_dir='Supplymentary_figure'):\n",
    "    \"\"\"\n",
    "    绘制训练损失曲线\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        主模型文件路径\n",
    "    save_dir : str\n",
    "        保存目录\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    config = data['config']\n",
    "    history = config['training_history']\n",
    "    train_loss = np.array(history['loss'])\n",
    "    val_loss = np.array(history['val_loss'])\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    # 创建图形\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "\n",
    "    # 绘制训练和验证损失曲线\n",
    "    ax.plot(epochs, train_loss, color='#1F78B4',\n",
    "            linewidth=1.5, label='Training Loss')\n",
    "    ax.plot(epochs, val_loss, color='#E31A1C',\n",
    "            linewidth=1.5, label='Validation Loss')\n",
    "    ax.set_xlabel('Epochs', fontweight='bold')\n",
    "    ax.set_ylabel('Loss', fontweight='bold')\n",
    "    ax.set_title('Training Loss', fontweight='bold')\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    arrow_x = FancyArrowPatch(\n",
    "        posA=(ax.get_xlim()[1] * 1.0, 0),\n",
    "        posB=(ax.get_xlim()[1] * 1.03, 0),\n",
    "        transform=ax.get_xaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_x.set_clip_on(False)\n",
    "    ax.add_patch(arrow_x)\n",
    "\n",
    "    y_lim = ax.get_ylim()\n",
    "    arrow_y = FancyArrowPatch(\n",
    "        posA=(0, y_lim[1] * 1.0),\n",
    "        posB=(0, y_lim[1] * 1.03),\n",
    "        transform=ax.get_yaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_y.set_clip_on(False)\n",
    "    ax.add_patch(arrow_y)\n",
    "    ax.tick_params(axis='x', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    ax.tick_params(axis='y', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_training_loss.png')\n",
    "    fig.savefig(save_path, dpi=300, format='png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(main_model_file, save_dir='Supplymentary_figure'):\n",
    "    \"\"\"\n",
    "    绘制ROC曲线\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        主模型文件路径\n",
    "    save_dir : str\n",
    "        保存目录\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    test_data = data['test_data']\n",
    "    \n",
    "    if test_data is None:\n",
    "        print(\"⚠️ 未找到测试数据\")\n",
    "        return\n",
    "    \n",
    "    if 'fpr' not in test_data or 'tpr' not in test_data or 'test_auc' not in test_data:\n",
    "        print(\"⚠️ 未找到ROC数据\")\n",
    "        return\n",
    "    \n",
    "    fpr = test_data['fpr']\n",
    "    tpr = test_data['tpr']\n",
    "    test_auc = float(test_data['test_auc'])\n",
    "    \n",
    "    # 创建图形\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "    \n",
    "    # 绘制ROC曲线\n",
    "    ax.plot(fpr, tpr, color='#1F78B4', linewidth=1.5, \n",
    "            label=f'ROC Curve (AUC = {test_auc:.3f})')\n",
    "    \n",
    "    # 绘制对角线（随机分类器）\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5, label='Random Classifier')\n",
    "    \n",
    "    ax.set_xlabel('False Positive Rate', fontweight='bold')\n",
    "    ax.set_ylabel('True Positive Rate', fontweight='bold')\n",
    "    ax.set_title('ROC Curve', fontweight='bold')\n",
    "    ax.legend(frameon=False, loc='lower right')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_xlim([0, 1.03])\n",
    "    ax.set_ylim([0, 1.03])\n",
    "\n",
    "    arrow_x = FancyArrowPatch(\n",
    "        posA=(ax.get_xlim()[1] * 1.0, 0),\n",
    "        posB=(ax.get_xlim()[1] * 1.03, 0),\n",
    "        transform=ax.get_xaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_x.set_clip_on(False)\n",
    "    ax.add_patch(arrow_x)\n",
    "\n",
    "    y_lim = ax.get_ylim()\n",
    "    arrow_y = FancyArrowPatch(\n",
    "        posA=(0, y_lim[1] * 1.0),\n",
    "        posB=(0, y_lim[1] * 1.03),\n",
    "        transform=ax.get_yaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_y.set_clip_on(False)\n",
    "    ax.add_patch(arrow_y)\n",
    "    ax.tick_params(axis='x', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    ax.tick_params(axis='y', which='major', length=2.5, width=0.5, pad=2, labelsize=5)    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_roc_curve.png')\n",
    "    fig.savefig(save_path, dpi=300, format='png')\n",
    "    print(f\"✅ ROC曲线图已保存: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_predicted_probability_distribution(main_model_file, save_dir='Supplymentary_figure'):\n",
    "    \"\"\"\n",
    "    绘制预测概率分布\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        主模型文件路径\n",
    "    save_dir : str\n",
    "        保存目录\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    test_data = data['test_data']\n",
    "    \n",
    "    if test_data is None:\n",
    "        print(\"⚠️ 未找到测试数据\")\n",
    "        return\n",
    "    \n",
    "    if 'y_test_pred' not in test_data or 'y_test' not in test_data:\n",
    "        print(\"⚠️ 未找到预测概率或真实标签数据\")\n",
    "        return\n",
    "    \n",
    "    y_test_pred = test_data['y_test_pred']\n",
    "    y_test = test_data['y_test']\n",
    "    \n",
    "    # 分离正负样本的预测概率\n",
    "    pos_probs = y_test_pred[y_test == 1]\n",
    "    neg_probs = y_test_pred[y_test == 0]\n",
    "    \n",
    "    # 创建图形\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "    \n",
    "    # 绘制直方图\n",
    "    bins = np.linspace(0, 1, 51)\n",
    "    ax.hist(neg_probs, bins=bins, alpha=0.6, color='#E31A1C', \n",
    "            label='Negative Class', density=True, edgecolor='black', linewidth=0.5)\n",
    "    ax.hist(pos_probs, bins=bins, alpha=0.6, color='#1F78B4', \n",
    "            label='Positive Class', density=True, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Predicted Probability', fontweight='bold')\n",
    "    ax.set_ylabel('Density', fontweight='bold')\n",
    "    ax.set_title('Predicted Probability Distribution', fontweight='bold')\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5, axis='y')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_xlim([0, 1.03])\n",
    "\n",
    "    arrow_x = FancyArrowPatch(\n",
    "        posA=(ax.get_xlim()[1] * 1.0, 0),\n",
    "        posB=(ax.get_xlim()[1] * 1.03, 0),\n",
    "        transform=ax.get_xaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_x.set_clip_on(False)\n",
    "    ax.add_patch(arrow_x)\n",
    "\n",
    "    y_lim = ax.get_ylim()\n",
    "    arrow_y = FancyArrowPatch(\n",
    "        posA=(0, y_lim[1] * 1.0),\n",
    "        posB=(0, y_lim[1] * 1.03),\n",
    "        transform=ax.get_yaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_y.set_clip_on(False)\n",
    "    ax.add_patch(arrow_y)\n",
    "    ax.tick_params(axis='x', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    ax.tick_params(axis='y', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    # 保存\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_predicted_probability_distribution.png')\n",
    "    fig.savefig(save_path, dpi=300,format='png')\n",
    "    print(f\"✅ 预测概率分布图已保存: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_performance_metrics(main_model_file, save_dir='Supplymentary_figure',panel_label=None):\n",
    "    \"\"\"\n",
    "    绘制性能指标对比图并打印指标\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        主模型文件路径\n",
    "    save_dir : str\n",
    "        保存目录\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    config = data['config']\n",
    "\n",
    "    metrics = config['training_metrics']\n",
    "\n",
    "    # 提取指标\n",
    "    metric_names = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    splits = ['train', 'val', 'test']\n",
    "\n",
    "    # 打印指标\n",
    "    print(\"Performance Metrics:\")\n",
    "    for split in splits:\n",
    "        if split in metrics:\n",
    "            print(f\"  {split.capitalize()}:\")\n",
    "            for m in metric_names:\n",
    "                value = metrics[split].get(m, 0)\n",
    "                print(f\"    {m.capitalize()}: {value:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # 准备数据\n",
    "    data_dict = {}\n",
    "    for split in splits:\n",
    "        if split not in metrics:\n",
    "            continue\n",
    "        data_dict[split] = [metrics[split].get(m, 0) for m in metric_names]\n",
    "\n",
    "    # 创建图形\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "\n",
    "    # 设置x轴位置\n",
    "    x = np.arange(len(metric_names))\n",
    "    width = 0.25\n",
    "\n",
    "    # 颜色\n",
    "    colors = {'train': '#1F78B4', 'val': '#E31A1C', 'test': '#33A02C'}\n",
    "    labels = {'train': 'Train', 'val': 'Validation', 'test': 'Test'}\n",
    "\n",
    "    # 绘制柱状图\n",
    "    offset = -width\n",
    "    for split in splits:\n",
    "        if split in data_dict:\n",
    "            ax.bar(x + offset, data_dict[split], width, \n",
    "                  label=labels[split], color=colors[split], \n",
    "                  edgecolor='black', linewidth=0.5, alpha=0.8)\n",
    "            offset += width\n",
    "\n",
    "    ax.set_xlabel('Metrics', fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontweight='bold')\n",
    "    ax.set_title('Performance Metrics', fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([m.capitalize() for m in metric_names])\n",
    "    ax.legend(frameon=False, loc='upper right', ncol=len(data_dict), bbox_to_anchor=(1, 1), bbox_transform=ax.transAxes)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5, axis='y')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_ylim([0.5, 1.03])\n",
    "\n",
    "    arrow_x = FancyArrowPatch(\n",
    "        posA=(ax.get_xlim()[1] * 1.0, 0),\n",
    "        posB=(ax.get_xlim()[1] * 1.03, 0),\n",
    "        transform=ax.get_xaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_x.set_clip_on(False)\n",
    "    ax.add_patch(arrow_x)\n",
    "\n",
    "    y_lim = ax.get_ylim()\n",
    "    arrow_y = FancyArrowPatch(\n",
    "        posA=(0, y_lim[1] * 1.0),\n",
    "        posB=(0, y_lim[1] * 1.02),\n",
    "        transform=ax.get_yaxis_transform(),\n",
    "        arrowstyle='simple',\n",
    "        color='black', linewidth=0, mutation_scale=8, zorder=20\n",
    "    )\n",
    "    arrow_y.set_clip_on(False)\n",
    "        # 添加面板标签\n",
    "    if panel_label:\n",
    "        fig.text(\n",
    "            0.01, 0.99, panel_label, ha='left', va='top', fontsize=7, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', edgecolor='none', alpha=0.7, pad=0.2, lw=0), zorder=100\n",
    "        )\n",
    "    ax.add_patch(arrow_y)\n",
    "    ax.tick_params(axis='x', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    ax.tick_params(axis='y', which='major', length=2.5, width=0.5, pad=2, labelsize=5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "    # 保存\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_performance_metrics.png')\n",
    "    fig.savefig(save_path, dpi=300, format='png')\n",
    "    print(f\"✅ 性能指标图已保存: {save_path}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82b8a94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 找到Transformer模型文件: landuse_transformer_generation_single_20251124_234807.pkl\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ROC曲线图已保存: Supplymentary_figure/Transformer_performance\\Figure_roc_curve.png\n",
      "✅ 预测概率分布图已保存: Supplymentary_figure/Transformer_performance\\Figure_predicted_probability_distribution.png\n",
      "Performance Metrics:\n",
      "  Train:\n",
      "    Accuracy: 0.9308\n",
      "    Precision: 0.9177\n",
      "    Recall: 0.9524\n",
      "    F1: 0.9347\n",
      "  Val:\n",
      "    Accuracy: 0.8942\n",
      "    Precision: 0.8720\n",
      "    Recall: 0.9337\n",
      "    F1: 0.9018\n",
      "  Test:\n",
      "    Accuracy: 0.8917\n",
      "    Precision: 0.8786\n",
      "    Recall: 0.9189\n",
      "    F1: 0.8983\n",
      "\n",
      "✅ 性能指标图已保存: Supplymentary_figure/Transformer_performance\\Figure_performance_metrics.png\n",
      "✅ 找到MLP模型文件: landuse_mlp_generation_single_20251210_172430.pkl\n",
      "\n",
      "✅ ROC曲线图已保存: Supplymentary_figure/MLP_performance\\Figure_roc_curve.png\n",
      "✅ 预测概率分布图已保存: Supplymentary_figure/MLP_performance\\Figure_predicted_probability_distribution.png\n",
      "Performance Metrics:\n",
      "  Train:\n",
      "    Accuracy: 0.8817\n",
      "    Precision: 0.8971\n",
      "    Recall: 0.8728\n",
      "    F1: 0.8848\n",
      "  Val:\n",
      "    Accuracy: 0.8773\n",
      "    Precision: 0.8869\n",
      "    Recall: 0.8759\n",
      "    F1: 0.8814\n",
      "  Test:\n",
      "    Accuracy: 0.8738\n",
      "    Precision: 0.8892\n",
      "    Recall: 0.8654\n",
      "    F1: 0.8771\n",
      "\n",
      "✅ 性能指标图已保存: Supplymentary_figure/MLP_performance\\Figure_performance_metrics.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i, model_file in enumerate(latest_model_files):\n",
    "    if model_file:\n",
    "        if isinstance(model_file, tuple):\n",
    "            model_path = model_file[0]\n",
    "        else:\n",
    "            model_path = model_file\n",
    "        model_type = \"Transformer\" if i == 0 else \"MLP\"\n",
    "        save_dir = f'Supplymentary_figure/{model_type}_performance'\n",
    "        print(f\"✅ 找到{model_type}模型文件: {os.path.basename(model_path)}\\n\")\n",
    "        plot_training_loss(model_path, save_dir=save_dir)\n",
    "        plot_roc_curve(model_path, save_dir=save_dir)\n",
    "        plot_predicted_probability_distribution(model_path, save_dir=save_dir)\n",
    "        # 根据模型类型设置面板标签\n",
    "        if model_type == \"MLP\":\n",
    "            panel_labels = ['a', 'b', 'c']\n",
    "        else:  # Transformer\n",
    "            panel_labels = ['d', 'e', 'f']\n",
    "        \n",
    "        plot_performance_metrics(model_path, save_dir=save_dir, panel_label=panel_labels[2])\n",
    "    else:\n",
    "        model_type = \"Transformer\" if i == 0 else \"MLP\"\n",
    "        print(f\"❌ 未找到{model_type}模型文件\\n\")\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
