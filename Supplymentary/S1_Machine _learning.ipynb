{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3108c3",
   "metadata": {},
   "source": [
    "# 1ã€Supplementary Figures of training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f69153",
   "metadata": {},
   "source": [
    "\n",
    "æˆ‘çš„æ ¸å¿ƒç›®çš„æ˜¯ï¼šæ ¹æ®ä¿å­˜çš„æ¨¡å‹å†…å®¹ï¼ˆå°¤å…¶æ˜¯å­¦ä¹ æ›²çº¿ä¸è®­ç»ƒç²¾åº¦ç­‰ï¼‰é‡æ–°ç»˜åˆ¶Complete Pipeline Analysiså‡½æ•°ä¸plot_learning_curve_nnçš„è®­ç»ƒæ›²çº¿ï¼›\n",
    "\n",
    "ç¬¬ä¸€ï¼Œè¯·ç»“åˆ3.0 pre-training.ipynbæ–‡ä»¶ï¼Œå†™ä¸€ä¸ªä¿å­˜ç»“æœçš„æ£€æµ‹å‡½æ•°ï¼Œè‹¥æ£€æµ‹åå‘ç°æœ‰å…³é”®æ•°æ®æ²¡æœ‰ä¿å­˜ï¼Œè¯·å®Œå–„å½“å‰çš„save_complete_model_pipelineï¼Œå¹¶ä¸”æˆ‘éœ€è¦é‡æ–°å¯åŠ¨è®­ç»ƒ\n",
    "\n",
    "\n",
    "æ¨¡å—ä¸€ï¼šå­¦ä¹ æ›²çº¿æ¨¡å—plot_learning_curve_nnçš„æ•°æ®æµï¼ˆæ ¸å¿ƒæ˜¯æ£€æµ‹è®­ç»ƒé›†ã€éªŒè¯é›†ä¹‹é—´çš„å‡å€¼ä¸æ–¹å·®å…³ç³»ï¼Œä»¥æ­¤åˆ¤æ–­è¿‡æ‹Ÿåˆçš„æ¨¡å¼ï¼‰ï¼›ç›®å‰æœ‰éƒ¨åˆ†æ•°æ®æ²¡æœ‰è¿”å›è¿›resultï¼ˆä¹Ÿå°±æ˜¯lc_analysisä¸­ï¼‰\n",
    "ï¼ˆ1ï¼‰ç”±plot_learning_curve_nné€šè¿‡StratifiedKFoldåˆ›å»ºçš„cvã€è¾“å…¥learning_curveæ‰€ç”Ÿæˆçš„train_sizes_abs, train_scores, val_scoresï¼›\n",
    "ï¼ˆ2ï¼‰æœ‰äº†ä»¥ä¸Šæ•°æ®åï¼Œå¯ä»¥è·¨cvæŠ˜æ±‚å‡å€¼/æ–¹å·®ï¼Œä¸»è¦åŒ…æ‹¬train_meanï¼Œtrain_stdï¼Œval_meanï¼Œval_stdï¼Œä»¥åŠæœ‰å…³analyze_overfittingäº§ç”Ÿçš„overfitting_analysiså­—å…¸ã€‚\n",
    "\n",
    "\n",
    "æ¨¡å—äºŒï¼šè®­ç»ƒæ®‹å·®æ¨¡å—plot_training_resultsåŸºç¡€æŒ‡æ ‡çš„æ•°æ®æµï¼ˆæ ¸å¿ƒæ˜¯å…³æ³¨éšç€è®­ç»ƒæ¬¡æ•°çš„å¢åŠ ï¼‰ï¼Œè¿™éƒ¨åˆ†æ•°æ®åº”è¯¥ä¸»è¦é›†ä¸­åœ¨training_resultsä¸­\n",
    "\n",
    "ï¼ˆ1ï¼‰ä»…åœ¨è°ƒç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ—¶å€™ç”Ÿæˆçš„historyæ•°æ®ï¼ˆæ ¹æ®modelå¯¹è±¡æ‰€å…·æœ‰çš„fitæ–¹æ³•æ‰€äº§ç”Ÿçš„historyå¯¹è±¡ï¼‰ï¼›ä¸»è¦åŒ…å«äº†history.history['loss'],history.history['val_loss'], history.history['accuracy'],history.history['val_accuracy']\n",
    "ä»¥ä¸Šæ•°æ®å¯ä»¥ç”¨äºç»˜åˆ¶éšç€epochså¢é•¿çš„lossä¸‹é™ã€Traning Accuracyï¼Œå¿…è¦æ—¶å¯ä»¥ä¿å­˜å„ä¸ªCVçš„å®Œæ•´è®­ç»ƒæ•°æ®\n",
    "\n",
    "\n",
    "ï¼ˆ2ï¼‰ROCæ›²çº¿ã€æµ‹è¯•é›†testçš„æ¦‚ç‡é¢„æµ‹åˆ†å¸ƒã€æ··æ·†çŸ©é˜µæ‰€éœ€è¦çš„ä»¥ä¸‹æ•°æ®\n",
    "            y_test_pred = model.predict(X_test, verbose=0).ravel()\n",
    "            y_test_bin = (y_test_pred > 0.5).astype(int)\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_test_pred)\n",
    "            test_auc = auc(fpr, tpr)\n",
    "            test_metrics = {\n",
    "                'accuracy': accuracy_score(y_test, y_test_bin),\n",
    "                'precision': precision_score(y_test, y_test_bin),\n",
    "                'recall': recall_score(y_test, y_test_bin),\n",
    "                'f1': f1_score(y_test, y_test_bin),\n",
    "                'auc': test_auc\n",
    "            }\n",
    "ä»¥ä¸Šæ•°æ®å¯ä»¥ç”¨äºç»˜åˆ¶ROCæ›²çº¿ã€æ··æ·†çŸ©é˜µcmã€æµ‹è¯•é›†çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™é‡Œä»¥ROCæ›²çº¿ä¸ºä¾‹ï¼š\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        axes[1].plot(fpr, tpr, label=f'{model_name} (AUC={metrics[\"auc\"]:.3f})', linewidth=2)\n",
    "\n",
    "æ¨¡å—ä¸‰ï¼šå®Œæ•´ç®¡çº¿è¯„ä¼°plot_complete_pipeline_resultsçš„æ€§èƒ½æŒ‡æ ‡æ•°æ®æµ\n",
    "ä¸»è¦æ•°æ®æ¥æºäºtrain_and_evaluate_modelå‡½æ•°ç”Ÿæˆçš„training_results\n",
    "    metrics_names = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    train_values = [training_results['metrics']['train'][m] for m in metrics_names]\n",
    "    val_values = [training_results['metrics']['val'][m] for m in metrics_names]\n",
    "    test_values = [training_results['metrics']['test'][m] for m in metrics_names]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7eae5a",
   "metadata": {},
   "source": [
    "data/US_data/ML_model/\n",
    "â”œâ”€â”€ {model_name}.pkl                    # ä¸»æ¨¡å‹æ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰è·¯å¾„å¼•ç”¨ï¼‰\n",
    "â”œâ”€â”€ {model_name}_config.json            # å®Œæ•´é…ç½®æ–‡ä»¶ï¼ˆåŒ…å«è®­ç»ƒå†å²ã€å­¦ä¹ æ›²çº¿ç­‰ï¼‰\n",
    "â”œâ”€â”€ {model_name}_gmm.pkl                # GMMæ¨¡å‹\n",
    "â”œâ”€â”€ {model_name}_dl.h5                  # æ·±åº¦å­¦ä¹ æ¨¡å‹\n",
    "â”œâ”€â”€ {model_name}_preprocessor.pkl       # é¢„å¤„ç†å™¨\n",
    "â”œâ”€â”€ {model_name}_test_data.npz          # æµ‹è¯•æ•°æ®ï¼ˆåŒ…å«ROCæ•°æ®ï¼‰\n",
    "â”œâ”€â”€ results_{model_type}_{timestamp}.csv # é¢„æµ‹ç»“æœ\n",
    "â””â”€â”€ summary_{model_type}_{timestamp}.json # è½»é‡çº§æ‘˜è¦ï¼ˆå¯é€‰ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56940280",
   "metadata": {},
   "source": [
    "## 1.1 Check data for plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ff9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ‰¾åˆ° 3 ä¸ªæ¨¡å‹æ–‡ä»¶ï¼Œä½¿ç”¨æœ€æ–°çš„: landuse_transformer_generation_single_20251117_054315.pkl\n",
      "\n",
      "================================================================================\n",
      "æ¨¡å‹æ•°æ®å®Œæ•´æ€§æ£€æµ‹æŠ¥å‘Š\n",
      "================================================================================\n",
      "\n",
      "âœ… æ€»ä½“çŠ¶æ€: COMPLETE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "è¯¦ç»†æ£€æµ‹ç»“æœ:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“¦ LEARNING_CURVE:\n",
      "  âœ… æ‰€æœ‰å¿…éœ€é”®éƒ½å­˜åœ¨\n",
      "\n",
      "ğŸ“¦ TRAINING_HISTORY:\n",
      "  âœ… æ‰€æœ‰å¿…éœ€é”®éƒ½å­˜åœ¨\n",
      "\n",
      "ğŸ“¦ ROC_DATA:\n",
      "  âœ… æ‰€æœ‰å¿…éœ€é”®éƒ½å­˜åœ¨\n",
      "\n",
      "ğŸ“¦ METRICS:\n",
      "  âœ… æ‰€æœ‰å¿…éœ€é”®éƒ½å­˜åœ¨\n",
      "\n",
      "ğŸ“¦ TEST_DATA:\n",
      "  âœ… æ‰€æœ‰å¿…éœ€é”®éƒ½å­˜åœ¨\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "æ–‡ä»¶è·¯å¾„:\n",
      "--------------------------------------------------------------------------------\n",
      "  âœ… main_model: c:\\Dev\\Landuse_Zhong_clean\\data\\US_data\\ML_model\\landuse_transformer_generation_single_20251117_054315.pkl\n",
      "  âœ… config: c:\\Dev\\Landuse_Zhong_clean\\data\\US_data\\ML_model\\landuse_transformer_generation_single_20251117_054315_config.json\n",
      "  âœ… test_data: c:\\Dev\\Landuse_Zhong_clean\\data\\US_data\\ML_model\\landuse_transformer_generation_single_20251117_054315_test_data.npz\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "æ•°æ®æ‘˜è¦:\n",
      "--------------------------------------------------------------------------------\n",
      "  learning_curve:\n",
      "    - n_train_sizes: 5\n",
      "    - train_sizes_range: [1932, 9660]\n",
      "  training_history:\n",
      "    - n_epochs: 50\n",
      "    - final_loss: 0.32459840178489685\n",
      "    - final_val_loss: 0.30153459310531616\n",
      "  roc_data:\n",
      "    - test_auc: 0.9583094072469159\n",
      "    - fpr_length: 686\n",
      "    - tpr_length: 686\n",
      "  test_data:\n",
      "    - X_test_shape: (4026, 23)\n",
      "    - y_test_shape: (4026,)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "å»ºè®®:\n",
      "--------------------------------------------------------------------------------\n",
      "  1. âœ… æ‰€æœ‰å¿…éœ€æ•°æ®éƒ½å·²ä¿å­˜ï¼Œå¯ä»¥æ­£å¸¸ç»˜å›¾\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ… æ•°æ®å®Œæ•´æ€§æ£€æµ‹é€šè¿‡ï¼Œå¯ä»¥è¿›è¡Œåç»­ç»˜å›¾æ“ä½œ\n"
     ]
    }
   ],
   "source": [
    "def check_saved_model_data_completeness(main_model_file):\n",
    "    \"\"\"\n",
    "    æ£€æµ‹ä¿å­˜çš„æ¨¡å‹æ•°æ®æ˜¯å¦å®Œæ•´ï¼Œç”¨äºåç»­ç»˜å›¾\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        ä¿å­˜çš„æ¨¡å‹ä¸»æ–‡ä»¶è·¯å¾„ï¼ˆ.pklæ–‡ä»¶ï¼‰ï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    report : dict\n",
    "        æ£€æµ‹æŠ¥å‘Šï¼ŒåŒ…å«ï¼š\n",
    "        - 'status': 'complete' | 'incomplete' | 'error'\n",
    "        - 'missing_data': list of missing data keys\n",
    "        - 'details': dict with detailed check results\n",
    "        - 'file_paths': dict with resolved file paths\n",
    "        - 'data_summary': dict with data shape/size information\n",
    "    \"\"\"\n",
    "    import joblib\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    report = {\n",
    "        'status': 'unknown',\n",
    "        'missing_data': [],\n",
    "        'details': {},\n",
    "        'recommendations': [],\n",
    "        'file_paths': {},\n",
    "        'data_summary': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # è§„èŒƒåŒ–æ–‡ä»¶è·¯å¾„\n",
    "        main_model_file = os.path.abspath(os.path.normpath(main_model_file))\n",
    "        report['file_paths']['main_model'] = main_model_file\n",
    "        \n",
    "        # æ£€æŸ¥ä¸»æ¨¡å‹æ–‡ä»¶\n",
    "        if not os.path.exists(main_model_file):\n",
    "            report['status'] = 'error'\n",
    "            report['error'] = f\"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {main_model_file}\"\n",
    "            return report\n",
    "        \n",
    "        # åŠ è½½ä¸»æ¨¡å‹æ–‡ä»¶\n",
    "        try:\n",
    "            main_model = joblib.load(main_model_file)\n",
    "        except Exception as e:\n",
    "            report['status'] = 'error'\n",
    "            report['error'] = f\"æ— æ³•åŠ è½½æ¨¡å‹æ–‡ä»¶: {str(e)}\"\n",
    "            return report\n",
    "        \n",
    "        # è·å–ç›¸å…³æ–‡ä»¶è·¯å¾„\n",
    "        config_path = main_model.get('config_path')\n",
    "        test_data_path = main_model.get('test_data_path')\n",
    "        \n",
    "        def resolve_path(path, base_dir):\n",
    "            \"\"\"è§£æè·¯å¾„ï¼šç»å¯¹è·¯å¾„ç›´æ¥ä½¿ç”¨ï¼Œç›¸å¯¹è·¯å¾„åŸºäºbase_diræ‹¼æ¥\"\"\"\n",
    "            if not path:\n",
    "                return None\n",
    "            \n",
    "            # ç»å¯¹è·¯å¾„ç›´æ¥ä½¿ç”¨\n",
    "            if os.path.isabs(path):\n",
    "                return os.path.normpath(path)\n",
    "            \n",
    "            # ç›¸å¯¹è·¯å¾„ï¼šå…ˆå°è¯•ç›´æ¥æ‹¼æ¥\n",
    "            full_path = os.path.normpath(os.path.join(base_dir, path))\n",
    "            if os.path.exists(full_path):\n",
    "                return full_path\n",
    "            \n",
    "            # å¦‚æœä¸å­˜åœ¨ï¼Œå°è¯•åªä½¿ç”¨æ–‡ä»¶åï¼ˆå¤„ç†è·¯å¾„é‡å¤çš„æƒ…å†µï¼‰\n",
    "            filename = os.path.basename(path)\n",
    "            fallback_path = os.path.normpath(os.path.join(base_dir, filename))\n",
    "            return fallback_path\n",
    "        \n",
    "        model_dir = os.path.dirname(main_model_file)\n",
    "        \n",
    "        # å¤„ç†é…ç½®æ–‡ä»¶è·¯å¾„\n",
    "        if not config_path:\n",
    "            report['status'] = 'error'\n",
    "            report['error'] = \"é…ç½®è·¯å¾„æœªåœ¨ä¸»æ¨¡å‹æ–‡ä»¶ä¸­æŒ‡å®š\"\n",
    "            return report\n",
    "        \n",
    "        config_path = resolve_path(config_path, model_dir)\n",
    "        report['file_paths']['config'] = config_path\n",
    "        \n",
    "        # å¤„ç†æµ‹è¯•æ•°æ®è·¯å¾„\n",
    "        if test_data_path:\n",
    "            test_data_path = resolve_path(test_data_path, model_dir)\n",
    "            report['file_paths']['test_data'] = test_data_path\n",
    "        \n",
    "        # åŠ è½½é…ç½®æ–‡ä»¶\n",
    "        if not os.path.exists(config_path):\n",
    "            report['status'] = 'error'\n",
    "            report['error'] = f\"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}\"\n",
    "            report['recommendations'].append(f\"ä¸»æ¨¡å‹æ–‡ä»¶ç›®å½•: {model_dir}\")\n",
    "            report['recommendations'].append(f\"åŸå§‹é…ç½®è·¯å¾„: {main_model.get('config_path')}\")\n",
    "            return report\n",
    "        \n",
    "        try:\n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                config = json.load(f)\n",
    "        except Exception as e:\n",
    "            report['status'] = 'error'\n",
    "            report['error'] = f\"æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶: {str(e)}\"\n",
    "            return report\n",
    "        \n",
    "        # åŠ è½½æµ‹è¯•æ•°æ®\n",
    "        test_data = None\n",
    "        if test_data_path and os.path.exists(test_data_path):\n",
    "            try:\n",
    "                test_data = np.load(test_data_path, allow_pickle=True)\n",
    "            except Exception as e:\n",
    "                report['warnings'] = report.get('warnings', [])\n",
    "                report['warnings'].append(f\"æ— æ³•åŠ è½½æµ‹è¯•æ•°æ®: {str(e)}\")\n",
    "        elif test_data_path:\n",
    "            report['warnings'] = report.get('warnings', [])\n",
    "            report['warnings'].append(f\"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_data_path}\")\n",
    "        \n",
    "        # ========== æ¨¡å—ä¸€ï¼šå­¦ä¹ æ›²çº¿æ•°æ®æ£€æµ‹ ==========\n",
    "        lc_check = {\n",
    "            'has_learning_curve': False,\n",
    "            'missing_keys': [],\n",
    "            'empty_keys': [],\n",
    "            'required_keys': [\n",
    "                'train_sizes',  # train_sizes_abs\n",
    "                'train_scores_mean',\n",
    "                'train_scores_std',\n",
    "                'val_scores_mean',\n",
    "                'val_scores_std',\n",
    "                'overfitting_analysis'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        if 'learning_curve_analysis' in config:\n",
    "            lc_data = config['learning_curve_analysis']\n",
    "            lc_check['has_learning_curve'] = True\n",
    "            \n",
    "            for key in lc_check['required_keys']:\n",
    "                if key not in lc_data:\n",
    "                    lc_check['missing_keys'].append(key)\n",
    "                    report['missing_data'].append(f'learning_curve.{key}')\n",
    "                else:\n",
    "                    value = lc_data[key]\n",
    "                    # æ£€æŸ¥æ˜¯å¦ä¸ºç©º\n",
    "                    if value is None:\n",
    "                        lc_check['empty_keys'].append(key)\n",
    "                        report['missing_data'].append(f'learning_curve.{key} (empty)')\n",
    "                    elif isinstance(value, (list, np.ndarray)):\n",
    "                        try:\n",
    "                            if len(value) == 0:\n",
    "                                lc_check['empty_keys'].append(key)\n",
    "                                report['missing_data'].append(f'learning_curve.{key} (empty)')\n",
    "                        except (TypeError, ValueError):\n",
    "                            pass  # æ— æ³•åˆ¤æ–­é•¿åº¦ï¼Œå‡è®¾éç©º\n",
    "            \n",
    "            # æ•°æ®æ‘˜è¦\n",
    "            if 'train_sizes' in lc_data and lc_data['train_sizes']:\n",
    "                report['data_summary']['learning_curve'] = {\n",
    "                    'n_train_sizes': len(lc_data['train_sizes']),\n",
    "                    'train_sizes_range': [min(lc_data['train_sizes']), max(lc_data['train_sizes'])]\n",
    "                }\n",
    "        else:\n",
    "            lc_check['missing_keys'] = lc_check['required_keys']\n",
    "            report['missing_data'].extend([f'learning_curve.{k}' for k in lc_check['required_keys']])\n",
    "        \n",
    "        report['details']['learning_curve'] = lc_check\n",
    "        \n",
    "        # ========== æ¨¡å—äºŒï¼šè®­ç»ƒå†å²æ•°æ®æ£€æµ‹ ==========\n",
    "        history_check = {\n",
    "            'has_history': False,\n",
    "            'missing_keys': [],\n",
    "            'empty_keys': [],\n",
    "            'required_keys': ['loss', 'val_loss', 'accuracy', 'val_accuracy']\n",
    "        }\n",
    "        \n",
    "        if 'training_history' in config:\n",
    "            history_data = config['training_history']\n",
    "            history_check['has_history'] = True\n",
    "            \n",
    "            for key in history_check['required_keys']:\n",
    "                if key not in history_data:\n",
    "                    history_check['missing_keys'].append(key)\n",
    "                    report['missing_data'].append(f'training_history.{key}')\n",
    "                else:\n",
    "                    value = history_data[key]\n",
    "                    # æ£€æŸ¥æ˜¯å¦ä¸ºç©º\n",
    "                    if value is None:\n",
    "                        history_check['empty_keys'].append(key)\n",
    "                        report['missing_data'].append(f'training_history.{key} (empty)')\n",
    "                    elif isinstance(value, list):\n",
    "                        try:\n",
    "                            if len(value) == 0:\n",
    "                                history_check['empty_keys'].append(key)\n",
    "                                report['missing_data'].append(f'training_history.{key} (empty)')\n",
    "                        except (TypeError, ValueError):\n",
    "                            pass  # æ— æ³•åˆ¤æ–­é•¿åº¦ï¼Œå‡è®¾éç©º\n",
    "            \n",
    "            # æ•°æ®æ‘˜è¦\n",
    "            if 'loss' in history_data and history_data['loss']:\n",
    "                report['data_summary']['training_history'] = {\n",
    "                    'n_epochs': len(history_data['loss']),\n",
    "                    'final_loss': history_data['loss'][-1] if history_data['loss'] else None,\n",
    "                    'final_val_loss': history_data.get('val_loss', [None])[-1] if history_data.get('val_loss') else None\n",
    "                }\n",
    "        else:\n",
    "            history_check['missing_keys'] = history_check['required_keys']\n",
    "            report['missing_data'].extend([f'training_history.{k}' for k in history_check['required_keys']])\n",
    "        \n",
    "        report['details']['training_history'] = history_check\n",
    "        \n",
    "        # ========== æ¨¡å—äºŒï¼šROCæ›²çº¿æ•°æ®æ£€æµ‹ ==========\n",
    "        roc_check = {\n",
    "            'has_roc_data': False,\n",
    "            'missing_keys': [],\n",
    "            'empty_keys': [],\n",
    "            'required_keys': ['fpr', 'tpr', 'test_auc', 'y_test_pred']\n",
    "        }\n",
    "        \n",
    "        if test_data is not None:\n",
    "            for key in roc_check['required_keys']:\n",
    "                if key not in test_data:\n",
    "                    roc_check['missing_keys'].append(key)\n",
    "                    report['missing_data'].append(f'roc_data.{key}')\n",
    "                else:\n",
    "                    data_item = test_data[key]\n",
    "                    # æ£€æŸ¥æ˜¯å¦ä¸ºç©º\n",
    "                    if data_item is None:\n",
    "                        roc_check['empty_keys'].append(key)\n",
    "                        report['missing_data'].append(f'roc_data.{key} (empty)')\n",
    "                    elif hasattr(data_item, '__len__') and not isinstance(data_item, (str, int, float, bool)):\n",
    "                        try:\n",
    "                            if len(data_item) == 0:\n",
    "                                roc_check['empty_keys'].append(key)\n",
    "                                report['missing_data'].append(f'roc_data.{key} (empty)')\n",
    "                        except (TypeError, ValueError):\n",
    "                            pass  # æ— æ³•åˆ¤æ–­é•¿åº¦ï¼Œå‡è®¾éç©º\n",
    "            \n",
    "            if len(roc_check['missing_keys']) == 0 and len(roc_check['empty_keys']) == 0:\n",
    "                roc_check['has_roc_data'] = True\n",
    "                # æ•°æ®æ‘˜è¦\n",
    "                if 'test_auc' in test_data:\n",
    "                    report['data_summary']['roc_data'] = {\n",
    "                        'test_auc': float(test_data['test_auc']) if test_data['test_auc'] is not None else None,\n",
    "                        'fpr_length': len(test_data['fpr']) if 'fpr' in test_data else None,\n",
    "                        'tpr_length': len(test_data['tpr']) if 'tpr' in test_data else None\n",
    "                    }\n",
    "        else:\n",
    "            roc_check['missing_keys'] = roc_check['required_keys']\n",
    "            report['missing_data'].extend([f'roc_data.{k}' for k in roc_check['required_keys']])\n",
    "            if not test_data_path:\n",
    "                report['warnings'] = report.get('warnings', [])\n",
    "                report['warnings'].append(\"æµ‹è¯•æ•°æ®è·¯å¾„æœªæŒ‡å®š\")\n",
    "            elif not os.path.exists(test_data_path):\n",
    "                report['warnings'] = report.get('warnings', [])\n",
    "                report['warnings'].append(f\"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_data_path}\")\n",
    "        \n",
    "        report['details']['roc_data'] = roc_check\n",
    "        \n",
    "        # ========== æ¨¡å—ä¸‰ï¼šæ€§èƒ½æŒ‡æ ‡æ£€æµ‹ ==========\n",
    "        metrics_check = {\n",
    "            'has_metrics': False,\n",
    "            'missing_keys': [],\n",
    "            'required_keys': ['train', 'val', 'test']\n",
    "        }\n",
    "        \n",
    "        if 'training_metrics' in config:\n",
    "            metrics_data = config['training_metrics']\n",
    "            metrics_check['has_metrics'] = True\n",
    "            \n",
    "            for split in metrics_check['required_keys']:\n",
    "                if split not in metrics_data:\n",
    "                    metrics_check['missing_keys'].append(split)\n",
    "                    report['missing_data'].append(f'metrics.{split}')\n",
    "                else:\n",
    "                    # æ£€æŸ¥æ¯ä¸ªsplitçš„æŒ‡æ ‡\n",
    "                    required_metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "                    for metric in required_metrics:\n",
    "                        if metric not in metrics_data[split]:\n",
    "                            report['missing_data'].append(f'metrics.{split}.{metric}')\n",
    "        else:\n",
    "            metrics_check['missing_keys'] = metrics_check['required_keys']\n",
    "            report['missing_data'].extend([f'metrics.{k}' for k in metrics_check['required_keys']])\n",
    "        \n",
    "        report['details']['metrics'] = metrics_check\n",
    "        \n",
    "        # ========== æµ‹è¯•æ•°æ®æ£€æµ‹ ==========\n",
    "        test_data_check = {\n",
    "            'has_test_data': False,\n",
    "            'missing_keys': [],\n",
    "            'empty_keys': [],\n",
    "            'required_keys': ['X_test', 'y_test', 'X_train', 'y_train', 'X_val', 'y_val']\n",
    "        }\n",
    "        \n",
    "        if test_data is not None:\n",
    "            for key in test_data_check['required_keys']:\n",
    "                if key not in test_data:\n",
    "                    test_data_check['missing_keys'].append(key)\n",
    "                    report['missing_data'].append(f'test_data.{key}')\n",
    "                else:\n",
    "                    data_item = test_data[key]\n",
    "                    # æ£€æŸ¥æ˜¯å¦ä¸ºç©º\n",
    "                    if data_item is None:\n",
    "                        test_data_check['empty_keys'].append(key)\n",
    "                        report['missing_data'].append(f'test_data.{key} (empty)')\n",
    "                    elif hasattr(data_item, '__len__') and not isinstance(data_item, (str, int, float, bool)):\n",
    "                        try:\n",
    "                            if len(data_item) == 0:\n",
    "                                test_data_check['empty_keys'].append(key)\n",
    "                                report['missing_data'].append(f'test_data.{key} (empty)')\n",
    "                        except (TypeError, ValueError):\n",
    "                            pass  # æ— æ³•åˆ¤æ–­é•¿åº¦ï¼Œå‡è®¾éç©º\n",
    "            \n",
    "            if len(test_data_check['missing_keys']) == 0 and len(test_data_check['empty_keys']) == 0:\n",
    "                test_data_check['has_test_data'] = True\n",
    "                # æ•°æ®æ‘˜è¦\n",
    "                if 'X_test' in test_data and test_data['X_test'] is not None:\n",
    "                    X_test = test_data['X_test']\n",
    "                    report['data_summary']['test_data'] = {\n",
    "                        'X_test_shape': X_test.shape if hasattr(X_test, 'shape') else f\"len={len(X_test)}\",\n",
    "                        'y_test_shape': test_data['y_test'].shape if 'y_test' in test_data and hasattr(test_data['y_test'], 'shape') else None\n",
    "                    }\n",
    "        else:\n",
    "            test_data_check['missing_keys'] = test_data_check['required_keys']\n",
    "            report['missing_data'].extend([f'test_data.{k}' for k in test_data_check['required_keys']])\n",
    "        \n",
    "        report['details']['test_data'] = test_data_check\n",
    "        \n",
    "        # ========== ç”Ÿæˆå»ºè®® ==========\n",
    "        if len(report['missing_data']) == 0:\n",
    "            report['status'] = 'complete'\n",
    "            report['recommendations'].append(\"âœ… æ‰€æœ‰å¿…éœ€æ•°æ®éƒ½å·²ä¿å­˜ï¼Œå¯ä»¥æ­£å¸¸ç»˜å›¾\")\n",
    "        else:\n",
    "            report['status'] = 'incomplete'\n",
    "            \n",
    "            if not history_check['has_history']:\n",
    "                report['recommendations'].append(\n",
    "                    \"âš ï¸ ç¼ºå°‘è®­ç»ƒå†å²æ•°æ®ã€‚éœ€è¦åœ¨ save_complete_model_pipeline ä¸­ä¿å­˜ history.history\"\n",
    "                )\n",
    "            elif history_check.get('empty_keys'):\n",
    "                report['recommendations'].append(\n",
    "                    f\"âš ï¸ è®­ç»ƒå†å²æ•°æ®å­˜åœ¨ä½†ä¸ºç©º: {', '.join(history_check['empty_keys'])}\"\n",
    "                )\n",
    "            \n",
    "            if not lc_check['has_learning_curve']:\n",
    "                report['recommendations'].append(\n",
    "                    \"âš ï¸ ç¼ºå°‘å­¦ä¹ æ›²çº¿æ•°æ®ã€‚éœ€è¦ç¡®ä¿ plot_learning_curve_nn è¿”å›å®Œæ•´ç»“æœå¹¶ä¿å­˜\"\n",
    "                )\n",
    "            elif lc_check.get('empty_keys'):\n",
    "                report['recommendations'].append(\n",
    "                    f\"âš ï¸ å­¦ä¹ æ›²çº¿æ•°æ®å­˜åœ¨ä½†ä¸ºç©º: {', '.join(lc_check['empty_keys'])}\"\n",
    "                )\n",
    "            \n",
    "            if not roc_check['has_roc_data']:\n",
    "                report['recommendations'].append(\n",
    "                    \"âš ï¸ ç¼ºå°‘ROCæ›²çº¿æ•°æ®ã€‚éœ€è¦åœ¨ä¿å­˜æµ‹è¯•æ•°æ®æ—¶æ·»åŠ  fpr, tpr, test_auc, y_test_pred\"\n",
    "                )\n",
    "            elif roc_check.get('empty_keys'):\n",
    "                report['recommendations'].append(\n",
    "                    f\"âš ï¸ ROCæ›²çº¿æ•°æ®å­˜åœ¨ä½†ä¸ºç©º: {', '.join(roc_check['empty_keys'])}\"\n",
    "                )\n",
    "        \n",
    "        return report\n",
    "        \n",
    "    except Exception as e:\n",
    "        report['status'] = 'error'\n",
    "        report['error'] = str(e)\n",
    "        import traceback\n",
    "        report['traceback'] = traceback.format_exc()\n",
    "        return report\n",
    "        \n",
    "def print_check_report(report):\n",
    "    \"\"\"æ‰“å°æ£€æµ‹æŠ¥å‘Šçš„å‹å¥½æ ¼å¼\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"æ¨¡å‹æ•°æ®å®Œæ•´æ€§æ£€æµ‹æŠ¥å‘Š\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if report['status'] == 'error':\n",
    "        print(f\"âŒ æ£€æµ‹å¤±è´¥: {report.get('error', 'Unknown error')}\")\n",
    "        if 'traceback' in report:\n",
    "            print(\"\\nè¯¦ç»†é”™è¯¯ä¿¡æ¯:\")\n",
    "            print(report['traceback'])\n",
    "        if 'recommendations' in report and report['recommendations']:\n",
    "            print(\"\\nå»ºè®®:\")\n",
    "            for i, rec in enumerate(report['recommendations'], 1):\n",
    "                print(f\"  {i}. {rec}\")\n",
    "        return\n",
    "    \n",
    "    # çŠ¶æ€\n",
    "    status_icons = {\n",
    "        'complete': 'âœ…',\n",
    "        'incomplete': 'âš ï¸',\n",
    "        'error': 'âŒ'\n",
    "    }\n",
    "    icon = status_icons.get(report['status'], 'â“')\n",
    "    print(f\"\\n{icon} æ€»ä½“çŠ¶æ€: {report['status'].upper()}\")\n",
    "    \n",
    "    # è¯¦ç»†ä¿¡æ¯\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"è¯¦ç»†æ£€æµ‹ç»“æœ:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for module_name, details in report['details'].items():\n",
    "        print(f\"\\nğŸ“¦ {module_name.upper()}:\")\n",
    "        \n",
    "        if 'has_' in details:\n",
    "            has_key = [k for k in details.keys() if k.startswith('has_')][0]\n",
    "            status = 'âœ…' if details[has_key] else 'âŒ'\n",
    "            print(f\"  {status} æ•°æ®å­˜åœ¨: {details[has_key]}\")\n",
    "        \n",
    "        if 'missing_keys' in details and details['missing_keys']:\n",
    "            print(f\"  âš ï¸ ç¼ºå¤±çš„é”®: {', '.join(details['missing_keys'])}\")\n",
    "        elif 'missing_keys' in details:\n",
    "            print(f\"  âœ… æ‰€æœ‰å¿…éœ€é”®éƒ½å­˜åœ¨\")\n",
    "        \n",
    "        if 'empty_keys' in details and details['empty_keys']:\n",
    "            print(f\"  âš ï¸ ç©ºçš„é”®: {', '.join(details['empty_keys'])}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæ–‡ä»¶è·¯å¾„\n",
    "    if 'file_paths' in report and report['file_paths']:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"æ–‡ä»¶è·¯å¾„:\")\n",
    "        print(\"-\" * 80)\n",
    "        for key, path in report['file_paths'].items():\n",
    "            exists = \"âœ…\" if os.path.exists(path) else \"âŒ\"\n",
    "            print(f\"  {exists} {key}: {path}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæ•°æ®æ‘˜è¦\n",
    "    if 'data_summary' in report and report['data_summary']:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"æ•°æ®æ‘˜è¦:\")\n",
    "        print(\"-\" * 80)\n",
    "        for key, summary in report['data_summary'].items():\n",
    "            print(f\"  {key}:\")\n",
    "            for k, v in summary.items():\n",
    "                print(f\"    - {k}: {v}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºè­¦å‘Š\n",
    "    if 'warnings' in report and report['warnings']:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"è­¦å‘Š:\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, warning in enumerate(report['warnings'], 1):\n",
    "            print(f\"  {i}. {warning}\")\n",
    "    \n",
    "    # ç¼ºå¤±æ•°æ®æ±‡æ€»\n",
    "    if report['missing_data']:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"ç¼ºå¤±æ•°æ®æ±‡æ€»:\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, missing in enumerate(report['missing_data'], 1):\n",
    "            print(f\"  {i}. {missing}\")\n",
    "    \n",
    "    # å»ºè®®\n",
    "    if report['recommendations']:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"å»ºè®®:\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, rec in enumerate(report['recommendations'], 1):\n",
    "            print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# æ‰§è¡Œéƒ¨åˆ†ï¼šæ£€æµ‹æ¨¡å‹æ•°æ®å®Œæ•´æ€§\n",
    "# ============================================================\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def find_latest_model(pattern='landuse_transformer_generation*.pkl', search_dir='../data/US_data/ML_model'):\n",
    "    \"\"\"æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶\"\"\"\n",
    "    abs_search_dir = os.path.abspath(os.path.normpath(search_dir))\n",
    "    abs_pattern = os.path.join(abs_search_dir, pattern)\n",
    "    model_files = glob.glob(abs_pattern)\n",
    "    \n",
    "    if model_files:\n",
    "        model_files.sort(key=os.path.getmtime, reverse=True)\n",
    "        return model_files[0], len(model_files)\n",
    "    return None, 0\n",
    "\n",
    "# æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶\n",
    "model_file, n_files = find_latest_model()\n",
    "\n",
    "if model_file:\n",
    "    print(f\"âœ… æ‰¾åˆ° {n_files} ä¸ªæ¨¡å‹æ–‡ä»¶ï¼Œä½¿ç”¨æœ€æ–°çš„: {os.path.basename(model_file)}\\n\")\n",
    "    \n",
    "    # æ‰§è¡Œæ£€æµ‹\n",
    "    report = check_saved_model_data_completeness(model_file)\n",
    "    print_check_report(report)\n",
    "    \n",
    "    # æ£€æµ‹ç»“æœåé¦ˆ\n",
    "    if report['status'] == 'complete':\n",
    "        print(\"\\nâœ… æ•°æ®å®Œæ•´æ€§æ£€æµ‹é€šè¿‡ï¼Œå¯ä»¥è¿›è¡Œåç»­ç»˜å›¾æ“ä½œ\")\n",
    "    elif report['status'] == 'incomplete':\n",
    "        print(\"\\nâš ï¸ æ•°æ®ä¸å®Œæ•´ï¼Œè¯·æ ¹æ®ä¸Šè¿°å»ºè®®ä¿®å¤åå†è¿›è¡Œç»˜å›¾\")\n",
    "    else:\n",
    "        print(\"\\nâŒ æ£€æµ‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼\")\n",
    "else:\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶\")\n",
    "    search_dir = os.path.abspath(os.path.normpath('../data/US_data/ML_model'))\n",
    "    print(f\"æœç´¢ç›®å½•: {search_dir}\")\n",
    "    print(f\"æœç´¢æ¨¡å¼: landuse_transformer_generation*.pkl\")\n",
    "    \n",
    "    if os.path.exists(search_dir):\n",
    "        print(f\"\\nç›®å½•å­˜åœ¨ï¼Œå†…å®¹é¢„è§ˆ:\")\n",
    "        files = os.listdir(search_dir)\n",
    "        pkl_files = [f for f in files if f.endswith('.pkl')]\n",
    "        print(f\"æ‰¾åˆ° {len(pkl_files)} ä¸ª .pkl æ–‡ä»¶:\")\n",
    "        for f in sorted(pkl_files)[:15]:\n",
    "            print(f\"  - {f}\")\n",
    "        \n",
    "        transformer_files = [f for f in pkl_files if 'transformer' in f.lower()]\n",
    "        if transformer_files:\n",
    "            print(f\"\\næ‰¾åˆ° {len(transformer_files)} ä¸ªåŒ…å« 'transformer' çš„æ–‡ä»¶:\")\n",
    "            for f in transformer_files[:5]:\n",
    "                print(f\"  - {f}\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ ç›®å½•ä¸å­˜åœ¨: {search_dir}\")\n",
    "        print(f\"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e99cae4",
   "metadata": {},
   "source": [
    "## 1.2 Plot for training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21085f78",
   "metadata": {},
   "source": [
    "### 1.2.1 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70347dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Dev\\\\Landuse_Zhong_clean\\\\data\\\\US_data\\\\ML_model\\\\landuse_transformer_generation_single_20251117_054315.pkl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def resolve_path(path, base_dir):\n",
    "    \"\"\"è§£æè·¯å¾„ï¼šç»å¯¹è·¯å¾„ç›´æ¥ä½¿ç”¨ï¼Œç›¸å¯¹è·¯å¾„åŸºäºbase_diræ‹¼æ¥\"\"\"\n",
    "    if not path:\n",
    "        return None\n",
    "    if os.path.isabs(path):\n",
    "        return os.path.normpath(path)\n",
    "    # ç›¸å¯¹è·¯å¾„ï¼šå…ˆå°è¯•ç›´æ¥æ‹¼æ¥\n",
    "    full_path = os.path.normpath(os.path.join(base_dir, path))\n",
    "    if os.path.exists(full_path):\n",
    "        return full_path\n",
    "    # å¦‚æœä¸å­˜åœ¨ï¼Œå°è¯•åªä½¿ç”¨æ–‡ä»¶åï¼ˆå¤„ç†è·¯å¾„é‡å¤çš„æƒ…å†µï¼‰\n",
    "    filename = os.path.basename(path)\n",
    "    return os.path.normpath(os.path.join(base_dir, filename))\n",
    "\n",
    "def load_model_data(main_model_file):\n",
    "    \"\"\"\n",
    "    åŠ è½½ä¿å­˜çš„æ¨¡å‹æ•°æ®\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        ä¸»æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆ.pklæ–‡ä»¶ï¼‰\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    data : dict\n",
    "        åŒ…å«æ‰€æœ‰æ¨¡å‹æ•°æ®çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    import joblib\n",
    "    import json\n",
    "    import numpy as np\n",
    "    \n",
    "    main_model_file = os.path.abspath(os.path.normpath(main_model_file))\n",
    "    model_dir = os.path.dirname(main_model_file)\n",
    "    \n",
    "    main_model = joblib.load(main_model_file)\n",
    "    config_path = resolve_path(main_model.get('config_path'), model_dir)\n",
    "    test_data_path = resolve_path(main_model.get('test_data_path'), model_dir)\n",
    "    \n",
    "    # åŠ è½½é…ç½®æ–‡ä»¶\n",
    "    if not config_path or not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}\")\n",
    "    \n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # åŠ è½½æµ‹è¯•æ•°æ®\n",
    "    test_data = None\n",
    "    if test_data_path and os.path.exists(test_data_path):\n",
    "        test_data = np.load(test_data_path, allow_pickle=True)\n",
    "    \n",
    "    return {\n",
    "        'config': config,\n",
    "        'test_data': test_data,\n",
    "        'main_model': main_model\n",
    "    }\n",
    "\n",
    "import glob\n",
    "\n",
    "# ç»Ÿä¸€æ¨¡å‹æ–‡ä»¶æœç´¢å‡½æ•°ï¼ˆå¦‚æœæ£€æµ‹éƒ¨åˆ†æœªè¿è¡Œï¼Œåœ¨æ­¤å®šä¹‰ï¼‰\n",
    "try:\n",
    "    _ = find_latest_model\n",
    "except NameError:\n",
    "    def find_latest_model(pattern='landuse_transformer_generation*.pkl', search_dir='../data/US_data/ML_model'):\n",
    "        \"\"\"æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶\"\"\"\n",
    "        abs_search_dir = os.path.abspath(os.path.normpath(search_dir))\n",
    "        abs_pattern = os.path.join(abs_search_dir, pattern)\n",
    "        model_files = glob.glob(abs_pattern)\n",
    "        if model_files:\n",
    "            model_files.sort(key=os.path.getmtime, reverse=True)\n",
    "            return model_files[0], len(model_files)\n",
    "        return None, 0\n",
    "\n",
    "# æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶\n",
    "model_file, n_files = find_latest_model()\n",
    "model_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698735c2",
   "metadata": {},
   "source": [
    "### 1.2.2 Plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "663c1b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: landuse_transformer_generation_single_20251117_054315.pkl\n",
      "\n",
      "âœ… å­¦ä¹ æ›²çº¿å›¾å·²ä¿å­˜: Supplymentary_figure\\Figure_learning_curve.png\n",
      "âœ… è¿‡æ‹Ÿåˆåˆ†æå›¾å·²ä¿å­˜: Supplymentary_figure\\Figure_overfitting_analysis.png\n",
      "âœ… æ–¹å·®åˆ†æå›¾å·²ä¿å­˜: Supplymentary_figure\\Figure_variance_analysis.png\n",
      "âœ… æ€§èƒ½æ”¹è¿›å›¾å·²ä¿å­˜: Supplymentary_figure\\Figure_performance_improvement.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# è®¾ç½®matplotlibå‚æ•°ï¼ˆå‚è€ƒ6.6 Figure1_Enviromental_plot.ipynbï¼‰\n",
    "# 60mm = 60/25.4 â‰ˆ 2.36 inches\n",
    "figsize_mm = 60\n",
    "figsize_inches = figsize_mm / 25.4\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 5,\n",
    "    'axes.titlesize': 5,\n",
    "    'axes.labelsize': 5,\n",
    "    'xtick.labelsize': 5,\n",
    "    'ytick.labelsize': 5,\n",
    "    'legend.fontsize': 5,\n",
    "    'font.family': 'Arial'\n",
    "})\n",
    "\n",
    "\n",
    "def plot_learning_curve(main_model_file, save_dir='Supplymentary_figure'):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶å­¦ä¹ æ›²çº¿å›¾\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        ä¸»æ¨¡å‹æ–‡ä»¶è·¯å¾„\n",
    "    save_dir : str\n",
    "        ä¿å­˜ç›®å½•\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    config = data['config']\n",
    "\n",
    "    lc_data = config['learning_curve_analysis']\n",
    "    train_sizes = np.array(lc_data.get('train_sizes', []))\n",
    "    train_scores_mean = np.array(lc_data.get('train_scores_mean', []))\n",
    "    train_scores_std = np.array(lc_data.get('train_scores_std', []))\n",
    "    val_scores_mean = np.array(lc_data.get('val_scores_mean', []))\n",
    "    val_scores_std = np.array(lc_data.get('val_scores_std', []))\n",
    "\n",
    "    # åˆ›å»ºå›¾å½¢\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "\n",
    "    # ç»˜åˆ¶è®­ç»ƒé›†æ›²çº¿ï¼ˆå¸¦è¯¯å·®æ¡ï¼‰\n",
    "    ax.plot(train_sizes, train_scores_mean, 'o-', color='#1F78B4', \n",
    "            label='Training Score', linewidth=1.5, markersize=3)\n",
    "    ax.fill_between(train_sizes, \n",
    "                    train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std,\n",
    "                    alpha=0.2, color='#1F78B4')\n",
    "\n",
    "    # ç»˜åˆ¶éªŒè¯é›†æ›²çº¿ï¼ˆå¸¦è¯¯å·®æ¡ï¼‰\n",
    "    ax.plot(train_sizes, val_scores_mean, 's-', color='#E31A1C',\n",
    "            label='Validation Score', linewidth=1.5, markersize=3)\n",
    "    ax.fill_between(train_sizes,\n",
    "                    val_scores_mean - val_scores_std,\n",
    "                    val_scores_mean + val_scores_std,\n",
    "                    alpha=0.2, color='#E31A1C')\n",
    "\n",
    "    ax.set_xlabel('Training Set Size', fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontweight='bold')\n",
    "    ax.set_title('Learning Curve', fontweight='bold')\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ä¿å­˜\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_learning_curve.png')\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches='tight', format='png')\n",
    "    print(f\"âœ… å­¦ä¹ æ›²çº¿å›¾å·²ä¿å­˜: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_overfitting_analysis(main_model_file, save_dir='Supplymentary_figure'):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶è¿‡æ‹Ÿåˆåˆ†æå›¾\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        ä¸»æ¨¡å‹æ–‡ä»¶è·¯å¾„\n",
    "    save_dir : str\n",
    "        ä¿å­˜ç›®å½•\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    config = data['config']\n",
    "\n",
    "    lc_data = config['learning_curve_analysis']\n",
    "    of_data = lc_data['overfitting_analysis']\n",
    "    train_sizes = np.array(lc_data.get('train_sizes', []))\n",
    "    train_scores_mean = np.array(lc_data.get('train_scores_mean', []))\n",
    "    val_scores_mean = np.array(lc_data.get('val_scores_mean', []))\n",
    "\n",
    "    # è®¡ç®—è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¹‹é—´çš„å·®è·\n",
    "    gap = train_scores_mean - val_scores_mean\n",
    "\n",
    "    # åˆ›å»ºå›¾å½¢\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "\n",
    "    # ç»˜åˆ¶å·®è·æ›²çº¿\n",
    "    ax.plot(train_sizes, gap, 'o-', color='#FF7F00', \n",
    "            linewidth=1.5, markersize=3, label='Train-Val Gap')\n",
    "\n",
    "    # æ·»åŠ è¿‡æ‹Ÿåˆé˜ˆå€¼çº¿ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "    if 'overfitting_level' in of_data:\n",
    "        of_level = of_data['overfitting_level']\n",
    "        if of_level == 'high':\n",
    "            threshold = 0.1  # ç¤ºä¾‹é˜ˆå€¼\n",
    "            ax.axhline(y=threshold, color='r', linestyle='--', \n",
    "                      linewidth=1, alpha=0.7, label='Overfitting Threshold')\n",
    "\n",
    "    ax.set_xlabel('Training Set Size', fontweight='bold')\n",
    "    ax.set_ylabel('Train-Val Score Gap', fontweight='bold')\n",
    "    ax.set_title('Overfitting Analysis', fontweight='bold')\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # ä¿å­˜\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_overfitting_analysis.png')\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches='tight', format='png')\n",
    "    print(f\"âœ… è¿‡æ‹Ÿåˆåˆ†æå›¾å·²ä¿å­˜: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_variance_analysis(main_model_file, save_dir='Supplymentary_figure'):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶æ–¹å·®åˆ†æå›¾\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        ä¸»æ¨¡å‹æ–‡ä»¶è·¯å¾„\n",
    "    save_dir : str\n",
    "        ä¿å­˜ç›®å½•\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    config = data['config']\n",
    "\n",
    "    # å–æ¶ˆæ•°æ®æ£€æŸ¥ï¼Œç›´æ¥æå–æ•°æ®ï¼ˆå‡è®¾è¿™äº›å­—æ®µå¿…å®šå­˜åœ¨ï¼‰\n",
    "    lc_data = config['learning_curve_analysis']\n",
    "    train_sizes = np.array(lc_data['train_sizes'])\n",
    "    train_scores_std = np.array(lc_data['train_scores_std'])\n",
    "    val_scores_std = np.array(lc_data['val_scores_std'])\n",
    "\n",
    "    # åˆ›å»ºå›¾å½¢\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "\n",
    "    # ç»˜åˆ¶æ ‡å‡†å·®æ›²çº¿\n",
    "    ax.plot(train_sizes, train_scores_std, 'o-', color='#1F78B4',\n",
    "            linewidth=1.5, markersize=3, label='Training Std')\n",
    "    ax.plot(train_sizes, val_scores_std, 's-', color='#E31A1C',\n",
    "            linewidth=1.5, markersize=3, label='Validation Std')\n",
    "\n",
    "    ax.set_xlabel('Training Set Size', fontweight='bold')\n",
    "    ax.set_ylabel('Score Std Dev', fontweight='bold')\n",
    "    ax.set_title('Variance Analysis', fontweight='bold')\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ä¿å­˜\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_variance_analysis.png')\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches='tight', format='png')\n",
    "    print(f\"âœ… æ–¹å·®åˆ†æå›¾å·²ä¿å­˜: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_performance_improvement(main_model_file, save_dir='Supplymentary_figure'):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶æ€§èƒ½æ”¹è¿›å›¾ï¼ˆè®­ç»ƒå†å²ï¼‰\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        ä¸»æ¨¡å‹æ–‡ä»¶è·¯å¾„\n",
    "    save_dir : str\n",
    "        ä¿å­˜ç›®å½•\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    config = data['config']\n",
    "\n",
    "    # å–æ¶ˆæ•°æ®æ£€æŸ¥ï¼Œç›´æ¥æå–æ•°æ®ï¼ˆå‡è®¾è¿™äº›å­—æ®µå¿…å®šå­˜åœ¨ï¼‰\n",
    "    history = config['training_history']\n",
    "\n",
    "    # é»˜è®¤ä¼˜å…ˆaccuracyï¼Œæ²¡æœ‰åˆ™ç”¨loss\n",
    "    if 'accuracy' in history and 'val_accuracy' in history:\n",
    "        train_metric = np.array(history['accuracy'])\n",
    "        val_metric = np.array(history['val_accuracy'])\n",
    "        metric_name = 'Accuracy'\n",
    "        ylabel = 'Accuracy'\n",
    "    else:\n",
    "        train_metric = np.array(history['loss'])\n",
    "        val_metric = np.array(history['val_loss'])\n",
    "        metric_name = 'Loss'\n",
    "        ylabel = 'Loss'\n",
    "\n",
    "    epochs = range(1, len(train_metric) + 1)\n",
    "\n",
    "    # åˆ›å»ºå›¾å½¢\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "\n",
    "    # ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æ›²çº¿\n",
    "    ax.plot(epochs, train_metric, 'o-', color='#1F78B4',\n",
    "            linewidth=1.5, markersize=3, label=f'Training {metric_name}')\n",
    "    ax.plot(epochs, val_metric, 's-', color='#E31A1C',\n",
    "            linewidth=1.5, markersize=3, label=f'Validation {metric_name}')\n",
    "\n",
    "    ax.set_xlabel('Epochs', fontweight='bold')\n",
    "    ax.set_ylabel(ylabel, fontweight='bold')\n",
    "    ax.set_title('Performance Improvement', fontweight='bold')\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ä¿å­˜\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_performance_improvement.png')\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches='tight', format='png')\n",
    "    print(f\"âœ… æ€§èƒ½æ”¹è¿›å›¾å·²ä¿å­˜: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "if model_file:\n",
    "    print(f\"âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {os.path.basename(model_file)}\\n\")\n",
    "    \n",
    "    # ç»˜åˆ¶æ‰€æœ‰å›¾è¡¨\n",
    "    plot_learning_curve(model_file)\n",
    "    plot_overfitting_analysis(model_file)\n",
    "    plot_variance_analysis(model_file)\n",
    "    plot_performance_improvement(model_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a13e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_loss(main_model_file, save_dir='Supplymentary_figure'):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        ä¸»æ¨¡å‹æ–‡ä»¶è·¯å¾„\n",
    "    save_dir : str\n",
    "        ä¿å­˜ç›®å½•\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    config = data['config']\n",
    "    \n",
    "    if 'training_history' not in config:\n",
    "        print(\"âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒå†å²æ•°æ®\")\n",
    "        return\n",
    "    \n",
    "    history = config['training_history']\n",
    "    \n",
    "    if 'loss' not in history or 'val_loss' not in history:\n",
    "        print(\"âš ï¸ æœªæ‰¾åˆ°lossæ•°æ®\")\n",
    "        return\n",
    "    \n",
    "    train_loss = np.array(history['loss'])\n",
    "    val_loss = np.array(history['val_loss'])\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    \n",
    "    # åˆ›å»ºå›¾å½¢\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "    \n",
    "    # ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿\n",
    "    ax.plot(epochs, train_loss, 'o-', color='#1F78B4',\n",
    "            linewidth=1.5, markersize=3, label='Training Loss')\n",
    "    ax.plot(epochs, val_loss, 's-', color='#E31A1C',\n",
    "            linewidth=1.5, markersize=3, label='Validation Loss')\n",
    "    \n",
    "    ax.set_xlabel('Epochs', fontweight='bold')\n",
    "    ax.set_ylabel('Loss', fontweight='bold')\n",
    "    ax.set_title('Training Loss', fontweight='bold')\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_training_loss.png')\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches='tight', format='png')\n",
    "    print(f\"âœ… è®­ç»ƒæŸå¤±å›¾å·²ä¿å­˜: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(main_model_file, save_dir='Supplymentary_figure'):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶ROCæ›²çº¿\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        ä¸»æ¨¡å‹æ–‡ä»¶è·¯å¾„\n",
    "    save_dir : str\n",
    "        ä¿å­˜ç›®å½•\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    test_data = data['test_data']\n",
    "    \n",
    "    if test_data is None:\n",
    "        print(\"âš ï¸ æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®\")\n",
    "        return\n",
    "    \n",
    "    if 'fpr' not in test_data or 'tpr' not in test_data or 'test_auc' not in test_data:\n",
    "        print(\"âš ï¸ æœªæ‰¾åˆ°ROCæ•°æ®\")\n",
    "        return\n",
    "    \n",
    "    fpr = test_data['fpr']\n",
    "    tpr = test_data['tpr']\n",
    "    test_auc = float(test_data['test_auc'])\n",
    "    \n",
    "    # åˆ›å»ºå›¾å½¢\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "    \n",
    "    # ç»˜åˆ¶ROCæ›²çº¿\n",
    "    ax.plot(fpr, tpr, color='#1F78B4', linewidth=1.5, \n",
    "            label=f'ROC Curve (AUC = {test_auc:.3f})')\n",
    "    \n",
    "    # ç»˜åˆ¶å¯¹è§’çº¿ï¼ˆéšæœºåˆ†ç±»å™¨ï¼‰\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5, label='Random Classifier')\n",
    "    \n",
    "    ax.set_xlabel('False Positive Rate', fontweight='bold')\n",
    "    ax.set_ylabel('True Positive Rate', fontweight='bold')\n",
    "    ax.set_title('ROC Curve', fontweight='bold')\n",
    "    ax.legend(frameon=False, loc='lower right')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_roc_curve.png')\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches='tight', format='png')\n",
    "    print(f\"âœ… ROCæ›²çº¿å›¾å·²ä¿å­˜: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_predicted_probability_distribution(main_model_file, save_dir='Supplymentary_figure'):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        ä¸»æ¨¡å‹æ–‡ä»¶è·¯å¾„\n",
    "    save_dir : str\n",
    "        ä¿å­˜ç›®å½•\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    test_data = data['test_data']\n",
    "    \n",
    "    if test_data is None:\n",
    "        print(\"âš ï¸ æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®\")\n",
    "        return\n",
    "    \n",
    "    if 'y_test_pred' not in test_data or 'y_test' not in test_data:\n",
    "        print(\"âš ï¸ æœªæ‰¾åˆ°é¢„æµ‹æ¦‚ç‡æˆ–çœŸå®æ ‡ç­¾æ•°æ®\")\n",
    "        return\n",
    "    \n",
    "    y_test_pred = test_data['y_test_pred']\n",
    "    y_test = test_data['y_test']\n",
    "    \n",
    "    # åˆ†ç¦»æ­£è´Ÿæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡\n",
    "    pos_probs = y_test_pred[y_test == 1]\n",
    "    neg_probs = y_test_pred[y_test == 0]\n",
    "    \n",
    "    # åˆ›å»ºå›¾å½¢\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "    \n",
    "    # ç»˜åˆ¶ç›´æ–¹å›¾\n",
    "    bins = np.linspace(0, 1, 51)\n",
    "    ax.hist(neg_probs, bins=bins, alpha=0.6, color='#E31A1C', \n",
    "            label='Negative Class', density=True, edgecolor='black', linewidth=0.5)\n",
    "    ax.hist(pos_probs, bins=bins, alpha=0.6, color='#1F78B4', \n",
    "            label='Positive Class', density=True, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Predicted Probability', fontweight='bold')\n",
    "    ax.set_ylabel('Density', fontweight='bold')\n",
    "    ax.set_title('Predicted Probability Distribution', fontweight='bold')\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5, axis='y')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_xlim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_predicted_probability_distribution.png')\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches='tight', format='png')\n",
    "    print(f\"âœ… é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå›¾å·²ä¿å­˜: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_performance_metrics(main_model_file, save_dir='Supplymentary_figure'):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”å›¾\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    main_model_file : str\n",
    "        ä¸»æ¨¡å‹æ–‡ä»¶è·¯å¾„\n",
    "    save_dir : str\n",
    "        ä¿å­˜ç›®å½•\n",
    "    \"\"\"\n",
    "    data = load_model_data(main_model_file)\n",
    "    config = data['config']\n",
    "    \n",
    "    if 'training_metrics' not in config:\n",
    "        print(\"âš ï¸ æœªæ‰¾åˆ°æ€§èƒ½æŒ‡æ ‡æ•°æ®\")\n",
    "        return\n",
    "    \n",
    "    metrics = config['training_metrics']\n",
    "    \n",
    "    # æå–æŒ‡æ ‡\n",
    "    metric_names = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    splits = ['train', 'val', 'test']\n",
    "    \n",
    "    # å‡†å¤‡æ•°æ®\n",
    "    data_dict = {}\n",
    "    for split in splits:\n",
    "        if split not in metrics:\n",
    "            continue\n",
    "        data_dict[split] = [metrics[split].get(m, 0) for m in metric_names]\n",
    "    \n",
    "    if not data_dict:\n",
    "        print(\"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ€§èƒ½æŒ‡æ ‡æ•°æ®\")\n",
    "        return\n",
    "    \n",
    "    # åˆ›å»ºå›¾å½¢\n",
    "    fig, ax = plt.subplots(figsize=(figsize_inches, figsize_inches))\n",
    "    \n",
    "    # è®¾ç½®xè½´ä½ç½®\n",
    "    x = np.arange(len(metric_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    # é¢œè‰²\n",
    "    colors = {'train': '#1F78B4', 'val': '#E31A1C', 'test': '#33A02C'}\n",
    "    labels = {'train': 'Train', 'val': 'Validation', 'test': 'Test'}\n",
    "    \n",
    "    # ç»˜åˆ¶æŸ±çŠ¶å›¾\n",
    "    offset = -width\n",
    "    for split in splits:\n",
    "        if split in data_dict:\n",
    "            ax.bar(x + offset, data_dict[split], width, \n",
    "                  label=labels[split], color=colors[split], \n",
    "                  edgecolor='black', linewidth=0.5, alpha=0.8)\n",
    "            offset += width\n",
    "    \n",
    "    ax.set_xlabel('Metrics', fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontweight='bold')\n",
    "    ax.set_title('Performance Metrics', fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([m.capitalize() for m in metric_names])\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5, axis='y')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'Figure_performance_metrics.png')\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches='tight', format='png')\n",
    "    print(f\"âœ… æ€§èƒ½æŒ‡æ ‡å›¾å·²ä¿å­˜: {save_path}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b8a94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: landuse_transformer_generation_single_20251117_054315.pkl\n",
      "\n",
      "âœ… è®­ç»ƒæŸå¤±å›¾å·²ä¿å­˜: Supplymentary_figure\\Figure_training_loss.png\n",
      "âœ… ROCæ›²çº¿å›¾å·²ä¿å­˜: Supplymentary_figure\\Figure_roc_curve.png\n",
      "âœ… é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå›¾å·²ä¿å­˜: Supplymentary_figure\\Figure_predicted_probability_distribution.png\n",
      "âœ… æ€§èƒ½æŒ‡æ ‡å›¾å·²ä¿å­˜: Supplymentary_figure\\Figure_performance_metrics.png\n"
     ]
    }
   ],
   "source": [
    "# æ‰§è¡Œæ–°æ·»åŠ çš„ç»˜å›¾å‡½æ•°\n",
    "# æ³¨æ„ï¼šéœ€è¦å…ˆè¿è¡Œ cell 5 ä»¥è·å– model_file å˜é‡\n",
    "\n",
    "if 'model_file' in locals() and model_file:\n",
    "    print(f\"âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {os.path.basename(model_file)}\\n\")\n",
    "    \n",
    "    # ç»˜åˆ¶æ–°å¢çš„å›¾è¡¨\n",
    "    plot_training_loss(model_file)\n",
    "    plot_roc_curve(model_file)\n",
    "    plot_predicted_probability_distribution(model_file)\n",
    "    plot_performance_metrics(model_file)\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
