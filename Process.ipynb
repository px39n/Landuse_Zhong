{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DEFINE THE FUNCTION\n",
    "\n",
    "\n",
    "**Reclassification Logic**\n",
    "\n",
    "The `relclass` function reclassifies ESA CCI land cover classes into simplified categories:\n",
    "\n",
    "| New Class | Description | Original ESA CCI Classes |\n",
    "|-----------|-------------|--------------------------|\n",
    "| 1 | Cropland | 10-40 |\n",
    "| 2 | Forest | 50-90, 160, 170 |\n",
    "| 3 | Savanna | 100, 110 |\n",
    "| 4 | Shrub | 120, 121, 122 |\n",
    "| 5 | Grassland and Arid Ecosystem | 130-153 |\n",
    "| 6 | Wetland | 180 |\n",
    "| 7 | Built-up | 190 |\n",
    "| 8 | Bare Area and Ice | 200, 201, 202 |\n",
    "| 9 | Water | 210 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def relclass(landcover_ds):\n",
    "    # 创建一个新的Dataset\n",
    "    reclassed = landcover_ds.copy()\n",
    "    \n",
    "    # 获取要处理的变量\n",
    "    landcover_da = landcover_ds['lccs_class']\n",
    "\n",
    "    #cropland\n",
    "    cropland_mask = (landcover_da >= 10) & (landcover_da <= 40)\n",
    "    reclassed['lccs_class'] = reclassed['lccs_class'].where(~cropland_mask, 1)\n",
    "    \n",
    "    #forest\n",
    "    forest_mask = ((landcover_da >= 50) & (landcover_da <= 90)) \\\n",
    "                  | (landcover_da == 160) | (landcover_da == 170)\n",
    "    reclassed['lccs_class'] = reclassed['lccs_class'].where(~forest_mask, 2)\n",
    "    \n",
    "    #savanna\n",
    "    savanna_mask = (landcover_da == 100) | (landcover_da == 110)\n",
    "    reclassed['lccs_class'] = reclassed['lccs_class'].where(~savanna_mask, 3)\n",
    "    \n",
    "    #shrub\n",
    "    shrub_mask = (landcover_da == 120) | (landcover_da == 121) | (landcover_da == 122)\n",
    "    reclassed['lccs_class'] = reclassed['lccs_class'].where(~shrub_mask, 4)\n",
    "    \n",
    "    #grassland and arid ecosystem\n",
    "    grassland_mask = ((landcover_da >= 130) & (landcover_da <= 153))\n",
    "    reclassed['lccs_class'] = reclassed['lccs_class'].where(~grassland_mask, 5)\n",
    "    \n",
    "    #wetland\n",
    "    wetland_mask = (landcover_da == 180)\n",
    "    reclassed['lccs_class'] = reclassed['lccs_class'].where(~wetland_mask, 6)\n",
    "    \n",
    "    #built-up\n",
    "    built_up_mask = (landcover_da == 190)\n",
    "    reclassed['lccs_class'] = reclassed['lccs_class'].where(~built_up_mask, 7)\n",
    "    \n",
    "    #bare area and ice\n",
    "    bare_area_mask = (landcover_da == 200) | (landcover_da == 201) | (landcover_da == 202)\n",
    "    reclassed['lccs_class'] = reclassed['lccs_class'].where(~bare_area_mask, 8)\n",
    "    \n",
    "    #water\n",
    "    water_mask = (landcover_da == 210)\n",
    "    reclassed['lccs_class'] = reclassed['lccs_class'].where(~water_mask, 9)\n",
    "\n",
    "    return reclassed\n",
    "\n",
    "# 定义统一的chunk大小\n",
    "CHUNK_SIZE = 'auto'  # 使用文件原有的chunk大小\n",
    "\n",
    "# 读取数据\n",
    "# lc = xr.open_dataset(\n",
    "#     r'data\\merged_lccs.nc',\n",
    "#     chunks=CHUNK_SIZE\n",
    "# )\n",
    "\n",
    "# 重分类处理\n",
    "# reclassed_ds = relclass(lc)\n",
    "\n",
    "# 设置输出路径\n",
    "output_path = r'output\\merged_lccs.nc'\n",
    "\n",
    "# # 使用dask进行计算并保存\n",
    "# with ProgressBar():\n",
    "#     reclassed_ds.to_netcdf(\n",
    "#         output_path,\n",
    "#         engine=\"netcdf4\",\n",
    "#         format=\"NETCDF4\",\n",
    "#         encoding={\"lccs_class\": {\"zlib\": True, \"complevel\": 4}},\n",
    "#         compute=True\n",
    "#     )\n",
    "\n",
    "# print(\"Saved the reclassed result!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reclassed_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Identify the cropland abandonment\n",
    "\n",
    "The identification of cropland abandonment follows these steps:\n",
    "\n",
    "1. Start with pixels that were cropland in the initial year\n",
    "2. For each subsequent year, check if:\n",
    "   - The pixel changed from cropland (class 1) to non-cropland/non-built-up (not class 1 or 7)\n",
    "   - This change persisted for at least 5 consecutive years\n",
    "   - The pixel was cropland in the year before the change\n",
    "\n",
    "3. For pixels meeting these criteria, record the first year when abandonment occurred\n",
    "4. Create a spatial map showing:\n",
    "   - The year of abandonment for abandoned croplands\n",
    "   - No data (NaN) for pixels that were never abandoned\n",
    "\n",
    "This approach allows us to track the temporal patterns of cropland abandonment across the study area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\xarray\\core\\computation.py:2223: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  data = dask_coord[duck_array_ops.ravel(indx.data)]\n",
      "c:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\array\\slicing.py:1084: PerformanceWarning: Increasing number of chunks by factor of 16\n",
      "  p = blockwise(\n",
      "c:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\xarray\\core\\dataarray.py:5962: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  return computation._calc_idxminmax(\n",
      "c:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\array\\core.py:4830: PerformanceWarning: Increasing number of chunks by factor of 13\n",
      "  result = blockwise(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 1% Completed | 525.96 ss\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 820. MiB for an array with shape (2, 5184, 10368) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar():\n\u001b[0;32m     86\u001b[0m     abandonment_year_da\u001b[38;5;241m=\u001b[39mabandonment_year_da\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m---> 87\u001b[0m     abandonment_year_da\u001b[38;5;241m.\u001b[39mto_netcdf(\n\u001b[0;32m     88\u001b[0m         output_path2,\n\u001b[0;32m     89\u001b[0m         engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetcdf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNETCDF4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     91\u001b[0m         encoding\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabandonment_year\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzlib\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplevel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m}}\n\u001b[0;32m     92\u001b[0m     )\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved the final result!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\xarray\\core\\dataarray.py:4095\u001b[0m, in \u001b[0;36mDataArray.to_netcdf\u001b[1;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4092\u001b[0m     \u001b[38;5;66;03m# No problems with the name - so we're fine!\u001b[39;00m\n\u001b[0;32m   4093\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dataset()\n\u001b[1;32m-> 4095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_netcdf(  \u001b[38;5;66;03m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;00m\n\u001b[0;32m   4096\u001b[0m     dataset,\n\u001b[0;32m   4097\u001b[0m     path,\n\u001b[0;32m   4098\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   4099\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m   4100\u001b[0m     group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[0;32m   4101\u001b[0m     engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m   4102\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   4103\u001b[0m     unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims,\n\u001b[0;32m   4104\u001b[0m     compute\u001b[38;5;241m=\u001b[39mcompute,\n\u001b[0;32m   4105\u001b[0m     multifile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   4106\u001b[0m     invalid_netcdf\u001b[38;5;241m=\u001b[39minvalid_netcdf,\n\u001b[0;32m   4107\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\xarray\\backends\\api.py:1348\u001b[0m, in \u001b[0;36mto_netcdf\u001b[1;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multifile:\n\u001b[0;32m   1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m writer, store\n\u001b[1;32m-> 1348\u001b[0m writes \u001b[38;5;241m=\u001b[39m writer\u001b[38;5;241m.\u001b[39msync(compute\u001b[38;5;241m=\u001b[39mcompute)\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, BytesIO):\n\u001b[0;32m   1351\u001b[0m     store\u001b[38;5;241m.\u001b[39msync()\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\xarray\\backends\\common.py:297\u001b[0m, in \u001b[0;36mArrayWriter.sync\u001b[1;34m(self, compute, chunkmanager_store_kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunkmanager_store_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    295\u001b[0m     chunkmanager_store_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 297\u001b[0m delayed_store \u001b[38;5;241m=\u001b[39m chunkmanager\u001b[38;5;241m.\u001b[39mstore(\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msources,\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets,\n\u001b[0;32m    300\u001b[0m     lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock,\n\u001b[0;32m    301\u001b[0m     compute\u001b[38;5;241m=\u001b[39mcompute,\n\u001b[0;32m    302\u001b[0m     flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    303\u001b[0m     regions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregions,\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchunkmanager_store_kwargs,\n\u001b[0;32m    305\u001b[0m )\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msources \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\xarray\\namedarray\\daskmanager.py:249\u001b[0m, in \u001b[0;36mDaskManager.store\u001b[1;34m(self, sources, targets, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstore\u001b[39m(\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    243\u001b[0m     sources: Any \u001b[38;5;241m|\u001b[39m Sequence[Any],\n\u001b[0;32m    244\u001b[0m     targets: Any,\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m store\n\u001b[1;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m store(\n\u001b[0;32m    250\u001b[0m         sources\u001b[38;5;241m=\u001b[39msources,\n\u001b[0;32m    251\u001b[0m         targets\u001b[38;5;241m=\u001b[39mtargets,\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    253\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\array\\core.py:1236\u001b[0m, in \u001b[0;36mstore\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute:\n\u001b[0;32m   1235\u001b[0m     store_dsk \u001b[38;5;241m=\u001b[39m HighLevelGraph(layers, dependencies)\n\u001b[1;32m-> 1236\u001b[0m     compute_as_if_collection(Array, store_dsk, map_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\base.py:341\u001b[0m, in \u001b[0;36mcompute_as_if_collection\u001b[1;34m(cls, dsk, keys, scheduler, get, **kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m schedule \u001b[38;5;241m=\u001b[39m get_scheduler(scheduler\u001b[38;5;241m=\u001b[39mscheduler, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, get\u001b[38;5;241m=\u001b[39mget)\n\u001b[0;32m    340\u001b[0m dsk2 \u001b[38;5;241m=\u001b[39m optimization_function(\u001b[38;5;28mcls\u001b[39m)(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m schedule(dsk2, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[0;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[1;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m get_async(\n\u001b[0;32m     90\u001b[0m     pool\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[0;32m     91\u001b[0m     pool\u001b[38;5;241m.\u001b[39m_max_workers,\n\u001b[0;32m     92\u001b[0m     dsk,\n\u001b[0;32m     93\u001b[0m     keys,\n\u001b[0;32m     94\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[0;32m     95\u001b[0m     get_id\u001b[38;5;241m=\u001b[39m_thread_get_id,\n\u001b[0;32m     96\u001b[0m     pack_exception\u001b[38;5;241m=\u001b[39mpack_exception,\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[1;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         raise_exception(exc, tb)\n\u001b[0;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[0;32m    513\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(exc, tb)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[1;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[1;32m--> 224\u001b[0m     result \u001b[38;5;241m=\u001b[39m _execute_task(task, data)\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[0;32m    226\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[1;34m(arg, cache, dsk)\u001b[0m\n\u001b[0;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m(_execute_task(a, cache) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\optimization.py:990\u001b[0m, in \u001b[0;36mSubgraphCallable.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys):\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m args, got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys), \u001b[38;5;28mlen\u001b[39m(args)))\n\u001b[1;32m--> 990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m core\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdsk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutkey, \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys, args)))\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\core.py:149\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, out, cache)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m toposort(dsk):\n\u001b[0;32m    148\u001b[0m     task \u001b[38;5;241m=\u001b[39m dsk[key]\n\u001b[1;32m--> 149\u001b[0m     result \u001b[38;5;241m=\u001b[39m _execute_task(task, cache)\n\u001b[0;32m    150\u001b[0m     cache[key] \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    151\u001b[0m result \u001b[38;5;241m=\u001b[39m _execute_task(out, cache)\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[1;34m(arg, cache, dsk)\u001b[0m\n\u001b[0;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m(_execute_task(a, cache) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\utils.py:73\u001b[0m, in \u001b[0;36mapply\u001b[1;34m(func, args, kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Apply a function given its positional and keyword arguments.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03mEquivalent to ``func(*args, **kwargs)``\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m>>> dsk = {'task-name': task}  # adds the task to a low level Dask task graph\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\zpy10\\anaconda3\\envs\\glbcropland\\Lib\\site-packages\\dask\\array\\chunk.py:276\u001b[0m, in \u001b[0;36mastype\u001b[1;34m(x, astype_dtype, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mastype\u001b[39m(x, astype_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mastype(astype_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 820. MiB for an array with shape (2, 5184, 10368) and data type object"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "\n",
    "\n",
    "    \n",
    "def identify_cropland_abandonment(reclassed_ds):\n",
    "    \"\"\"\n",
    "    Open a large netCDF file (with land cover classes), identify cropland abandonment,\n",
    "    and save the result as a compressed netCDF4 file in C:\\\\PhDart\\\\DATA.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reclassed_ds : xarray.Dataset\n",
    "        Dataset containing 'lccs_class' variable with land cover classifications.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.DataArray\n",
    "        2D array (lat, lon) of the first year of abandonment or NaN if never abandoned.\n",
    "        Coordinates, chunking, etc., retained until final compute.\n",
    "    \"\"\"\n",
    "    # 1) Extract the DataArray from the Dataset\n",
    "    reclassed_da = reclassed_ds['lccs_class']\n",
    "    \n",
    "    # Auto-chunk if not already chunked\n",
    "    if not reclassed_da.chunks:\n",
    "        reclassed_da = reclassed_da.chunk('auto')\n",
    "\n",
    "    # 2) Define conditions:\n",
    "    #    - Cropland = 1, Built-up = 7\n",
    "    #    - Must be cropland in first year (time=0)\n",
    "    cropland_initial_mask = (reclassed_da.isel(time=0) == 1)\n",
    "\n",
    "    # 3) Non-cropland means not 1 and not 7\n",
    "    is_non_cropland = (reclassed_da != 1) & (reclassed_da != 7)\n",
    "\n",
    "    # 4) Rolling 5-year window\n",
    "    consecutive_5 = (\n",
    "        is_non_cropland\n",
    "        .rolling(time=5, min_periods=5)\n",
    "        .construct(\"window_dim\")\n",
    "        .all(\"window_dim\")\n",
    "    )\n",
    "\n",
    "    # 5) Previous year must be cropland\n",
    "    prev_year_cropland = (reclassed_da.shift(time=1) == 1)\n",
    "\n",
    "    # 6) Combine conditions; fill NaNs with False\n",
    "    abandonment_condition = (consecutive_5 & prev_year_cropland).fillna(False)\n",
    "\n",
    "    # 7) Identify if a pixel ever meets the condition\n",
    "    ever_abandoned = abandonment_condition.any(dim=\"time\")\n",
    "\n",
    "    # 8) idxmax gives the first time coordinate where True occurs\n",
    "    first_abandonment_datetime = abandonment_condition.idxmax(dim=\"time\")\n",
    "\n",
    "    # Where condition never occurs or was never cropland initially, set to NaT\n",
    "    mask = ever_abandoned & cropland_initial_mask\n",
    "    first_abandonment_datetime = first_abandonment_datetime.where(mask, np.datetime64(\"NaT\"))\n",
    "\n",
    "    # 9) Convert datetime -> year (float). Fill missing with 0, then revert 0 to NaN\n",
    "    abandonment_year = first_abandonment_datetime.dt.year\n",
    "    abandonment_year = abandonment_year.astype(float).fillna(0)\n",
    "    abandonment_year = abandonment_year.where(abandonment_year > 0, np.nan)\n",
    "\n",
    "    return abandonment_year.rename(\"abandonment_year\")\n",
    "\n",
    "# 读取数据\n",
    "with dask.config.set(**{\n",
    "    'array.slicing.split_large_chunks': True,\n",
    "    'array.slicing.split_large_chunks_limit': '128 MiB'\n",
    "}):\n",
    "    rc = xr.open_dataset(\n",
    "        output_path,\n",
    "        engine=\"netcdf4\",\n",
    "        chunks=\"auto\"  # 使用自动分块\n",
    "    )\n",
    "\n",
    "# 处理数据识别耕地撂荒\n",
    "abandonment_year_da = identify_cropland_abandonment(rc)\n",
    "\n",
    "output_path2=r\"output\\abandonment_year_computed_V1.nc\"\n",
    "\n",
    "\n",
    "\n",
    "with ProgressBar():\n",
    "    abandonment_year_da=abandonment_year_da.chunk(\"auto\" )\n",
    "    abandonment_year_da.to_netcdf(\n",
    "        output_path2,\n",
    "        engine=\"netcdf4\",\n",
    "        format=\"NETCDF4\",\n",
    "        encoding={\"abandonment_year\": {\"zlib\": True, \"complevel\": 4}}\n",
    "    )\n",
    "print(\"Saved the final result!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全球撂荒耕地对气候行动的潜在贡献被显著低估\n",
    "# 应该慎重考虑耕地撂荒再开发以避免可持续发展目标or人类福祉受损\n",
    "\n",
    "ceteris paribus(to control the other factors unvaried)\n",
    "\n",
    "Agricultutal land abandonment: The FAO defines as agricultual land a portion of land dedicated to food production, and if this condition ceases for a period longer than 5 years, the land is considered abandoned.\n",
    "\n",
    "\n",
    "## 1. cropland abandonment(1992-2022):\n",
    "firstly, combine 31 datasets into one, this step is to make sure the data is time continuous. \n",
    "\n",
    "core aim: to identify the cropland abandonment, the abdonment could be defined as rules as:\n",
    "\n",
    "caculation 1: expansion cropland but persistent abandonment  \n",
    "caculation 2: stable cropland and persistent abandonment \n",
    "caculation 3: abandonment and recultivation (abandonment and built-up should alse be excluded)\n",
    "\n",
    "before calculation, the data should be reclassified to 1992-2022, and the reclassification rules should follow the following rules[1](https://www.nature.com/articles/s41597-020-00599-8#:~:text=Independently%20validating%20a%20glob):\n",
    "\n",
    "1. forest(50:50-90,160,170)\n",
    "2. savanna(100,110)\n",
    "3. cropland(10:10-40)\n",
    "4. built-up(190)\n",
    "5. wetland(180)\n",
    "6. shrub(120:120-122)\n",
    "7. arid ecosystem(150:140,150-153)\n",
    "8. bare area and ice(200:200-202)\n",
    "9. water(210)\n",
    "10. grassland(130)\n",
    "\n",
    "===========================================================\n",
    "small calculation:\n",
    "1) in calculation 1, to see the interval of expansion pixels and abandonment, the first time of expansion, the first time of abandonment; \n",
    "2) in calculation 3, to see the persistence of abandonment;\n",
    "3) in calculation 2, to see the first time of cropland abandonment;\n",
    "\n",
    "(each condition should be independent)\n",
    "\n",
    "--->---calcluation 1: expansion cropland but persistent abandonment\n",
    "\n",
    "aim: to see if the cropland expansion hotspot shift to Global South and to see if the hotspot transformation is due to the Global North cropland abandonment.\n",
    "\n",
    "1) _**effective change**_:change to cropland and remain for at least 2 years;\n",
    "2) _**persistence**_: after identified as abandonment, it remains non-cropland (excluding built-up area)\n",
    "3) _**long-term non-cropland**_: GLC_FCS data using majority method to resample to 1km resolution. If the pixel in 1985 and 1990 are non-cropland, but it changed within the first 5 years (convert for 2 consecutive years like ), it could also be considered as cropland expansion. (to avoid underminishing the cropland expansion). According to [referencen], not consider 1985 and 1990 cropland.\n",
    "\n",
    "--->---calculation 2: stable cropland and persistent abandonment \n",
    "\n",
    "aim: some reseach only consider the initial cropland, however, the immigration and scoical-economic fators may accelerate the cropland abandonment.\n",
    "\n",
    "\n",
    "1) _**effecitive abandon**_:the consecutive 5 years of non-cropland (exclude built-up area);\n",
    "2) _**persistence**_: it should remain as abandonment \n",
    "3) _**stable cropland**_:GLC_FCS to ensure the effectiveness of abandonment. If a conversion happens within the first 5 years, and 1985 and 1990 are cropland, it should be considered as cropland abandonment;\n",
    "\n",
    "--->---calculation 3: abandonment and recultivation\n",
    "\n",
    "aim: this situation is considered as special situation, to see with time-series data, \n",
    "\n",
    "1) _**effective abandon**_: it should be identified as cropland abandonment\n",
    "2) _**reclutivate change**_: the pixel turns back to cropland for two consecutive years\n",
    "3) _**persitence**_: the first time is just OK\n",
    "\n",
    "\n",
    "\n",
    "## 2. reclassification and biodiversity:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nexus approach of analyzing complex interlinkages\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glbcropland",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
